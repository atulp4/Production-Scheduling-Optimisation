{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Importing Libraris"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7de1ce65-f00d-4ccd-bda1-289e86608170",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# General Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Maths and Stats\n",
    "import math\n",
    "\n",
    "# Datetime\n",
    "import datetime as dt\n",
    "\n",
    "# Exclude warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# importing modeling Libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import random\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from dateutil import parser\n",
    "import os\n",
    "import datetime"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d203ffc1-2259-4669-a8ba-48a3a711bafa",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decleare Functions"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdf49652-3365-414e-a234-de1ca227a84c",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "def date_constraint(df = None, data_cip = None, clean_start_time = None, clean_end_time =  None):\n",
    "    \"\"\" Handels all possible date related contraint and return a data frame \n",
    "        with updated clean start and end date\"\"\"\n",
    "    data_max_mc = df.groupby(['MC Group'], as_index = False)['new_clean_end'].max()   # Get the Busytill time for each  CIP\n",
    "    data_max_const = df.groupby(['Constraint'], as_index = False)['new_clean_end'].max() # Get the Busytill time for each  Constraint\n",
    "\n",
    "    data_cip = pd.merge(data_cip, data_max_mc, how = 'left', on = 'MC Group') # bring the dates for each mc group\n",
    "    if data_max_mc.shape[0] > 0:                    # To hable the scenarios excluding first iteration\n",
    "        data_cip['Clean_End'] = np.where(data_cip['new_clean_end'].isnull(), data_cip['Clean_End'], \n",
    "                                        data_cip['new_clean_end'])    # Fill the clean end time with the busy till time\n",
    "    data_cip = data_cip.drop(['new_clean_end'], axis = 1)             # Drop the column\n",
    "    \n",
    "    # Repeat the above operation for Constraint too\n",
    "    data_cip = pd.merge(data_cip, data_max_const, how = 'left', on = 'Constraint')\n",
    "    if data_max_const.shape[0] > 0:\n",
    "        data_cip['Clean_End'] = np.where(data_cip['new_clean_end'].isnull(), data_cip['Clean_End'], \n",
    "                                        data_cip['new_clean_end'])\n",
    "    data_cip = data_cip.drop(['new_clean_end'], axis = 1)            \n",
    "    # Check the condition and change the flag accordingly\n",
    "    data_cip['Status'] = np.where(clean_start_time > data_cip['Clean_End'], 0,1)\n",
    "#                                   data_cip['Status'])\n",
    "    data_cip['Clean_Start'] = np.where(clean_start_time > data_cip['Clean_End'], static_date, \n",
    "                                       data_cip['Clean_Start'])\n",
    "    data_cip['Clean_End'] = np.where(clean_start_time > data_cip['Clean_End'], static_date, \n",
    "                                     data_cip['Clean_End'])\n",
    "    return data_cip"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7927f7f4-4164-45fc-a147-ccc1f8e4de4c",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "def available_CIP(used_cip = [], used_constraint = [], cip_data = None, data = None):\n",
    "    \n",
    "    used_cip = cip_data[cip_data['Status'] == 1]['MC Group'].unique().tolist()\n",
    "    used_constraint = cip_data[cip_data['Status'] == 1]['Constraint'].unique().tolist()\n",
    "    \n",
    "    mc_list = cip_data[(cip_data['Resource'] == res) & \n",
    "                       (~cip_data['Constraint'].isin(used_constraint)) & \n",
    "                       (~cip_data['MC Group'].isin(used_cip))]['MC Group'].tolist()\n",
    "    \n",
    "    data_group = data.groupby('MC Group', as_index = False)['clean_flag'].sum()\n",
    "    mc_order = pd.merge(mc_matrix[['MC Group', 'flex_fact']], data_group, how = 'left', \n",
    "                        on = 'MC Group').fillna(0).sort_values(['clean_flag', 'flex_fact'])['MC Group'].tolist()\n",
    "    \n",
    "    mc_list = [x for x in mc_order if x in mc_list]\n",
    "    \n",
    "    return mc_list"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2fb2a567-a0ae-47dd-a298-8e2123bc6564",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_constraint(df = None):\n",
    "    # Reformat the new clenaing timings\n",
    "    df['Clean_Start_Time']= pd.to_datetime(df['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    df['Clean_End_Time']= pd.to_datetime(df['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    df['Usage_End']= pd.to_datetime(df['Usage_End'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    df['Next Usage'] =  pd.to_datetime(df['Next Usage'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    df['Actual DHT'] = df['Clean_End_Time'] - df['Usage_End']\n",
    "    df['Actual DHT'] = df['Actual DHT']/(np.timedelta64(1, 's')*3600)\n",
    "\n",
    "    # If there is NAN due to shift. fill the last clean date as the plan start date time\n",
    "    df['Actual CHT'] = df['Next Usage'] - df['Clean_End_Time']\n",
    "    df['Actual CHT'] = df['Actual CHT']/(np.timedelta64(1, 's')*3600)\n",
    "\n",
    "    df['DHT Violation Flag'] = np.where((df['Actual DHT']> df['Max DHT']) | (df['Actual DHT'].isnull()), 1, 0)\n",
    "    df['CHT Violation(Intermediate Gap)'] = np.where(df['Actual CHT']> df['Max CHT'], 1, 0)\n",
    "    df['CHT_Adjust_Range'] = round((df['Last Cleaning_DHT'] - df['Start Cleaning_CHT'])/(np.timedelta64(1, 's')*60),0)\n",
    "    cond1 = (df['Unutilized_gap'] < df['Max DHT']+df['Max CHT'])\n",
    "    cond2 = (df['CHT Violation(Intermediate Gap)'] ==1)\n",
    "    cond3 = (df['CHT_Adjust_Range'] > 0)\n",
    "    df['CHT_Adjust'] = np.where(cond1 & cond2 & cond3, 1, 0)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6f2d1e6-2837-42dc-a702-a8a782ef03af",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def CIP_assignment(df = None, index = None, main_flag = 0, val_dict = None):\n",
    "    val_dict = [i for i in val_dict.items()]\n",
    "    count = 0\n",
    "    for val in val_dict:\n",
    "        count = count+1\n",
    "        if (count > 5) & (main_flag == 0):\n",
    "            break\n",
    "        df.loc[index, val[0]] = val[1] \n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18325dc0-1f0e-47c7-bbc7-416f5bca4f12",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_allocate(df_mc = None, df_cons = None, res = '', df_update = None, index = None, loop_df= None, main_flag = 0, \n",
    "                   mc_const_df = None, clean_start = None, clean_end = None, m_start = None, m_end = None, \n",
    "                   run_flag = 0, th_time = None, reclean = 0, flag_var = '', buffer_time = None):\n",
    "    \n",
    "    mc_avail = check_cond(df = df_mc, col = 'MC Group', cleaning_start = clean_start, \n",
    "                          cleaning_end = clean_end)['MC Group'].unique().tolist()\n",
    "    const_avail = check_cond(df = df_cons, col = 'Constraint', cleaning_start = clean_start, \n",
    "                             cleaning_end = clean_end)['Constraint'].unique().tolist()\n",
    "    rows = 0\n",
    "    for mc in mc_avail:\n",
    "        for const in const_avail:\n",
    "            \n",
    "            rows = mc_const_df[(mc_const_df['Resource'] == res) & (mc_const_df['Constraint'] ==const) \n",
    "                               & (mc_const_df['MC Group'] ==mc)].shape[0]\n",
    "            if rows>0:\n",
    "                cl_start, cl_end = enough_gap_constraint(df = df_update.copy(), mc_group = mc,\n",
    "                                                         resource = res, mins = buffer_time,\n",
    "                                                         clean_start_tm = clean_start,\n",
    "                                                         clean_end_tm = clean_end, th_time=th_time)\n",
    "                \n",
    "                cl_start, cl_end = test_overlap(df = loop_df.copy(), fin_data= final_df.iloc[:-1,:], cl_st = cl_start, \n",
    "                                                cl_end = cl_end, cip_gr = mc,const_gr= const, mins = buffer_time)\n",
    "                if (clean_start > th_time):\n",
    "                    rows = 0\n",
    "                    continue\n",
    "                if main_flag == 1:\n",
    "                    print(m_start, m_end)\n",
    "                dict_values = {'Clean_Start_Time': clean_start, 'Clean_End_Time': clean_end, \n",
    "                               'MC Group': mc, 'Constraint': const, flag_var: 1, 'Usage_Start': m_start, \n",
    "                               'Usage_End': m_end, 'Maint_Flag': main_flag}\n",
    "                \n",
    "                loop_df = CIP_assignment(df = loop_df, index = index, main_flag = main_flag, val_dict = dict_values)\n",
    "                break\n",
    "        if rows>0:\n",
    "            break\n",
    "    return  rows, loop_df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8228c695-d75a-404b-8262-7a4d0aa3ffe5",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_constraint_preclean(df = None):\n",
    "    # temp object creationa and creating CHT timing\n",
    "    temp_df = pd.DataFrame()\n",
    "    for res in df.Resource.unique():\n",
    "        data_temp = df[df['Resource'] == res]\n",
    "        data_temp = data_temp.sort_values(['Usage_Start', 'Clean_Start_Time'])\n",
    "        # Shift the data 1 level up to get perform the substraction operation\n",
    "        data_temp['Last Clean End'] = data_temp['Clean_End_Time'].ffill(axis = 0).shift(1)\n",
    "        temp_df = pd.concat([temp_df, data_temp])        \n",
    "    df = temp_df.sort_values('Usage_End').reset_index(drop = True)\n",
    "    del(temp_df)\n",
    "    df_par = df[df['Resource'].isin(parellal_res)].sort_values(['Clean_Start_Time']).reset_index(drop = True)\n",
    "    df = df[~df['Resource'].isin(parellal_res)]\n",
    "    df_par['Last Clean End'] = df_par['Clean_End_Time'].ffill(axis = 0).shift(1)\n",
    "    df_par['Last Clean End'] = np.where(df_par['Clean_Start_Time'] == df_par['Clean_Start_Time'].ffill(axis = 0).shift(1),\n",
    "                                        df_par['Clean_End_Time'].ffill(axis = 0).shift(2), \n",
    "                                        df_par['Clean_End_Time'].ffill(axis = 0).shift(1))\n",
    "    \n",
    "    df = pd.concat([df_par, df]).sort_values('Usage_Start').reset_index(drop = True)\n",
    "    \n",
    "    df['Last Clean End'] = df['Last Clean End'].fillna(df.Usage_Start.min())\n",
    "    df['Last CHT'] = df['Usage_Start'] - df['Last Clean End']\n",
    "    df['Last CHT'] = df['Last CHT']/(np.timedelta64(1, 's')*3600)\n",
    "    df['CHT Violation Flag Pre Clean'] = np.where(df['Last CHT']> df['Max CHT'], 1, 0)\n",
    "\n",
    "    # Create the starting time slot to preclean \n",
    "    df['Pre Clean Start'] = df['Max CHT']+ df['Washing Time']\n",
    "    df['Pre Clean Start'] = df['Usage_Start'] - pd.to_timedelta(df['Pre Clean Start'], unit='h')\n",
    "    df['Pre Clean Start'] = pd.to_datetime(df['Pre Clean Start'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6188d099-24d3-4438-8ed2-9f46a14baa3f",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def add_frozen_cip(pre_data, post_data):\n",
    "    df = pre_data.copy()\n",
    "    try:\n",
    "        post_data['Clean_Start_Time'] = pd.to_datetime(post_data['Clean_Start_Time'])\n",
    "        post_data['Clean_End_Time'] = pd.to_datetime(post_data['Clean_End_Time'])\n",
    "        pre_data['Clean_Start_Time'] = np.nan\n",
    "        pre_data['Clean_End_Time'] = np.nan\n",
    "        pre_data['MC Group'] = np.nan\n",
    "        pre_data['Constraint'] = np.nan\n",
    "        post_data['par_clean_flag'] = np.where(post_data['Cleaning Type']=='Parallel Cleaning', 1, 0)\n",
    "        for i,r in post_data.iterrows():\n",
    "            res = r['Resource']\n",
    "            temp_df= pre_data[pre_data['Resource'] == res].sort_values('Usage_Start')\n",
    "            index_list = temp_df.index\n",
    "            pre_data.loc[index_list[0],'Clean_Start_Time'] =r['Clean_Start_Time']\n",
    "            pre_data.loc[index_list[0],'Clean_End_Time'] =r['Clean_End_Time']        \n",
    "            pre_data.loc[index_list[0],'MC Group'] =r['MC Group']\n",
    "            pre_data.loc[index_list[0],'par_clean_flag'] = r['par_clean_flag']\n",
    "        pre_data['Clean_Start_Time'] = pd.to_datetime(pre_data['Clean_Start_Time'])\n",
    "        pre_data['Clean_End_Time'] = pd.to_datetime(pre_data['Clean_End_Time'])\n",
    "        \n",
    "        pre_data['Cleaning Type'] = np.where((~pre_data['MC Group'].isnull()) & \n",
    "                                             (pre_data['Clean_Start_Time'] >= pre_data['Usage_End']),'q-Post Utilization Cleaning', \n",
    "                                              np.where((~pre_data['MC Group'].isnull()) & \n",
    "                                                     (pre_data['Clean_End_Time'] <= pre_data['Usage_Start']),'q-Pre Cleaning', np.nan))\n",
    "        print(pre_data.shape)\n",
    "        pre_cleaning_df = pre_data[pre_data['Cleaning Type']=='q-Pre Cleaning']\n",
    "        print(pre_cleaning_df.shape)\n",
    "        pre_cleaning_df[['Clean_Start_Time', 'Clean_End_Time','MC Group', 'Cleaning Type']]= np.nan\n",
    "        pre_data = pd.concat([pre_data, pre_cleaning_df]).sort_values(['Resource', 'Usage_Start']).reset_index(drop =True)\n",
    "        \n",
    "        pre_data['Clean_Start_Time'] = np.where(pre_data['Clean_Start_Time'].isnull(),np.nan,pre_data['Clean_Start_Time'])\n",
    "        pre_data['Clean_End_Time'] = np.where(pre_data['Clean_End_Time'].isnull(),np.nan,pre_data['Clean_End_Time'])\n",
    "        pre_data['Clean_Start_Time'] = pd.to_datetime(pre_data['Clean_Start_Time'])\n",
    "        pre_data['Clean_End_Time'] = pd.to_datetime(pre_data['Clean_End_Time'])\n",
    "        return pre_data\n",
    "    except:\n",
    "        df['Clean_Start_Time'] = np.nan; df['Clean_End_Time'] = np.nan\n",
    "        df['MC Group'] = np.nan; df['Constraint'] = np.nan; df['Cleaning Type']= np.nan\n",
    "        return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "488a9836-db59-472a-bee5-b863ecd091ce",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Variable Definition"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9d9c2da-adc8-4d35-abc7-8356690c5699",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# Defining the variables to be used\n",
    "buffer = 10\n",
    "date_before = datetime.date(2021, 8, 14)\n",
    "buffer_dec = buffer/60\n",
    "# File names to be read\n",
    "raw_file_name = '20210713_Takeda Utilization Planned Schedule.xlsx'\n",
    "map_file_name = 'Takeda Mapping Input Params.xlsx'\n",
    "\n",
    "# Sheet to be used to read\n",
    "raw_sheet_name = 'Sheet1'\n",
    "map_sheet_name = 'CIP Constraint Mapping'\n",
    "par_clean_sheet = 'Exception - Parelall Cleaning'\n",
    "\n",
    "# Resource to be selected to create framework\n",
    "# res_select = ['615', 'I/N 1619', 'P1', 'P2', 'P3','P4', 'SA', 'SC']\n",
    "\n",
    "parellal_res = ['Short Line - L717S', 'AS26 TD']\n",
    "\n",
    "\n",
    "# input_path= r'C:\\Users\\AtulPoddar\\OneDrive - TheMathCompany Private Limited\\01. Accounts\\03. Takeda\\03. Data Processing\\02. Planned Data\\20210612-4'\n",
    "def last_modified_file():\n",
    "  l = []\n",
    "  path = \"/dbfs/mnt/TakedaMount2/Src1/\"\n",
    "  for root, dirs, files in os.walk(path):\n",
    "      for file in files:\n",
    "          if(file.endswith(\".TXT\")):\n",
    "              l.append(os.path.join(root,file))\n",
    "  l.sort(key=os.path.getmtime, reverse=True)\n",
    "  return(l[0])\n",
    "def last_modified_file_path():\n",
    "  full_file_path = last_modified_file().split('/')\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path_without_filename='/'.join(full_file_path)\n",
    "  return(full_file_path_without_filename)\n",
    "def utcnow():\n",
    "  full_file_path = last_modified_file().split('/')\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path.pop(-1)\n",
    "  x=full_file_path.pop(-1)\n",
    "  full_file_path_without_filename='/'.join(full_file_path)\n",
    "  return(x)\n",
    "def uploaded_file_name():\n",
    "  full_file_path = last_modified_file().split('/')\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path.pop(-1)\n",
    "  x=full_file_path.pop(-1)\n",
    "  full_file_path_without_filename='/'.join(full_file_path)\n",
    "  return(x)\n",
    "input_path =  last_modified_file_path() + '/input/'\n",
    "def create_folder():\n",
    "  full_file_path = last_modified_file().split('/')\n",
    "  full_file_path.pop(0)\n",
    "  full_file_path.pop(0)\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path.pop(-1)\n",
    "  full_file_path_without_filename='/'.join(full_file_path)\n",
    "  full_file_path_without_filename = '/' + full_file_path_without_filename\n",
    "  return(full_file_path_without_filename)\n",
    "os.chdir(input_path)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c057dd2d-9663-4240-97cc-55870e358655",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initializing Variables\n",
    "1. Buffer variable is provided to get a buffer time while pipes are switching from one tank to another. \n",
    "2. File names containing utlization of resources and mapping file is mentioned.\n",
    "3. We have also specified the sheet names.\n",
    "- raw sheet name is sheet name for utilization file we'll be creating.\n",
    "4. In a list we have mentioned the resource names to be processed parallelly. Path of all input files has been stored in a variable and curent directory has been changed to the path mentioned."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining the variables to be used\n",
    "buffer = 10\n",
    "#date_before = datetime.date(2021, 8, 14)\n",
    "buffer_dec = buffer/60\n",
    "# File names to be read\n",
    "raw_file_name = '20210713_Takeda Utilization Planned Schedule.xlsx'\n",
    "map_file_name = 'Takeda Mapping Input Params.xlsx'\n",
    "\n",
    "# Sheet to be used to read\n",
    "raw_sheet_name = 'Sheet1'\n",
    "map_sheet_name = 'CIP Constraint Mapping'\n",
    "par_clean_sheet = 'Exception - Parelall Cleaning'\n",
    "\n",
    "# Resource to be selected to create framework\n",
    "# res_select = ['615', 'I/N 1619', 'P1', 'P2', 'P3','P4', 'SA', 'SC']\n",
    "\n",
    "parellal_res = ['Short Line - L717S', 'AS26 TD']\n",
    "\n",
    "\n",
    "# input_path= r'C:\\Users\\NischalKumar\\OneDrive - TheMathCompany Private Limited\\01. Accounts\\03. Takeda\\03. Data Processing\\02. Planned Data\\20210612-4'\n",
    "input_path= r'C:\\Users\\AtulPoddar\\OneDrive - TheMathCompany Private Limited\\Documents\\Takeda\\testing\\lot_code_test3'\n",
    "# input_path= r'C:\\Users\\NischalKumar\\OneDrive - TheMathCompany Private Limited\\01. Accounts\\03. Takeda\\03. Data Processing\\02. Planned Data\\20211006'\n",
    "os.chdir(input_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concatinating all input into one dataframe\n",
    "1. combined all input files except CIP, ENG and down time file (yet to receive downtime file).\n",
    "2. A list arr is created with all the file names except those mentioned in ignore_file list.\n",
    "3. Blank dataframe master_df is created. Read each file and store it in df, creating a new column 'File Name' indicating data is picked up from which file(.TXT of file name is replaced with ' ').\n",
    "4. After each loop dataframes are concatinated with the master_df. At the end index has been reset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- CIP file -- this file has data when a clean activity is already scheduled(usually empty)\n",
    "- ENG batch file-- this file contains maintainance details of tanks\n",
    "- Down time file-- this file would contain down time of cleaning groups-- now not in use"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ignore_file = ['CIP Q - CIP.TXT', 'CIP Q - ENG batch.TXT', 'CIP Q - Down Time.txt']\n",
    "arr = [x for x in os.listdir() if x not in ignore_file]\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "for i in arr:\n",
    "    try:\n",
    "        df = pd.read_csv(i, delimiter = \"\\t\", header = None)\n",
    "    except:\n",
    "        continue\n",
    "    df['File Name'] = str(i).replace('.TXT', '')\n",
    "    master_df = pd.concat([master_df, df])\n",
    "master_df.reset_index(drop = True, inplace = True)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35ed9009-817c-421f-8caf-7372a6a461f4",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "master_df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- In addT column i.e 6th column null values are imputed with 0.0. This column indicates if scheduled time needs to be prepond and  max CHT time needs to be extended."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "master_df[6] = pd.to_numeric(master_df[6],errors='coerce').fillna(0.0)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcdb0e69-015e-469a-8565-147de68654dd",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "1. Define the column names of master_df. Set the format of usage_start and usage_end column. Many a times position of Usage_start and usage_end time are swaped, to check this we substract start time from end time, if  value is negetive store the records in dataframe master_df1. In case of postive, store it in  master_df1. \n",
    "2. swap the column values for master_df2. \n",
    "3. Divide addT by 100 to get the exact hour value. To standardize values under Type column values containing Centrifugation substing converted to Centrifugation. Get minimum time from Usage_start time and store it in today varaible. This varable is used to name the comibed data .xlsx file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# master_df[5] = master_df[5].fillna(master_df[6])\n",
    "#master_df[6] = pd.to_numeric(master_df[6],errors='coerce').fillna(0.0)\n",
    "master_df = master_df[[0,1,2,3,4,5,'File Name',6]]\n",
    "master_df.columns = ['Code', 'Resource', 'LOT', 'Usage_Start', 'Usage_End', 'Type', 'File Name','addT']\n",
    "\n",
    "# # Converting the columns to date and time\n",
    "master_df['Usage_Start']= pd.to_datetime( master_df['Usage_Start'], format = '%d/%m/%Y %H:%M:%S')\n",
    "master_df['Usage_End']= pd.to_datetime(master_df['Usage_End'], format = '%d/%m/%Y %H:%M:%S')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from random import randint\n",
    "\n",
    "master_df_test_null = master_df[master_df['LOT'].isna()]\n",
    "master_df = master_df[~master_df['LOT'].isna()]\n",
    "res_len = len(master_df_test_null['Resource'])\n",
    "lot_random_list = random.sample(range(100,1000),res_len)\n",
    "lot_list = ['#DUMMY{0}'.format(i) for i in  lot_random_list]\n",
    "master_df_test_null['LOT'] = lot_list \n",
    "master_df = pd.concat([master_df,master_df_test_null])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "utilization = (master_df['Usage_End']- master_df['Usage_Start'])/(np.timedelta64(1, 's')*3600)\n",
    "master_df1 = master_df[utilization<0]\n",
    "master_df2 = master_df[utilization>0]\n",
    "\n",
    "use_end = master_df1['Usage_Start'].copy()\n",
    "use_start = master_df1['Usage_End'].copy()\n",
    "master_df1['Usage_Start'] = use_start\n",
    "master_df1['Usage_End'] = use_end\n",
    "# # Converting the columns to date and time\n",
    "\n",
    "\n",
    "master_df = pd.concat([master_df1, master_df2])\n",
    "master_df['Usage_Start']= pd.to_datetime( master_df['Usage_Start'], format = '%d/%m/%Y %H:%M:%S')\n",
    "master_df['Usage_End']= pd.to_datetime(master_df['Usage_End'], format = '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "master_df['addT'] = master_df['addT']/100\n",
    "\n",
    "master_df.loc[master_df['Type'].str.contains('Centrifugation'), 'Type'] = 'Centrifugation'\n",
    "\n",
    "today = master_df.Usage_Start.min().strftime('%Y%m%d')\n",
    "\n",
    "# # date_after =  parser.parse('2021-08-15 00:00:00') \n",
    "# # master_df = master_df[master_df['Usage_Start']>=date_after]\n",
    "\n",
    "# master_df.to_csv(last_modified_file_path() + '/input/' + today + '_' + utcnow() + '_Takeda_Utilization_Planned_Schedule.csv', index = False)\n",
    "# master_df.shape\n",
    "\n",
    "\n",
    "\n",
    "master_df.to_excel(today+'_Takeda Utilization Planned Schedule.xlsx', index = False)\n",
    "master_df.shape"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be6a1fa9-5580-4655-b3fb-c88aae983856",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the data\n",
    "- we'll get a schedule json file, if file is not present then MClist has values  ['MC1','MC2','MC3','MC4']. This file will give info. about all active MC groups."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7e0abaa-c459-41cb-98c0-b3cd8da92a6d",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "try:\n",
    "    f = open('schedule_input.json')\n",
    "    schedule_input = json.load(f)\n",
    "    f.close()\n",
    "    MClist = schedule_input['data']['Active MC']\n",
    "    if MClist==[]:\n",
    "        MClist = ['MC1','MC2','MC3','MC4']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    MClist = ['MC1','MC2','MC3','MC4']"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "034f0c17-3901-43dd-8f64-b01029858ccc",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# Reading the data and naming the columns\n",
    "data = pd.read_csv(last_modified_file_path() + '/input/' + today + '_' + utcnow() + '_Takeda_Utilization_Planned_Schedule.csv')\n",
    "# data.columns = ['Code', 'Resource', 'LOT Code', 'Usage_Start', 'Usage_End']\n",
    "data = data.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "#Columns renaming\n",
    "data = data.rename(columns = {'Start use': 'Usage_Start', 'End use': 'Usage_End'})\n",
    "# data = data.rename(columns = {'Usage Start': 'Usage_Start', 'Usage End': 'Usage_End'})\n",
    "\n",
    "# # Converting the columns to date and time\n",
    "data.Usage_Start= pd.to_datetime(data.Usage_Start).dt.round('S').astype('datetime64[s]')\n",
    "data.Usage_End= pd.to_datetime(data.Usage_End).dt.round('S').astype('datetime64[s]')\n",
    "res_select =  data.Resource.unique().tolist()\n",
    "res_select = [str.strip(str(x)) for x in res_select]"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c5a4e38b-e92d-4bb9-8de3-06f1a343a546",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Read the combined data .xlsx file to a pandas dataframe. Then we drop duplicates from the dataframe. Get all uniquie resource names in a list. It might contain some extra spaces hence strip the values. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reading the data and naming the columns\n",
    "data = pd.read_excel(today+'_Takeda Utilization Planned Schedule.xlsx', sheet_name = raw_sheet_name, engine='openpyxl')\n",
    "# data.columns = ['Code', 'Resource', 'LOT Code', 'Usage_Start', 'Usage_End']\n",
    "data = data.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "#Columns renaming\n",
    "# data = data.rename(columns = {'Start use': 'Usage_Start', 'End use': 'Usage_End'})\n",
    "# data = data.rename(columns = {'Usage Start': 'Usage_Start', 'Usage End': 'Usage_End'})\n",
    "\n",
    "# # Converting the columns to date and time\n",
    "data.Usage_Start= pd.to_datetime(data.Usage_Start, format = '%d/%m/%Y %H:%M:%S').dt.round('S').astype('datetime64[s]')\n",
    "data.Usage_End= pd.to_datetime(data.Usage_End, format = '%d/%m/%Y %H:%M:%S').dt.round('S').astype('datetime64[s]')\n",
    "res_select =  data.Resource.unique().tolist()\n",
    "res_select = [str.strip(str(x)) for x in res_select]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing MC group downtime file\n",
    "1. Get columns names to be assigned to schedule daaframe in a list. \n",
    "2. Open Schedule_input.json file. Read it into a dataframe and assign the column names. Column name 'Clean_Start_Time', 'Clean_End_Time' has been provided in this way so as to append it later with a dataframe such that scheduled maintainace time will be assumed as the group/pipeline is engaged in other resource cleaning.\n",
    "3. ' - Group' is appended to all MC Group names"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "down_columns = ['MC Group', 'Clean_Start_Time', 'Clean_End_Time']\n",
    "try:\n",
    "    f = open('schedule_input.json')\n",
    "    schedule_input = json.load(f)\n",
    "    f.close()\n",
    "    df_downtime = pd.DataFrame(schedule_input['data']['activeMachine'])\n",
    "    df_downtime = df_downtime.rename(columns = {'activeMachine': 'MC Group', \n",
    "                                                'startDate':'Clean_Start_Time', 'endDate': 'Clean_End_Time'})\n",
    "    df_downtime = df_downtime[down_columns]\n",
    "except Exception as e:\n",
    "    df_downtime = pd.DataFrame(columns = down_columns)\n",
    "    \n",
    "df_downtime['Clean_Start_Time']= pd.to_datetime(df_downtime['Clean_Start_Time']).dt.tz_localize(None).astype('datetime64[s]')\n",
    "df_downtime['Clean_End_Time']= pd.to_datetime(df_downtime['Clean_End_Time']).dt.tz_localize(None).astype('datetime64[s]')\n",
    "df_downtime['MC Group'] = df_downtime['MC Group']+' - Group'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df_downtime)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resource maintainance file\n",
    "- Store column names of Maintainance dataframe in a list. Read the maintainance file to a dataframe df_maintainance, and assigne the column names. Calulate and store maintainance time span in another column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "maintain_columns = ['Code', 'Resource', 'dummy', 'Start_time', 'End_time', 'Action']\n",
    "try:\n",
    "    df_maintenance = pd.read_csv(input_path+ '\\\\'+ 'CIP Q - ENG batch.TXT', delimiter = \"\\t\", header = None)\n",
    "    df_maintenance.columns = maintain_columns\n",
    "    df_maintenance = df_maintenance.rename(columns = {'slno': 'Code'}).dropna(axis = 1, thresh= 1)\n",
    "except:\n",
    "    df_maintenance = pd.DataFrame(columns = maintain_columns)\n",
    "\n",
    "df_maintenance.Start_time= pd.to_datetime(df_maintenance.Start_time, format = '%d/%m/%Y %H:%M:%S').astype('datetime64[s]')\n",
    "df_maintenance.End_time= pd.to_datetime(df_maintenance.End_time, format = '%d/%m/%Y %H:%M:%S').astype('datetime64[s]')\n",
    "\n",
    "df_maintenance['M_time'] = df_maintenance['End_time'] - df_maintenance['Start_time'] \n",
    "df_maintenance['M_time'] = df_maintenance['M_time']/(np.timedelta64(1, 's')*3600)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ea27b21-b895-4477-a58c-df7a6926aa2a",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Freezed Cleaning file\n",
    "- Similar process is applied for CIP data  and resource renaming is done according business requirement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cipq_columns = ['LOT', 'Resource', 'Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Cleaning Type']\n",
    "try:\n",
    "    df_qcip = pd.read_csv(input_path +'\\\\'+ 'CIP Q - CIP.TXT', delimiter = \"\\t\", header = None)\n",
    "    df_qcip = df_qcip.dropna(axis= 1, thresh= 1)\n",
    "    df_qcip.columns = cipq_columns\n",
    "    replace_di = {'AS26 TD Line - L2445': 'AS26 TD',  'AS26 VII Line - L2464': 'AS26 VII'}\n",
    "    df_qcip.Resource = df_qcip.Resource.replace(replace_di) \n",
    "except:\n",
    "    df_qcip = pd.DataFrame(columns = cipq_columns)\n",
    "\n",
    "df_qcip['Clean_Start_Time']= pd.to_datetime(df_qcip['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S').astype('datetime64[s]')\n",
    "df_qcip['Clean_End_Time']= pd.to_datetime(df_qcip['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S').astype('datetime64[s]')"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f8a672f-8bcb-4344-a044-29dddec01e23",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- To be Continued"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chroma/T65/Special data\n",
    "1. Sort the main dataframe on basis of resource name and usage_end. \n",
    "- usage end name is given priority over usage start as we schedule cleaning process once utilization ends. Create a column named utilization whose value is usage_end- usage_start, in order to take the data in required format divide it by 3600 seconds. \n",
    "2. Now create a separate dataframe named data_chroma which is subset of main dataframe. here we consider records with resource name as chroma.\n",
    "3. Now create data_T65 dataframe subset where resource name is T65\n",
    "- By default resources with same LOT code, resource name and type are grouped and min usage start time,max start time and mean of addT are taken into consideration i.e these utilizations are considered as one utilization\n",
    "- For resources named as T65 the condition is: we need not group the resources and on basis of LOT code and Type. Each utilization is considered different even if they have same LOT code and same Type.\n",
    "4. Once the dataframes for T65 and Chroma has been created these records are removed from main dataframe\n",
    "5. Then we are alining the column names for data_T65 dataframe again\n",
    "6. Now we are creating a exception_resource list. These resources though have different  product Type can be considered under same utilization if their LOT id is same\n",
    "7. these excetion resource records are stored in data_special dataframe\n",
    "8. Type of these resources are hardcoded as 'Special Resource'\n",
    "9. Now remove these resources from main data frame and store it in Data_new dataframe\n",
    "10. append Data_new and data_special, reset index and store it in data dataframe\n",
    "11.  Now group by the records on basis of Resource, Type, LOT. Take min of usage_start, max of usage_end, sum od utilized and mean of addT. For exception resources as the product type is same i.e Special resource hence irrespective of product type exception resources will be grouped.\n",
    "12. Now append data_T65 to data, as data_T65 records need not be grouped according to business requirement.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Sorting the data inorder to get it in an expected order\n",
    "data = data.sort_values(['Resource', 'Usage_End']).reset_index(drop= True)\n",
    "\n",
    "# Calculating the duration from the available column\n",
    "data['Utilized'] = data['Usage_End'] - data['Usage_Start']\n",
    "\n",
    "# Changing the date delta form of numbers to floating number\n",
    "data['Utilized'] = data['Utilized']/(np.timedelta64(1, 's')*3600)\n",
    "\n",
    "data_chroma = data[data['Resource'] == 'Chroma']\n",
    "\n",
    "data_T65 = data[data['Resource']=='T65']\n",
    "data = data[~((data['Resource']=='T65') | (data['Resource'] == 'Chroma'))]\n",
    "\n",
    "\n",
    "\n",
    "data_T65 =  data_T65[['Resource', 'Type', 'LOT', 'Usage_Start', 'Usage_End', 'Utilized', 'addT']]\n",
    "\n",
    "exception_resource = ['Harvested Blow & Wash Additions', '4F Line - L2437', 'Long Transfer Line - L717L']\n",
    "\n",
    "data_special =  data[data['Resource'].isin(exception_resource)]\n",
    "\n",
    "data_special['Type'] = 'Special Resource'\n",
    "\n",
    "data_new = data[~(data['Resource'].isin(exception_resource))]\n",
    "\n",
    "data = pd.concat([data_new,data_special]).reset_index(drop =True)\n",
    "\n",
    "data = data.groupby(['Resource','Type', 'LOT'], as_index = False).agg({'Usage_Start': 'min',\n",
    "                                                                      'Usage_End': 'max',\n",
    "                                                                      'Utilized': 'sum',\n",
    "                                                                      'addT': 'mean'})\n",
    "data = pd.concat([data, data_T65]).reset_index(drop = True)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33acaa29-7374-4f2f-8e44-7f95f44ca7c8",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mapping and chroma mapping\n",
    "1. Read the mapping file(CIP Constraint Mapping sheet) and store it in mapping_df dataframe\n",
    "2. Then read the revised duration file. This file contains resource name and revised duration of resources on basis of historical data which differ in value from Washing time mentioned in mapping data.\n",
    "3. Then read the mapping file(Chroma sheet) and store it in map_chroma_df\n",
    "4. Read the mapping file(Exception - Parelall Cleaning) and store it in map_par_clean_df\n",
    "5. Rename column Product as Type in map_chroma_df\n",
    "6. Now merge data_chroma(containing utilization details of all chroma resources) with map_chroma_df[Resource,Type, Process resources]. We are not considering other columns, as our mapping_df has all the information for chroma records .\n",
    "- In data_chroma you'll find there is only one record for chroma resource, but when we'll join our chroma mapping with chroma utilization records we'll get 8 records against each chroma utilization record. Hence we'll perform cleaning for all product type of chroma resource.\n",
    "7. Products names with Flexibles - Kit Colonna as Process resources are stored in a list\n",
    "8. for these products the Type in Data_chroma data frame is defined as  Kit Colona others remain as it is\n",
    "9. Now group by Data_chroma on basis of 'Process resource', 'Resource','Type', 'LOT' as we did before.\n",
    "10. now there would be 4 rows(according to unique Process resource) against each chroma utilization record"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uncomment revised duration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pipe mapping data to be read\n",
    "# mapping_path = '/dbfs/mnt/TakedaMount2/Src1/Mapping Input Params1/'\n",
    "# mapping_df = pd.read_csv(mapping_path+'Takeda_Mapping_Input_Params_New_CIP_Constraint_Mapping.csv')\n",
    "# map_chroma_df = pd.read_csv(mapping_path+'Takeda_Mapping_Input_Params_New_Chroma.csv')\n",
    "# map_par_clean_df = pd.read_csv(mapping_path+'Takeda_Mapping_Input_Params_New_Exception_Parelall_Cleaning.csv')\n",
    "\n",
    "mapping_path = r'C:\\Users\\AtulPoddar\\OneDrive - TheMathCompany Private Limited\\Documents\\Takeda'\n",
    "mapping_df = pd.read_excel(mapping_path+'\\\\'+map_file_name, sheet_name = map_sheet_name, engine='openpyxl')\n",
    "#rev_duration = pd.read_excel(mapping_path+'\\\\'+'Takeda Mapping Revised Cleaning Durations.xlsx', engine='openpyxl')\n",
    "#rev_duration = rev_duration[['Resource', 'Revised Cleaning Duration']]\n",
    "\n",
    "map_chroma_df = pd.read_excel(mapping_path+'\\\\'+map_file_name,\n",
    "                                 sheet_name = 'Chroma', engine='openpyxl')\n",
    "\n",
    "map_par_clean_df = pd.read_excel(mapping_path+'\\\\'+map_file_name,\n",
    "                                 sheet_name = 'Exception - Parelall Cleaning', engine='openpyxl')\n",
    "\n",
    "map_chroma_df = map_chroma_df.rename(columns = {'Product': 'Type'})\n",
    "\n",
    "data_chroma = pd.merge(data_chroma, map_chroma_df.iloc[:, :3], on = ['Resource', 'Type'], how = 'left')\n",
    "\n",
    "# Product type to be added\n",
    "mul_type = [\"Column Product Charge\",\"Equilibration\",\"Eluition Chroma\",\"Rigeneration Buffer Add\", \"Washing Chroma\"]\n",
    "data_chroma['Type'] = np.where(data_chroma['Type'].isin(mul_type), 'Kit Colona', data_chroma['Type']) \n",
    "\n",
    "data_chroma = data_chroma.groupby(['Process resource', 'Resource','Type', 'LOT'], \n",
    "                                  as_index = False).agg({'Usage_Start': 'min',\n",
    "                                                          'Usage_End': 'max',\n",
    "                                                          'Utilized': 'sum',\n",
    "                                                          'addT': 'mean'})"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d523ae8a-3d80-4689-b3ab-268a28c5683c",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Active/absent MC list and Revised cleaning duartion\n",
    "1. Clean the mapping df by removing unnamed columns\n",
    "2. create a list allmcgroup  containing all MC groups \n",
    "3. MClist is the list we had prepared before using schedule json file, append ' - Group ' string to it\n",
    "- mc_list will contain only those groups which are being used and allmcgroup list will contain all group names\n",
    "4. absent_mc would contain the list of all MC groups which are not active now\n",
    "5. slno column is created just for developer's reference\n",
    "6. in mapping_df spaces are present in column names hence column names have been trimmed\n",
    "7. Now to update the Washing time of resources with revised cleaning duration, merge mapping_df with rev_duration on resource. update washing time revised cleaning duration where revised cleaning duration is not null.\n",
    "8. Drop Revised cleaning duration "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uncomment revised clean duration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mapping_df = mapping_df[[x for x in mapping_df.columns if 'Unnamed' not in x]] # Keeping the req info\n",
    "allmcgroup = ['MC1 - Group', 'MC2 - Group', 'MC3 - Group', 'MC4 - Group', 'MC5 - Group', 'MC6 - Group']\n",
    "mc_list = [str(x) + ' - Group' for x in MClist]\n",
    "absent_mc = [x for x in allmcgroup if x not in mc_list]\n",
    "#mapping_df = mapping_df.drop(absent_mc, axis = 1)\n",
    "# mapping_df = mapping_df.iloc[:46]\n",
    "# Creating a slno column to follow the same order\n",
    "mapping_df['slno'] = range(mapping_df.shape[0])\n",
    "mapping_df['slno'] = mapping_df['slno']+1\n",
    "# Removing the extra space from the column name to avoid confusions\n",
    "mapping_df.columns = [x.strip() for x in mapping_df.columns]\n",
    "# mapping_df = pd.merge(mapping_df, rev_duration, on='Resource', how = 'left')\n",
    "# mapping_df['Washing Time'] = np.where(mapping_df['Revised Cleaning Duration'].isnull(), \n",
    "#                                       mapping_df['Washing Time'], \n",
    "#                                       mapping_df['Revised Cleaning Duration'])\n",
    "# mapping_df = mapping_df.drop('Revised Cleaning Duration', axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. replace_di dictonary is created containing resource name which needs to be renamed as mappinf_df and utilization records differ in value. Hence replacimg the resource names in mapping_df according the dictonary\n",
    "2. We had created a parallel resource list initally which contains resource names which can be considered for parallel cleaning.\n",
    "3. create a a daataframe named df_special for these resouces.\n",
    "4. As the pipenames assigned to these resources were merged in mapping  .xlxs file while reading this only the first record contains the value. Hence did a forward fill in df_special dataframe.\n",
    "5. A separate data frame df_special was created because we cant do forward fill in mapping_df as there are many records with pipe value as NA, and that colud get affected.\n",
    "6. Then remove these parallel resources from mapping_df and then merge them back"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parallel/Special/Resource mapping preprocessing\n",
    "- Context--> We usually get 1 record of flexibles resource, MC1 and MC4 group would contain V as pipe name and other groups have some other values. But there is Pipe E under MC1 and MC4 group which can clean these Flexible resource. Hence we need to insert another row for flexible resource where E is populated under MC1 and MC4 group and and other groups are NA.\n",
    "- Create a blank templist list which would later contain MC1 - Group and MC2- Group.\n",
    "-  Flexibles resources are stored in df_flexi_chip and a copy of this in df_flexi_chip1.\n",
    "- Then we check if MC1-group is present id absent_mc list, if not then 'MC1- group' column of df_flex_chip1 is populated as E. Then add MC1- Group in templist. Similar thing is done for MC4 -Group.\n",
    "- create a list named drop_mc which contais all active groups except MC1 and MC4.\n",
    "- Now drop the columns present in drop_mc list from df_flex_cip1.\n",
    "- concatinate df_flex_cip and df_flex_cip1\n",
    "- create a dataframe temp_df from map_par_clean_df and only consider columns Resource,Type and P-clean flag\n",
    "- Filter out the records containing P-clean flag as 1\n",
    "- create a blank dataframe df_flexible_cip and a list named par_cl_res containing Process resource names in temp_df\n",
    "- loop through each and every element in par_cl_res. Assign the process resiurce name as resource in df_flex_cip and continate it with df_flexible_cip.\n",
    "- Now concatinate mapping_df with df_flexible_cip and reset the index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Resource column to make as string\n",
    "mapping_df['Resource'] = mapping_df['Resource'].astype('str')\n",
    "\n",
    "replace_di = {'AS26 TD Line - L2445': 'AS26 TD',  'AS26 VII Line - L2464': 'AS26 VII'}\n",
    "mapping_df.Resource = mapping_df.Resource.replace(replace_di) \n",
    "\n",
    "# Treating the special resources having same constraint and can be cleaned togather\n",
    "df_special = mapping_df[mapping_df.Resource.isin(parellal_res)].reset_index(drop = True)\n",
    "df_special = df_special.ffill()\n",
    "mapping_df = mapping_df[~mapping_df.Resource.isin(parellal_res)].reset_index(drop = True)\n",
    "\n",
    "# Concatenating both the dataset\n",
    "mapping_df = pd.concat([mapping_df,df_special])\n",
    "\n",
    "templist=[]\n",
    "df_flex_cip = mapping_df[mapping_df['Resource']=='Flexibles']\n",
    "df_flex_cip1 = df_flex_cip.copy()\n",
    "if 'MC1 - Group' not in absent_mc:\n",
    "    df_flex_cip1['MC1 - Group'] = 'E'\n",
    "    templist = ['MC1 - Group']\n",
    "if 'MC4 - Group' not in absent_mc:\n",
    "    df_flex_cip1[ 'MC4 - Group'] = 'E'\n",
    "    templist.append('MC4 - Group')\n",
    "drop_mc = [x for x in mc_list if x not in templist]\n",
    "  \n",
    "#df_flex_cip1[['MC1 - Group', 'MC4 - Group']] = 'E'\n",
    "df_flex_cip1 = df_flex_cip1.drop(drop_mc, axis = 1)\n",
    "df_flex_cip = pd.concat([df_flex_cip, df_flex_cip1])\n",
    "\n",
    "\n",
    "temp_df = map_par_clean_df.iloc[:,:3]\n",
    "temp_df = temp_df[temp_df['P-Clean Flag'] == 1]\n",
    "\n",
    "df_flexible_cip = pd.DataFrame()\n",
    "par_cl_res = temp_df['Process resource'].tolist()\n",
    "for i in par_cl_res[2:]:\n",
    "    print(i)\n",
    "    df_flex_cip['Resource'] = i\n",
    "    df_flexible_cip = pd.concat([df_flexible_cip, df_flex_cip])\n",
    "mapping_df = pd.concat([mapping_df, df_flexible_cip]).sort_values(['slno', 'Resource']).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c593887-d542-47de-b485-4555bb410525",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unpivoting parallel mapping- df_par_clean\n",
    "- Melting map_par \n",
    "- PC option with value 1 are taken into consideration\n",
    "- Output would be as follows:\n",
    "- Resource -----  Process resource ------- PC option"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_par_clean_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_par_clean= pd.melt(map_par_clean_df, id_vars = map_par_clean_df.columns[:2],\n",
    "                      value_vars=map_par_clean_df.columns[3:], var_name='PC Option')\n",
    "df_par_clean = df_par_clean[df_par_clean['value']==1].iloc[:,:-1].sort_values('Process resource').reset_index(drop = True)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e1dc0dd-96d5-452f-b742-8bf8f642d4fe",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merging Raw file with Mapping file and chroma data and process resources\n",
    "1. strip the resource column in utlization and mapping data.\n",
    "2. store resource and process resource columns from map_par_clean_df in df_table in data dataframe\n",
    "3. merge data and df_table\n",
    "4. append data_chroma to data"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3fcaca4c-ee9c-422d-9c2b-644a8b21679a",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preparing the data for merging with mapping file (Making sure the format of mapping column same)\n",
    "data['Resource'] = [str.strip(str(x)) for x in data['Resource'].tolist()]\n",
    "mapping_df['Resource'] = [str.strip(str(x)) for x in mapping_df['Resource'].tolist()]\n",
    "\n",
    "df_table = map_par_clean_df[['Resource', 'Process resource']]\n",
    "data = pd.merge(df_table, data, on = 'Resource', how = 'right')\n",
    "\n",
    "data = pd.concat([data, data_chroma], axis = 0)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4cd9edd8-9e46-4d1d-9954-8f22cc120e79",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_flex_4f = data[data['Resource']== '4F Line - L2437']\n",
    "\n",
    "dup_list = ['Flexibles - Mass Capture 1/2', 'Flexibles - Mass Capture 3/4']\n",
    "temp_flex = data[data['Process resource'].isin(dup_list)]\n",
    "\n",
    "a = [x for x in dup_list if x not in temp_flex['Process resource'].unique()] \n",
    "if len(a)>0:\n",
    "    data_flex_4f['Process resource'] = a[0]\n",
    "else:\n",
    "    for i, r in data_flex_4f.iterrows():\n",
    "        temp_flex['Distance'] = (r['Usage_Start'] - temp_flex['Usage_Start'])/(np.timedelta64(1, 's')*60)\n",
    "\n",
    "        temp_flex['time_needed1'] = temp_flex['Utilized']*60 +78+2\n",
    "        temp_flex['time_needed2'] = r['Utilized']*60+78+2\n",
    "\n",
    "        temp1 = temp_flex[(temp_flex['Distance']> 0) & (temp_flex['Distance'] < temp_flex['time_needed1'])]\n",
    "        temp2 = temp_flex[(temp_flex['Distance']< 0) & (temp_flex['Distance'] > -temp_flex['time_needed2'])]\n",
    "\n",
    "        a = [x for x in dup_list if x not in temp1['Process resource'].unique()] \n",
    "        if len(a)>0:\n",
    "            data_flex_4f.loc[i, 'Process resource'] = a[0]\n",
    "            continue\n",
    "        a = [x for x in dup_list if x not in temp2['Process resource'].unique()]\n",
    "        if len(a)>0:\n",
    "            data_flex_4f.loc[i, 'Process resource'] = a[0]\n",
    "            continue\n",
    "            \n",
    "data = pd.concat([data, data_flex_4f], axis = 0)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "748228c8-5250-473e-a221-ab6729dc26fc",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Now as mapping contains process resource names for flexible and chroma resources we need to refer process resource column in data dataframe\n",
    "2. Renaming the columns, and fill na values with resource exported column\n",
    "3. Merge data and Mapping_df dataframe on basis of resource. In case a resource name in mapping and data doesn't match that perticular resource will not be picked.\n",
    "4. Cleaning needs to be done after a maintainance process hence when shoud have info about max DHT, to know time available after a maintainance process.\n",
    "5. Sort the values in data1 on basis of priority, resource and usage_start"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# doubt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = data.rename(columns = {'Resource': 'Resource Exported','Process resource': 'Resource'})\n",
    "data['Resource'] = data['Resource'].fillna(data['Resource Exported'])\n",
    "\n",
    "# merging with only useful columns in the secondary table\n",
    "data1 = pd.merge(data, mapping_df[['Resource', 'Priority', 'Washing Time', 'Max DHT', 'Max CHT']].drop_duplicates(), \n",
    "                 on = 'Resource', how = 'inner')\n",
    "df_maintenance = df_maintenance.merge(mapping_df[['Resource','Max DHT']], on = 'Resource', how = 'left')\n",
    "\n",
    "# Sorting the data frame based on the column names\n",
    "data1 = data1.sort_values(['Priority', 'Resource', 'Usage_Start']).reset_index(drop = True)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "544b9a49-98e1-4bf0-b20a-10d55de3d195",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# addT\n",
    "1. Prepone usage_start time, extend max cht and utilization time on basis of addT column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data1['Usage_Start'] = data1['Usage_Start'] - pd.to_timedelta(data1['addT']*3600, unit='s')\n",
    "data1['Utilized'] = data1['Utilized'] + data1['addT']\n",
    "data1['Max CHT'] = data1['Max CHT'] + data1['addT']"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d6c8bc46-0291-420b-9618-2f7094f8ae80",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next utilization and unutilized\n",
    "1. Get Next utilization time of a resource and calculate gap duration between two consecutive operations. This will provide information about what time slot is avaiable for us to start cleaning between two utilization.\n",
    "2. Loop through each unique resource in data1 dataframe. Store records of each unique resource in data_temp, sort it on basis of usage_start.\n",
    "3. create a column next usage which would contain usage start time of next usage.\n",
    "4. Column Unutilized_gap is created to store the time slot available between two utilization\n",
    "5. concatinate all data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data1 = data1.sort_values(['Resource', 'Usage_Start', 'LOT', 'Usage_End'])\n",
    "\n",
    "# empty data frame to be used to concatenate in every iteration\n",
    "temp_df = pd.DataFrame()\n",
    "#looping through each resource to calculate the gap duration between two consicutive operation\n",
    "for res in data1.Resource.unique():\n",
    "    data_temp = data1[data1['Resource'] == res]\n",
    "    data_temp = data_temp.sort_values('Usage_Start')\n",
    "    # Shift the data 1 level up to get perform the substraction operation\n",
    "    data_temp['Next Usage'] = data_temp['Usage_Start'].shift(-1)\n",
    "    data_temp['Unutilized_gap'] = data_temp['Next Usage'] - data_temp['Usage_End']\n",
    "    temp_df = pd.concat([temp_df, data_temp])\n",
    "\n",
    "data2 = temp_df.reset_index(drop = True)\n",
    "del(temp_df)\n",
    "\n",
    "# Changing the date delta form of numbers to floating number\n",
    "data2['Unutilized_gap'] = data2['Unutilized_gap']/(np.timedelta64(1, 's')*3600)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f282761-622f-46fa-ba32-62baadde1741",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Formation of Raw dataset from the mapping file\n",
    "- Creating a dataframe to be used in optimization, i.e.\n",
    "- Get resource name, and all active group columns from mapping_df and store it in df_cip_gconst dataframe\n",
    "- Now melt df_cip_gconst, we'll get Resource, MC group, values\n",
    "- Remove records where value is null\n",
    "- Create a MC matrix which contains MC group, no of unique resources under each group and no. of unique pipes assiciated with it\n",
    "- const_matrix contains no. of resources associated with each pipe.\n",
    "- drop duplicates from df_cip_gconst_temp ane rename the column value to constraint"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15b5bdb5-952d-4ad2-b81b-05fd01cf38f9",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "select_var=['Resource']+allmcgroup\n",
    "df_cip_gconst = mapping_df.loc[:, select_var]\n",
    "df_cip_gconst = pd.melt(df_cip_gconst, id_vars = df_cip_gconst.columns[0], value_vars=df_cip_gconst.columns[1:], var_name='MC Group')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mapping_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a temporary data frame to hold the melted data\n",
    "\n",
    "# Creating a slno column to follow the same order\n",
    "df_cip_gconst = pd.merge(df_cip_gconst, mapping_df[['Resource', 'slno']], how = 'left', on = 'Resource')\n",
    "df_cip_gconst = df_cip_gconst[~df_cip_gconst['value'].isnull()]\n",
    "df_cip_gconst_all = df_cip_gconst.rename(columns = {'value': 'Constraint'}).drop_duplicates()\n",
    "df_cip_gconst= df_cip_gconst[~df_cip_gconst[\"MC Group\"].isin(absent_mc)].reset_index(drop=True)\n",
    "df_cip_gconst_temp = df_cip_gconst.drop_duplicates()\n",
    "df_cip_gconst_temp = df_cip_gconst_temp.rename(columns = {'value': 'Constraint'})\n",
    "\n",
    "# Creation of matrix needed for MC group matrix\n",
    "mc_matrix = df_cip_gconst.groupby('MC Group', as_index = False).agg({'Resource': pd.Series.nunique,\n",
    "                                                            'value': pd.Series.nunique})\n",
    "# Creation of matrix needed for Group constraint\n",
    "const_matrix = df_cip_gconst.groupby('value', as_index = False).agg({'Resource': pd.Series.nunique})"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "692ed640-d5ff-455f-94c8-5ccb74c63700",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Freezing"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8134a148-2adf-43fd-9ec1-9a0fc9d28245",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Context\n",
    "\n",
    "- There might be few cases when our schedules and cleaning may have been carried forward from previous week or continued after last week schedule time frame.\n",
    "- These cleanings are included in our CIP data for this week and are identified as freezed cleaning. These cleaning cant be changed.\n",
    "- Our code will tag these utilization with the schedules available provided for this week.\n",
    "- In case we find any information about previous utilization then its a post cleaning otherwise its tagged as precleaning\n",
    "- There will be few cases where two cleanings will be scheduled in between two utilization. In this case the 1st cleaning is tagged as post cleaning and 2nd cleaning as pre cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Create a copy of data2 in util_data\n",
    "2. Create a column DHT_Time - This column would give us the information about by when cleaning should complete after a utilization. This will help us identifying a freezed post cleaning for a utilization if any.\n",
    "2. Impute null values of Next usage with DHT time\n",
    "3. dropping DHT_Time??"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "util_data= data2.copy()\n",
    "util_data['DHT_Time'] = util_data['Usage_End'] + pd.to_timedelta(data2['Max DHT']*3600, unit='s')\n",
    "util_data['Next Usage'] = util_data['Next Usage'].fillna(util_data['DHT_Time'])\n",
    "util_data = util_data.drop('DHT_Time', axis = 1)\n",
    "util_data['Cleaning Type'] = np.nan\n",
    "qpre_data = df_qcip.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "df_qcip.to_clipboard(index = False)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "clean_start = r['Clean_Start_Time']\n",
    "\n",
    "temp_df= util_data[util_data['Resource'] == res].sort_values('Usage_Start')\n",
    "temp_df_1 = temp_df[(temp_df['Usage_End'] <= clean_start) & (temp_df['Next Usage'] > clean_start)]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. check if qpre_data contains any record,\n",
    "  - yes ---> then next step \n",
    "  - else ----> create columns 'Clean_Start_Time','Clean_End_Time', 'MC Group','Constraint','Cleaning Type'\n",
    "2. preclean_rep dataframe is created which will be used later to store freezed cleaning records not tagged as post cleaning\n",
    "3. Loop through each and every record of CIP and get the index and row values of each record\n",
    "4. res variable stores resource name of each record\n",
    "5. clean_start stores Clean_Start_Time of each record\n",
    "6. temp_df dataframe stores subset of util_data where resource name = res and sort by Utlization start\n",
    "7. temp_df_1 is subset of temp_df where clean_start_time > Usage _end time and next usage start time > usage start time and cleaning type is null\n",
    "8. store index of the above record in index_list list\n",
    "9. if index list not 0 then create and update columns 'Clean_Start_Time','Clean_End_Time','MC Group' and 'Cleaning Type'= 'q-Post Utilization Cleaning' in util_data\n",
    "10. if index list is o the concatinate the recorde with preclean_rep\n",
    "11. once the first loop is done for all cip records, iter through preclean_rep records.\n",
    "12. Same as above except another record would go under condition temp_df['Usage_Start'] > clean_end\n",
    "13. In case there are two cleanings in between utilization there would two records with 1st utilization one with post cleaning record and one with pre cleaning record "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "qpre_data.to_clipboard()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "qpre_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#### test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if qpre_data.shape[0]>0:\n",
    "    preclean_rep= pd.DataFrame()\n",
    "    for i,r in qpre_data.iterrows():\n",
    "        res = r['Resource']\n",
    "        clean_start = r['Clean_Start_Time']\n",
    "\n",
    "        temp_df= util_data[util_data['Resource'] == res].sort_values('Usage_Start')\n",
    "        temp_df_1 = temp_df[(temp_df['Usage_End'] <= clean_start) & (temp_df['Next Usage'] > clean_start)\n",
    "                            & (temp_df['Cleaning Type'].isnull())]\n",
    "\n",
    "        index_list = temp_df_1.index.to_list()\n",
    "        if len(index_list)==0:\n",
    "            preclean_rep = pd.concat([preclean_rep, qpre_data.loc[[i],:]])\n",
    "            print(\"Data unavailable for post cleaning\")\n",
    "        else:\n",
    "            util_data.loc[index_list[0],'Clean_Start_Time'] =r['Clean_Start_Time']\n",
    "            util_data.loc[index_list[0],'Clean_End_Time'] =r['Clean_End_Time']\n",
    "            util_data.loc[index_list[0],'MC Group'] =r['MC Group']\n",
    "            util_data.loc[index_list[0],'Cleaning Type'] ='q-Post Utilization Cleaning'\n",
    "\n",
    "    for i,r in preclean_rep.iterrows():\n",
    "        res = r['Resource']\n",
    "        clean_end = r['Clean_End_Time']; \n",
    "        clean_start = r['Clean_Start_Time']\n",
    "\n",
    "        temp_df = util_data[util_data['Resource'] == res].sort_values('Usage_Start')\n",
    "        temp_df_1 = temp_df[(temp_df['Usage_Start'] > clean_end)].sort_values('Usage_Start')\n",
    "\n",
    "        index_list = temp_df_1.index.to_list()\n",
    "        if len(index_list)==0:\n",
    "            print('Data Unavailable in the List ('+ str(i) + ',' + str(res) +\")\")\n",
    "        else:\n",
    "            rec_rec = util_data.loc[index_list[0]]\n",
    "            rec_rec['Clean_Start_Time'] = clean_start\n",
    "            rec_rec['Clean_End_Time'] = clean_end\n",
    "            rec_rec['MC Group'] = r['MC Group']\n",
    "            rec_rec['Cleaning Type'] = 'q-Pre Cleaning'\n",
    "            util_data = pd.concat([util_data, rec_rec.to_frame().T])\n",
    "    util_data = util_data.sort_values(['Resource', 'Usage_Start', 'Clean_Start_Time']).reset_index(drop = True)\n",
    "    convert_columns = ['Utilized', 'addT', 'Priority', 'Washing Time', 'Max DHT', 'Max CHT', 'Unutilized_gap']\n",
    "    util_data[convert_columns] = util_data[convert_columns].apply(pd.to_numeric)\n",
    "    \n",
    "else:\n",
    "    util_data['Clean_Start_Time'] = np.nan\n",
    "    util_data['Clean_End_Time'] = np.nan\n",
    "    util_data['MC Group'] = np.nan\n",
    "    util_data['Constraint'] = np.nan\n",
    "    util_data['Cleaning Type'] = np.nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "util_data.to_clipboard()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Copy updated Util data \n",
    "2. Create Map_table dataframe which is a subset of df_cip_gconst_temp(unpivoted datagframe of mapping dataframe) containing columns Resource, MC-Group and Constraint\n",
    "3. Removing records from map_table where constraint is E and resource starts with Flex so thatfreezed flexible resources are not allocated both v and e constraints\n",
    "4. rename column Constraint to Const\n",
    "5. Remove ' - Group' from MC Group name"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = util_data.copy() # add_frozen_cip(pre_data= data2.copy(), post_data = df_qcip.copy()).replace('nan', np.nan)\n",
    "map_table = df_cip_gconst_all[['Resource', 'MC Group', 'Constraint']]\n",
    "map_table = map_table[(map_table['Constraint']!='E') | (~map_table['Resource'].str.contains('Flex'))]\n",
    "map_table = map_table.rename(columns = {'Constraint': 'Const'})\n",
    "map_table['MC Group'] = map_table['MC Group'].str.replace(' - Group', '')\n"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d94f69e-433c-4041-8b87-c887453d195e",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre allocated Flexibles\n",
    "1. Segrigating allocated cleaning records and unallocated one into df_pre and df_post dataframes\n",
    "2. Merge Df_pre with map_table to get Constraint details\n",
    "3. df_flex_special datframe is created containing records where MC_group is 1 and 4 and resources should be a Flexible resource.\n",
    "4. Consider only 'Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Constraint' columns\n",
    "- in df_flex_special we have freezed cleanings of flexible resources handeled by MC1 and MC2 group\n",
    "- This is done to handel cleaning allocation for fleible resources as these resources have 2 set of constraints under MC1 and MC2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pre = df[~df['MC Group'].isnull()]\n",
    "df_post = df[df['MC Group'].isnull()]\n",
    "df_pre = pd.merge(df_pre, map_table, on = ['Resource', 'MC Group'], how = 'left')\n",
    "df_pre['Constraint'] = df_pre['Const']\n",
    "df_pre = df_pre.drop('Const', axis = 1)\n",
    "\n",
    "df_flex_special = df_pre[(df_pre['MC Group'].isin(['MC1', 'MC4'])) & \n",
    "                         (df_pre.Resource.str.contains('Flex'))].sort_values('Clean_Start_Time')\n",
    "df_flex_special = df_flex_special[['Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Constraint']].drop_duplicates()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. create dataframe df_flex_par_temp by grouping Clean_start_time, clean_end_time and MC_group in df_pre dataframe.\n",
    "2. Filter records where count is >1. This will provide us details about resources with parallel freezed cleanings.\n",
    "3. Create a column named Parallel cleaning flag which is like assiging sereal no to the records in df_flex_par_temp. \n",
    "4. storing the max no of parallel cleaning in last_par_count variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_flex_par_temp = df_pre.groupby(df_flex_special.columns[:-1].tolist(), as_index = False)['Resource'].count()\n",
    "df_flex_par_temp = df_flex_par_temp[df_flex_par_temp['Resource']==2]\n",
    "df_flex_par_temp['Parallel Clean Flag'] = list(range(1, df_flex_par_temp.shape[0]+1))\n",
    "try:\n",
    "    last_par_count = int(df_flex_par_temp['Parallel Clean Flag'].max())\n",
    "except:\n",
    "    last_par_count = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre allocated flexibles overlap\n",
    "1. if df_flex_special contains any record next steps are followed\n",
    "- These process is done to check if tehre is an overlap between two flexi cleaning.\n",
    "- by default V is assigned to all flexi resource, but in case of any overlap the 2nd cleaning will be assigned to E.\n",
    "- in case there are three records with an overlap, i.e 1st two records havea overlap and 2nd 3rd resource have an overlap then allocation of only 2nd resource needs to be changed. 3rd one will already be allocated to V.\n",
    "- Note: all flexible resources have washing duration of 78 minutes.\n",
    "2. Create a column 'last_clean_gap' which is diffrence between next resource cleaning start time and present record cleaning start time.\n",
    "- as we know time duration of cleaning for a flexi resource is 79 mins 1st record is assigned 79 mins.\n",
    "- in case of overlap the diffrence between two consicutive start time will be <79 mins\n",
    "3. create a column last_const where last cleaning constraint is taken into consideration.\n",
    "4. Assign the constrain E in case of overlap."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if df_flex_special.shape[0]>0:\n",
    "    df_flex_special['last_clean_gap'] = df_flex_special['Clean_Start_Time']-df_flex_special['Clean_Start_Time'].shift(1)\n",
    "    df_flex_special['last_clean_gap'] = df_flex_special['last_clean_gap']/(np.timedelta64(1, 's')*60)\n",
    "    df_flex_special['last_clean_gap'] = df_flex_special['last_clean_gap'].fillna(79)\n",
    "    df_flex_special['last_const'] = df_flex_special['Constraint'].shift(1)\n",
    "    count = 0\n",
    "    for i, r in df_flex_special.iterrows():\n",
    "        df_flex_special['last_const'] = df_flex_special['Constraint'].shift(1)\n",
    "        if count == 0:\n",
    "            count = count+1\n",
    "            continue\n",
    "        last_const = df_flex_special['last_const'][i]\n",
    "        print(last_const)\n",
    "        if (r['last_clean_gap']<78) & (last_const == r['Constraint']):\n",
    "            const =[x for x in ['V', 'E'] if x not in [last_const]][0]\n",
    "            print(const)\n",
    "            df_flex_special.loc[i,'Constraint'] = const\n",
    "            df_pre.loc[(df_pre['MC Group'] == r['MC Group']) & \n",
    "                       (df_pre['Clean_Start_Time'] == r['Clean_Start_Time']), 'Constraint'] = const"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parallel Clean flag\n",
    "1. Concat df_pre, df_post to create a final dataframe after frew cleanings \n",
    "2. Handel null values\n",
    "3. append ' - Group ' string to ['MC Group'] column\n",
    "4. take a copy of df in data2 dataframe\n",
    "5. append ' - Group ' string to ['MC Group'] column in df_flex_par_temp\n",
    "6. merging dara2 with df_flex_par_temp on 'Clean_Start_Time', 'Clean_End_Time', 'MC Group' this will give us the parallel cleaning flag in data2 dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.concat([df_pre, df_post], axis = 0).reset_index(drop = True)\n",
    "df = df.replace('nan', np.nan).replace('NaN', np.nan)\n",
    "df['MC Group'] = df['MC Group']+' - Group'\n",
    "data2 = df.copy()\n",
    "data2['Clean_Start_Time'] = pd.to_datetime(data2['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "data2['Clean_End_Time'] = pd.to_datetime(data2['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "df_flex_par_temp['MC Group'] = df_flex_par_temp['MC Group'] + ' - Group'\n",
    "\n",
    "data2 = pd.merge(data2, df_flex_par_temp[['Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Parallel Clean Flag']], \n",
    "                 on = ['Clean_Start_Time', 'Clean_End_Time', 'MC Group'], how = 'left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#data2.to_clipboard()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Working with the mapping file"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81ff68ac-6a0c-48cb-bacf-8fb3e1f4e308",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data2['Max DHT'] = data2['Max DHT']+(10/60)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa1e7765-c0c7-4352-9fbe-67f59c7bd768",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# creating few necessary columns\n",
    "1. Few necesary columns are required :\n",
    "2. Clean_start column- usage_end + 1 min\n",
    "- Clean start time is from what time the clean process can be scheduled\n",
    "3. clean end time- for the time it is null\n",
    "4. Ideal time = unutilized gap - washing time\n",
    "- Ideal time indicates duration when resource is nethier utilized nor washed\n",
    "5. Run flag = 1 and clean flag = 1\n",
    "- For the time all the records will hold run flag and clean flag as 1\n",
    "- in case of recleaning(later part of code) flag will be 0\n",
    "6. Max DTH = Utlization end + max DTH\n",
    "7. Last possible cleaning = next usage - Washing time\n",
    "- This indicates max possible start time to clean a resource\n",
    "- For few records this value can be null in case its the last utilization of a resource as Next usage will be null\n",
    "8. Last cleaning DHT - 'Max DHT Time' - 'washing time'\n",
    "- on basis of DHT what is the max possible start time to clean a resource\n",
    "9. Cond= last possible clean> last cleaning dht or last possible cleaning is null\n",
    "- we cant take last cleaning DHT time in case where maxt DHT exeeds next utilization start time.\n",
    "- In above case next utilization start time needs to be considered.\n",
    "- hence created above condition to check the same\n",
    "10. LAST Possible clean = if last possible clean> last clean dth or last possible clean is null then assign last cleaning DTH else last possible clean.\n",
    "11. Start Cleaning CHT - Next Usage - washing time\n",
    " - on basis of CHT what is earliest possible start time of cleaning\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creation of cleaning start just after the first buffer is finished\n",
    "data2['clean_start'] = data2['Usage_End'] + pd.Timedelta(minutes = 1)\n",
    "data2['clean_end'] = np.nan\n",
    "\n",
    "# Calculation of available hours post removing the buffer time\n",
    "data2['idle_time'] = data2['Unutilized_gap']-data2['Washing Time']; data2['run_flag'] = 1; data2['clean_flag'] = 1\n",
    "\n",
    "data2['Max DHT Time'] = data2['Usage_End'] + pd.to_timedelta(data2['Max DHT']*3600, unit='s')\n",
    "data2['Last Possible Clean'] = data2['Next Usage'] - pd.to_timedelta(data2['Washing Time']*3600, unit='s')\n",
    "data2['Last Cleaning_DHT'] = data2['Max DHT Time'] - pd.to_timedelta(data2['Washing Time']*3600, unit='s')\n",
    "\n",
    "data2[data2['Resource']=='P2'].tail()\n",
    "\n",
    "cond = (data2['Last Possible Clean']> data2['Last Cleaning_DHT']) | (data2['Last Possible Clean'].isnull())\n",
    "\n",
    "data2['Last Possible Clean'] = np.where(cond, data2['Last Cleaning_DHT'], data2['Last Possible Clean'])\n",
    "\n",
    "data2['Start Cleaning_CHT'] = data2['Next Usage'] - pd.to_timedelta(data2['Max CHT']*3600, unit='s')\n",
    "data2['Start Cleaning_CHT'] = data2['Start Cleaning_CHT'] - pd.to_timedelta(data2['Washing Time']*3600, unit='s')\n",
    "\n",
    "# Can create a column to see possible amount of time where it can run this"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3de13ca-91d9-4739-bd0d-0d99b0fb50c1",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handeling high priority resources\n",
    "1. Res_adjust- list of some important resources\n",
    "- These resources have long cleaning time, hence hardly violates max CHT. And maximum time we'll find their CHT and DHT are overlapped\n",
    "- For these resources we need to avoid recleaning, as there is a high chance we'll not get a recleaning slot due to above reason\n",
    "2. Cond1 = start cleaning CHT > clean start and resource is in res_adjust\n",
    "   Cond2 = start cleaning CHT is not null and \n",
    "   - Start cleaning CHT should be greater than clean start as there might be few cases where CHT time will exceed time gap between two utilization\n",
    "   - start cleaning_CHT should not be null -- this is required as there won't be any next utilization time, if we dont keet this condition recleanings might get alloted.\n",
    "   - resources should belong to res_adjust list\n",
    "   - start cleaning CHT < last possible clean\n",
    "3. Only if the above condition satisfies assign clean_start = Start clean_cht otherwise clean CHT\n",
    "4. Clean end time = clean_start + washing time\n",
    "5. last possible pre cleaning = ---- doubt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_adjust = ['P1', 'P2', 'P3', 'P4', 'SA', 'SB', 'SC', 'SD', 'AS26 TD',\n",
    "              'I/N 97', 'I/N 2224', 'I/N 2667','I/N 2315', 'I/N 1619','I/N 2668','I/N 2316','I/N 1554', \n",
    "              'Dome 121', 'Dome 89', 'Dome 2215', '617', '618', '619']\n",
    "\n",
    "cond1 = (data2['Start Cleaning_CHT']> data2['clean_start']) & (data2['Resource'].isin(res_adjust)) \n",
    "cond2 = (~data2['Start Cleaning_CHT'].isnull()) & (data2['Start Cleaning_CHT']< data2['Last Possible Clean'])\n",
    "# data2['Cond'] = cond1\n",
    "data2['clean_start'] = np.where(cond1 & cond2, data2['Start Cleaning_CHT'], data2['clean_start'])\n",
    "# Assuming the cleaning time is exactly as the defined time\n",
    "data2['clean_end'] = data2['clean_start'] + pd.to_timedelta(data2['Washing Time']*3600, unit='s')\n",
    "\n",
    "data2['Last_pos_preclean'] = data2['Usage_Start'] - pd.to_timedelta(data2['Washing Time']*3600, unit='s') - pd.Timedelta(minutes = buffer/5)\n",
    "data2['Day Flag'] = np.where(data2['clean_start'].dt.date!=data2['Last Cleaning_DHT'].dt.date, 1, 0)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "579c2040-53c1-498f-9621-064414c24380",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating Matrix to set priority\n",
    "- perc_rem_time --- smaller value means high priority resource as its less flexible.\n",
    "- resource with less ideal time has more priority\n",
    "- resource with less avaible time has more importance\n",
    "- resource with more washing time has more imp.\n",
    "- resource with more dht has more imp."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data filtered only slected resources\n",
    "data_selected = data2[data2['Resource'].isin(res_select)]\n",
    "data_selected1 = data_selected[['Resource', 'Day Flag', 'clean_start', 'clean_flag']]\n",
    "\n",
    "# Creation of idle time and utilization matrix at resource level\n",
    "data_matrix = data_selected.groupby('Resource', as_index= False).agg({'Unutilized_gap': 'mean', \n",
    "                                                                      'run_flag': 'sum',\n",
    "                                                                      'Washing Time': 'mean',\n",
    "                                                                      'Max CHT': 'mean',\n",
    "                                                                      'Max DHT': 'mean'})\n",
    "\n",
    "\n",
    "# Creation of %idel time metric based on the washing time and the idle time between 2 cycles\n",
    "data_matrix['perc_rem_time'] = 1 - (data_matrix['Washing Time']/data_matrix['Unutilized_gap'])\n",
    "data_matrix['perc_rem_time'] = data_matrix['perc_rem_time'].round(2)\n",
    "\n",
    "# Creation of same day flag for generating the flexibility factor at daily level\n",
    "data_selected1.clean_start = data_selected1.clean_start.dt.date\n",
    "data_selected_pivot = pd.pivot_table(data = data_selected1.copy(), values = 'clean_flag', \n",
    "                                     index = ['clean_start', 'Day Flag'], columns = 'Resource',\n",
    "                                     aggfunc = 'sum', dropna = False)\n",
    "\n",
    "data_clean_freq_matrix = data_selected_pivot.fillna(0).sum(axis = 1).reset_index(name ='Total Freq')\n",
    "\n",
    "data_clean_freq_matrix['day_index'] = data_clean_freq_matrix['clean_start'] - data_clean_freq_matrix['clean_start'].min()\n",
    "data_clean_freq_matrix['day_index'] = (data_clean_freq_matrix['day_index']/(np.timedelta64(1, 's')*3600))/24\n",
    "\n",
    "# Creation of column to have values such as 'day index, the schedule index'\n",
    "data_clean_freq_matrix['day_flag_index'] = data_clean_freq_matrix['day_index'] + data_clean_freq_matrix['Day Flag']\n",
    "data_clean_freq_matrix.day_index = data_clean_freq_matrix.day_index.astype('int').astype('str')\n",
    "data_clean_freq_matrix.day_flag_index  = data_clean_freq_matrix.day_flag_index.astype('int').astype('str')\n",
    "data_clean_freq_matrix['day_index'] = data_clean_freq_matrix['day_index']+','+data_clean_freq_matrix['day_flag_index']\n",
    "\n",
    "\n",
    "# Craetion of flexibility using all possible metrics\n",
    "data_matrix['idle_flex_fact'] = data_matrix.Unutilized_gap.rank(method = 'dense', ascending = True)\n",
    "data_matrix['util_flex_fact'] = data_matrix.run_flag.rank(method = 'dense', ascending = False)\n",
    "data_matrix['aval_flex_fact'] = data_matrix['perc_rem_time'].rank(method = 'dense', ascending = True)\n",
    "data_matrix['wash_flex_fact'] = data_matrix['Washing Time'].rank(method = 'dense', ascending = False)\n",
    "data_matrix['DHT_flex_fact'] = data_matrix['Max DHT'].rank(method = 'dense', ascending = True)\n",
    " \n",
    "\n",
    "data_clean_freq_matrix['flex_factor'] = data_clean_freq_matrix['Total Freq'].rank(method = 'dense', ascending = False)\n",
    "data_clean_freq_matrix.drop(['day_flag_index', 'Day Flag'], axis = 1, inplace = True)\n",
    "\n",
    "# Craetion of flexibility factor at CIP group level\n",
    "mc_matrix['res_p_const'] =  mc_matrix['Resource']/mc_matrix['value']\n",
    "mc_matrix['flex_fact'] = mc_matrix.res_p_const.rank(method = 'dense', ascending = False)\n",
    "\n",
    "# Craetion of flexibility factor at Constraint level\n",
    "const_matrix['flex_fact'] = const_matrix.Resource.rank(method = 'dense', ascending = True)\n",
    "const_matrix.rename(columns = {'value': 'Constraint', 'Resource': '#Resource'}, inplace = True)\n"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5032a5c5-0ece-4e2c-9b31-8af2b622b44f",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Final Flexibility factor created\n",
    "- In order to consider every flexibility factor and to take a decision, a final flexiblity factor is very important\n",
    "- Final flexibility factor is going to be the avergae of all the flexibility factor calculated\n",
    "- Dorting the data metrics using the final flexibility factor\n",
    "- The order post sorting says the priority of the resources"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d7fb9cf-986d-464e-8055-9d8b5088f2f8",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creation of final data metrics\n",
    "data_matrix['final_flex_fact'] = (data_matrix['idle_flex_fact']+data_matrix['util_flex_fact']+\n",
    "                                 data_matrix['aval_flex_fact']+data_matrix['wash_flex_fact']+\n",
    "                                 data_matrix['DHT_flex_fact'])/5\n",
    "data_matrix['final_flex_fact'] = data_matrix['final_flex_fact'].rank(method = 'dense', ascending = True)\n",
    "data_matrix = data_matrix.sort_values('final_flex_fact')"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21e93187-4879-4211-9728-4a7453a3309b",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resource ranking\n",
    "1. df_resource1 contains unique no. of MC groups and unique no. of pipes\n",
    "2. total working hours = mx usage time of  datafram - min usage time of dataframe\n",
    "3. df_resource2 -> group by resource and aggregate 'Utilized': ['sum', 'mean'],                                              'Unutilized_gap': 'sum','Washing Time': ['sum', 'mean'],'run_flag': 'sum','Max CHT': 'mean','Max DHT': 'mean'\n",
    "4. '%Utilized'= 'Utilized sum']/total_working_hrs\n",
    "5. '%idle_postclean' = 1- (df_resource2['Washing Time sum']/df_resource2['Unutilized_gap sum'])\n",
    "6. merge resource1 and respource 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Group by to have a dataset with information related to CIP at resource level\n",
    "df_resource1 = df_cip_gconst.groupby('Resource', as_index =False).agg({'MC Group': pd.Series.nunique,\n",
    "                                                             'value': pd.Series.nunique})\n",
    "total_working_hrs = data2.Usage_End.max() - data2.Usage_Start.min()\n",
    "total_working_hrs = total_working_hrs/(np.timedelta64(1, 's')*3600)\n",
    "\n",
    "# Group by to have a dataset with information related to resource and associated metrics\n",
    "df_resource2 = data2.groupby('Resource', as_index = False).agg({'Utilized': ['sum', 'mean'], \n",
    "                                                                'Unutilized_gap': 'sum',\n",
    "                                                                'Washing Time': ['sum', 'mean'],\n",
    "                                                                'run_flag': 'sum', \n",
    "                                                                'Max CHT': 'mean', \n",
    "                                                                'Max DHT': 'mean'})\n",
    "# Handling column mmulti index\n",
    "df_resource2.columns = [' '.join(x).strip() for x in df_resource2.columns]\n",
    "\n",
    "# creation of additional columns which will help segment the groups very well\n",
    "df_resource2['%Utilized'] = df_resource2['Utilized sum']/total_working_hrs\n",
    "df_resource2['%idle_postclean'] = 1- (df_resource2['Washing Time sum']/df_resource2['Unutilized_gap sum'])\n",
    "df_resource2.drop(['Utilized sum', 'Washing Time sum', 'Unutilized_gap sum'], axis = 1, inplace= True)\n",
    "\n",
    "# Merging of the columns to get one final dataset\n",
    "df_resource = pd.merge(df_resource1, df_resource2, how = 'right', on  = 'Resource')"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e15ca883-d954-4c3f-9932-c940c9afb2ea",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Replace all infinite values with 1 under %idle_postclean\n",
    "2. assign priority- if %idle_postclean < 0.6 then 1,elseif <0.85 then 2 else 3 1 highest and 3 being least priority\n",
    "3. assign the resources in parellal_res as priority 1\n",
    "4. flexible are least important hence 1 ia added to max priority value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_resource = df_resource.sort_values('%idle_postclean')\n",
    "df_resource['%idle_postclean'] = df_resource['%idle_postclean'].replace([np.inf, -np.inf], 1)\n",
    "df_resource['Priority'] = df_resource['%idle_postclean'].apply(lambda x: 1 if x < 0.6 else (2 if x < 0.85 else 3))\n",
    "df_resource['Priority'] = np.where(df_resource.Resource.isin(parellal_res), 1, df_resource['Priority'])\n",
    "df_resource.loc[df_resource['Resource'].str.contains('Flexible'), 'Priority'] = df_resource['Priority'].max()+1"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24d65435-9d8b-4e05-9fe2-aa95f0f5f333",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Sort values on basis of matrix\n",
    "2. give reource rank on basis of 'Priority', '%idle_postclean', 'Max DHT mean', 'Max CHT mean', 'value', 'Washing Time mean'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_resource = df_resource.sort_values(['Priority', '%idle_postclean', 'Max DHT mean', 'Max CHT mean', 'value', 'Washing Time mean'], \n",
    "                                        ascending = [True, True, True, True, True, False])\n",
    "\n",
    "df_resource['Resource Rank'] = range(1,df_resource.shape[0]+1)\n",
    "res_order = df_resource.Resource.tolist()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0cfe5f04-78d5-4955-889d-b9dede2aeffa",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Drop priority\n",
    "2. merge the priorities and ranking with main dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data2 = data2.drop('Priority', axis = 1)\n",
    "data2 = pd.merge(data2, df_resource[['Resource', 'Priority', 'Resource Rank']], on = 'Resource', how = 'inner')\n",
    "df_cip_gconst = pd.merge(df_cip_gconst, df_resource[['Resource', 'Priority']], on = 'Resource', how = 'inner')\n",
    "\n",
    "cip_list = ['MC1 - Group', 'MC2 - Group']\n",
    "df_cip_gconst['Group'] = np.where(df_cip_gconst['MC Group'].isin(cip_list), 1, 2)\n",
    "\n",
    "df_cip_gconst_bkp = df_cip_gconst.copy()\n",
    "\n",
    "data_bkp = data2.copy()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8bc41612-a613-4232-a629-2ff34c22c6e7",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cip_avail(df = None, sort_by_col = '', for_col = '', sub_col = 'Clean_End_Time', max_clean = None):\n",
    "    map_columns = ['MC Group', 'Constraint', 'Clean_Start_Time', 'Clean_End_Time']\n",
    "#     df_cip_new = df[map_columns].drop_duplicates().copy()\n",
    "    df_par = df[~df['Parallel Clean Flag'].isnull()]\n",
    "    df_non_par = df[df['Parallel Clean Flag'].isnull()]\n",
    "    df_par = df_par[map_columns].drop_duplicates().copy()\n",
    "    df_non_par = df_non_par[map_columns]\n",
    "    df_cip_new = pd.concat([df_non_par, df_par], axis = 0).reset_index(drop = True)\n",
    "    if for_col=='MC Group':\n",
    "        df_cip_new = pd.concat([df_downtime, df_cip_new])\n",
    "    \n",
    "    df_cip_new['Clean_Start_Time'] = pd.to_datetime(df_cip_new['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    df_cip_new['Clean_End_Time'] = pd.to_datetime(df_cip_new['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    for val in df_cip_new[for_col].unique():\n",
    "        data_temp = df_cip_new[df_cip_new[for_col] == val]\n",
    "        data_temp = data_temp.sort_values(sort_by_col)\n",
    "        data_temp['idle_till_time'] = data_temp[sort_by_col].shift(-1)\n",
    "        data_temp['idle_till_time'] = data_temp['idle_till_time'].fillna(max_clean)\n",
    "        temp_df = pd.concat([temp_df, data_temp])\n",
    "    \n",
    "    temp_df['idle_till_time'] = pd.to_datetime(temp_df['idle_till_time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    temp_df[sub_col] = pd.to_datetime(temp_df[sub_col], format = '%d/%m/%Y %H:%M:%S')\n",
    "    temp_df = temp_df.sort_values([for_col, sort_by_col]).reset_index(drop = True)\n",
    "    temp_df['Available_Time']  = ((temp_df['idle_till_time'] - temp_df[sub_col])/(np.timedelta64(1, 's')*3600))*60\n",
    "    return temp_df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "025052d9-2f96-4af7-9cb0-59b701d3613a",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to get the added column with information related to availability for recleaning and last cleaning time\n",
    "def cleaning_availablity(df = None, sort_by_col = '', for_col = '', sub_col = 'Clean_End_Time', max_clean = None, \n",
    "                         clean_dur = 0, df_avail = None, buffer_mins = 0):\n",
    "    map_columns = [for_col, 'Clean_Start_Time', 'Clean_End_Time']\n",
    "    df_par = df[~df['Parallel Clean Flag'].isnull()]\n",
    "    df_non_par = df[df['Parallel Clean Flag'].isnull()]\n",
    "    df_par = df_par[map_columns].drop_duplicates().copy()\n",
    "    df_non_par = df_non_par[map_columns]\n",
    "    df = pd.concat([df_par, df_non_par])\n",
    "    df_cip = df[map_columns]\n",
    "    df_cip = df[map_columns]\n",
    "    temp_df = df.groupby(for_col, as_index = False).agg({'Clean_Start_Time':'min'})\n",
    "    temp_df['Clean_End_Time'] = min_clean\n",
    "    \n",
    "    if for_col=='MC Group':\n",
    "        df_cip_new = pd.concat([temp_df,df_downtime, df_cip, df_avail])\n",
    "    else:\n",
    "        df_cip_new = pd.concat([temp_df, df_cip, df_avail])\n",
    "    \n",
    "#     df_cip_new = df_cip_new.drop_duplicates()\n",
    "    df_cip_new = df_cip_new[~df_cip_new[for_col].isnull()]\n",
    "    df_cip_new['Clean_Start_Time'] = pd.to_datetime(df_cip_new['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    df_cip_new['Clean_End_Time'] = pd.to_datetime(df_cip_new['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    for val in df_cip_new[for_col].unique():\n",
    "        data_temp = df_cip_new[df_cip_new[for_col] == val]\n",
    "        data_temp = data_temp.sort_values(sort_by_col)\n",
    "        data_temp['idle_till_time'] = data_temp[sort_by_col].shift(-1)\n",
    "        data_temp['idle_till_time'] = data_temp['idle_till_time'].fillna(max_clean)\n",
    "        temp_df = pd.concat([temp_df, data_temp])\n",
    "    temp_df['idle_till_time'] = pd.to_datetime(temp_df['idle_till_time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    temp_df[sub_col] = pd.to_datetime(temp_df[sub_col], format = '%d/%m/%Y %H:%M:%S')\n",
    "    temp_df = temp_df.sort_values([for_col, sort_by_col]).reset_index(drop = True)\n",
    "    temp_df['Available_Time']  = (temp_df['idle_till_time'] - temp_df[sub_col])/(np.timedelta64(1, 's')*3600)\n",
    "    temp_df['Last_pos_Clean'] = temp_df['idle_till_time'] - pd.Timedelta(minutes=clean_dur*60+buffer_mins)\n",
    "    return temp_df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de2f81c2-720b-4dff-93b4-9b4d9666246f",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_cond(df = None, col = None, cleaning_start = None, cleaning_end = None):\n",
    "    wash_time = (cleaning_end - cleaning_start)/(np.timedelta64(1, 's')*3600)\n",
    "    cond1 = (df['Last_pos_Clean'] > cleaning_start)\n",
    "    cond2 = (df['Available_Time'] > wash_time)\n",
    "    cond3 = (df['Clean_End_Time'] < cleaning_start)\n",
    "    final_cond = cond1 & cond2 & cond3\n",
    "    list_out = df[final_cond].sort_values('Available_Time', ascending = False)\n",
    "    return list_out"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b81832a-21b9-4153-b2d5-56e289b7345e",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_overlap(df = None, fin_data= None, cl_st = None, cl_end = None, cip_gr = None, const_gr= None, mins = None):\n",
    "    dict_values = {'Clean_Start_Time': [cl_st], 'Clean_End_Time': [cl_end], 'MC Group': [cip_gr], \n",
    "                    'Constraint': [const_gr], 'Initial Assignment': [1], 'Usage_Start': [None], \n",
    "                    'Usage_End': [None], 'Maint_Flag': [0]}\n",
    "    df = CIP_assignment(df = df.copy(), index = index, main_flag = 0, val_dict = dict_values)\n",
    "    temp_d = df.loc[[index],:].copy()\n",
    "    fin_data = pd.concat([fin_data, temp_d])\n",
    "    df_cip_o= cleaning_availablity(df = fin_data.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                            sub_col =  'Clean_End_Time', for_col = 'MC Group', max_clean = max_clean, \n",
    "                                            clean_dur = washing_time, df_avail = df_cip_avail.copy(), buffer_mins = mins)\n",
    "\n",
    "    df_const_o = cleaning_availablity(df = fin_data.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                            sub_col =  'Clean_End_Time', for_col = 'Constraint', max_clean = max_clean, \n",
    "                                            clean_dur = washing_time, df_avail = df_const_avail.copy(), buffer_mins = 0)\n",
    "    \n",
    "    c_overlap = df_const_o[df_const_o['Available_Time']<0].shape[0]\n",
    "    d_violation = fin_data[fin_data['Last Possible Clean']< fin_data['Clean_Start_Time']].shape[0]\n",
    "    m_overlap = df_cip_o[df_cip_o['Available_Time']<0].shape[0]\n",
    "\n",
    "    if (c_overlap + d_violation + m_overlap) > 0:\n",
    "        cl_st = th_time+pd.Timedelta(minutes = 1)\n",
    "    return cl_st, cl_end"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9fe65450-3ed1-4146-8f5e-ad79fc321578",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def enough_gap_constraint(df = None, mc_group = None, resource = None, mins = 0, \n",
    "                          clean_start_tm = None, clean_end_tm = None, th_time = None):\n",
    "    max_date_df = df[(df['MC Group'] == mc_group) & (df['Resource'] != resource)]\n",
    "    if max_date_df.shape[0]>0:\n",
    "        max_date_df = max_date_df[max_date_df['Clean_End_Time'] <= clean_start_tm]\n",
    "    max_date = max_date_df['Clean_End_Time'].max()\n",
    "    washing_time = (clean_end_tm - clean_start_tm)/(np.timedelta64(1, 's')*60)\n",
    "    if max_date_df.shape[0] > 0:\n",
    "        time_gap = (clean_start_tm - max_date)/(np.timedelta64(1, 's')*60)\n",
    "    else:\n",
    "        time_gap = mins\n",
    "    if time_gap >= mins:\n",
    "        clean_start_tm = clean_start_tm\n",
    "        clean_end_tm = clean_end_tm\n",
    "    else:\n",
    "        clean_start_tm_temp = max_date + pd.Timedelta(minutes = mins)\n",
    "        if clean_start_tm_temp >= th_time:\n",
    "            clean_start_tm = clean_start_tm\n",
    "        else:\n",
    "            clean_start_tm = clean_start_tm_temp\n",
    "        clean_end_tm = clean_start_tm + pd.Timedelta(minutes = washing_time)\n",
    "    return (clean_start_tm, clean_end_tm)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "56833781-76f6-4a8a-9600-982c29ecb581",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_Transfermation1(df = None, clean_end = None, clean_start = None):\n",
    "\n",
    "    df['Clean_Start_Time'] = pd.to_datetime(df['Clean_Start_Time'].fillna(pd.NaT), errors='coerce', \n",
    "                                            format = '%d/%m/%Y %H:%M:%S')\n",
    "    df['Clean_End_Time'] = pd.to_datetime(df['Clean_End_Time'].fillna(pd.NaT), errors='coerce',\n",
    "                                          format = '%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    df['Last Possible Clean'] = pd.to_datetime(df['Last Possible Clean'].fillna(pd.NaT), errors='coerce', \n",
    "                                               format = '%d/%m/%Y %H:%M:%S')\n",
    "    drop_col = ['idle_till_time', 'Last_pos_Clean']\n",
    "\n",
    "    cip_df = cleaning_availablity(df = df.copy(), sort_by_col = 'Clean_Start_Time', sub_col =  'Clean_End_Time', \n",
    "                                  for_col = 'MC Group', max_clean = max_clean, clean_dur = washing_time,\n",
    "                                  df_avail = df_cip_avail.copy(), buffer_mins = buffer/5).drop(drop_col, axis = 1)\n",
    "\n",
    "    const_df = cleaning_availablity(df = df.copy(), sort_by_col = 'Clean_Start_Time', sub_col =  'Clean_End_Time', \n",
    "                                    for_col = 'Constraint', max_clean = max_clean, clean_dur = washing_time,\n",
    "                                    df_avail = df_const_avail.copy(), buffer_mins = 0).drop(drop_col, axis = 1)\n",
    "\n",
    "\n",
    "    cip_df =cip_df.rename(columns = {'Available_Time': 'MC_Available_Time'})\n",
    "    const_df = const_df.rename(columns = {'Available_Time': 'C_Available_Time'})\n",
    "    \n",
    "    df_merged = pd.merge(df, cip_df, on = ['MC Group','Clean_Start_Time', 'Clean_End_Time'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, const_df, on = ['Constraint','Clean_Start_Time', 'Clean_End_Time'], how = 'left')\n",
    "    \n",
    "    df_merged[['MC_Available_Time','C_Available_Time']] = df_merged[['MC_Available_Time','C_Available_Time']]*60 - (washing_time*60)\n",
    "    \n",
    "    df_merged[['MC_Available_Time','C_Available_Time']] = df_merged[['MC_Available_Time','C_Available_Time']].round(2)\n",
    "    \n",
    "    df_merged['Clean_gap'] = (clean_end - df_merged['Clean_Start_Time'])/(np.timedelta64(1, 's')*60)\n",
    "    df_merged['Clean_st_gap'] = (df_merged['Clean_End_Time'] - clean_start)/(np.timedelta64(1, 's')*60)\n",
    "    \n",
    "    df_merged['idle post buffer'] = df_merged['Last Possible Clean'] - df_merged['Clean_Start_Time']\n",
    "    df_merged['idle post buffer'] = df_merged['idle post buffer']/(np.timedelta64(1, 's')*60)\n",
    "    return df_merged"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "166e6235-0ba8-4248-87ae-387d8f63edf6",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_Transfermation(df = None, clean_end = None, clean_start = None):\n",
    "\n",
    "    df['Clean_Start_Time'] = pd.to_datetime(df['Clean_Start_Time'].fillna(pd.NaT), errors='coerce', \n",
    "                                            format = '%d/%m/%Y %H:%M:%S')\n",
    "    df['Clean_End_Time'] = pd.to_datetime(df['Clean_End_Time'].fillna(pd.NaT), errors='coerce',\n",
    "                                          format = '%d/%m/%Y %H:%M:%S')\n",
    "    df['Last Possible Clean'] = pd.to_datetime(df['Last Possible Clean'].fillna(pd.NaT), errors='coerce', \n",
    "                                               format = '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    cip_df = cleaning_availablity(df = df.copy(), sort_by_col = 'Clean_Start_Time', sub_col =  'Clean_End_Time', \n",
    "                                  for_col = 'MC Group', max_clean = max_clean, clean_dur = washing_time,\n",
    "                                  df_avail = df_cip_avail.copy(), buffer_mins = buffer/5)\n",
    "\n",
    "    const_df = cleaning_availablity(df = df.copy(), sort_by_col = 'Clean_Start_Time', sub_col =  'Clean_End_Time', \n",
    "                                    for_col = 'Constraint', max_clean = max_clean, clean_dur = washing_time,\n",
    "                                    df_avail = df_const_avail.copy(), buffer_mins = 0)\n",
    "\n",
    "\n",
    "    cip_df =cip_df.rename(columns = {'idle_till_time':'MC_idle_till_time', 'Available_Time': 'MC_Available_Time',\n",
    "                                                         'Last_pos_Clean': 'MC_Last_pos_Clean'})\n",
    "    df_merged = pd.merge(df, cip_df, on = ['MC Group','Clean_Start_Time', 'Clean_End_Time'], how = 'left')\n",
    "    df_merged['MC_Available_Time'] = df_merged['MC_Available_Time'] - washing_time\n",
    "    df_merged['MC_Available_Time'] = df_merged['MC_Available_Time']*60\n",
    "    df_merged['MC_Available_Time'] = df_merged['MC_Available_Time'].round(2)\n",
    "    df_merged['Clean_gap'] = (clean_end - df_merged['Clean_Start_Time'])/(np.timedelta64(1, 's')*60)\n",
    "    df_merged['Clean_st_gap'] = (df_merged['Clean_End_Time'] - clean_start)/(np.timedelta64(1, 's')*60)\n",
    "    df_merged['idle post buffer'] = df_merged['Last Possible Clean'] - df_merged['Clean_Start_Time']\n",
    "    df_merged['idle post buffer'] = df_merged['idle post buffer']/(np.timedelta64(1, 's')*60)\n",
    "    return df_merged"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d3165449-a9eb-4273-909f-8cb7bb5ab4e8",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_condition(df = None, usage_end = None):\n",
    "    cond1 = (df['Clean_gap']>0) \n",
    "#     | ((df['Clean_gap']>=0) & (df['idle_time']>=1)) \n",
    "    cond2 = (df['idle post buffer'] > df['Clean_gap'])\n",
    "    cond3 = (df['MC_Available_Time'] > 0)\n",
    "    cond4 = (df['Pre_Exist'] != 1)\n",
    "    cond = cond1 & cond2 & cond3 & cond4\n",
    "    df_push = df[cond].sort_values('Clean_gap').reset_index(drop = True)\n",
    "    return df_push"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63484587-7b1f-4e6e-93c1-e968d060a642",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_condition_st(df = None, usage_end = None):\n",
    "    df['clean_dist1'] = (clean_start - df['Clean_Start_Time'])/(np.timedelta64(1, 's')*60)\n",
    "    df['clean_dist'] = df['clean_dist1']+ washing_time*60\n",
    "    cond1 = (df['Clean_st_gap']>0) \n",
    "    cond2 = (df['idle post buffer'] > df['clean_dist'])\n",
    "    cond3 = (df['clean_dist1']>=0)\n",
    "    cond4 = (df['Pre_Exist'] != 1)\n",
    "    cond = cond1 & cond2 & cond3 & cond4\n",
    "    df = df.drop('clean_dist1', axis = 1)\n",
    "    df_push = df[cond].sort_values('clean_dist').reset_index(drop = True)\n",
    "    return df_push"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55e8f18c-cbba-42e2-baac-5e53b39697fb",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_CIP(df = None, for_col = None, element  = '', start_time = None):\n",
    "    \n",
    "    df_push=  df[(df[for_col]==element) & (df['Clean_Start_Time'] >= start_time)].sort_values('Clean_Start_Time')\n",
    "    a = df_push.groupby([for_col, 'Clean_Start_Time', 'Clean_End_Time'], as_index = False)['MC_Available_Time'].min()\n",
    "    b = df_push.groupby([for_col, 'Clean_Start_Time', 'Clean_End_Time'], as_index = False)['idle post buffer'].min()\n",
    "    df_push = df_push.drop(['MC_Available_Time', 'idle post buffer'], axis = 1)\n",
    "    df_push = pd.merge(df_push, a, how = 'left', on = [for_col, 'Clean_Start_Time', 'Clean_End_Time'])\n",
    "    df_push = pd.merge(df_push, b, how = 'left', on = [for_col, 'Clean_Start_Time', 'Clean_End_Time'])\n",
    "    df_push['available_time'] = np.where(df_push['idle post buffer']< df_push['MC_Available_Time'],\n",
    "                                         df_push['idle post buffer'], df_push['MC_Available_Time'])-buffer/5\n",
    "    \n",
    "    df_push['available_time_cs'] = df_push['available_time'].cumsum()\n",
    "    \n",
    "    return df_push"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a784740c-43dc-4d3a-9ba4-bc10eb60d890",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_Setuptime(last_mc = None, last_const = None, start_time = None, clean_start = None, clean_end =None):\n",
    "    buffer_val = buffer/5\n",
    "    max_time = max(last_mc, last_const)\n",
    "    av_time = (clean_start - max_time)/(np.timedelta64(1, 's')*60)\n",
    "    added_time = 0\n",
    "    if av_time < buffer_val:\n",
    "        clean_start = max_time + pd.Timedelta(minutes = buffer_val)\n",
    "        clean_end = clean_start + pd.Timedelta(minutes = washing_time*60)\n",
    "        added_time = buffer_val - av_time\n",
    "    return added_time, clean_start, clean_end"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5834c6b-30b2-4d6d-a546-d727486599f4",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_Updated(df = None, add_min = 0, clean_min = 0):\n",
    "    df_backup = df.copy()\n",
    "    df = df.loc[:df[df['available_time'] >= add_min].index[0]]\n",
    "    df['Clean_Start_Time_New'] = df['Clean_Start_Time']+ pd.Timedelta(minutes = add_min)\n",
    "    df['Clean_End_Time_New'] = df['Clean_End_Time'] + pd.Timedelta(minutes = add_min)\n",
    "    list_par = df['Parallel Clean Flag'].dropna().unique().tolist()\n",
    "    if sum(list_par)>0:\n",
    "        df = pd.concat([df, df_backup[df_backup['Parallel Clean Flag'].isin(list_par)]]).sort_values('Clean_Start_Time')\n",
    "        df[['Clean_Start_Time_New', 'Clean_End_Time_New']] = df[['Clean_Start_Time_New', 'Clean_End_Time_New']].ffill()\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad5a0dd3-77e3-4f7d-af03-cab9efa2906f",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning_final_ds(main_df = None, cip_df = None):\n",
    "    main_df = pd.merge(main_df, cip_df[['Resource', 'LOT', 'Usage_Start', 'Clean_Start_Time_New', 'Clean_End_Time_New', 'clean_start']], \n",
    "                on = ['Resource', 'LOT', 'Usage_Start', 'clean_start'], how = 'left')\n",
    "    main_df['Clean_Start_Time'] = np.where(main_df['Clean_Start_Time_New'].isnull(), main_df['Clean_Start_Time'], \n",
    "                                           main_df['Clean_Start_Time_New'])\n",
    "    main_df['Clean_End_Time'] = np.where(main_df['Clean_End_Time_New'].isnull(), main_df['Clean_End_Time'], \n",
    "                                           main_df['Clean_End_Time_New'])\n",
    "\n",
    "    col_list = main_df.columns.to_list()\n",
    "    if 'MC_Available_Time' in col_list:\n",
    "        col_index = col_list.index('MC_Available_Time') + 1\n",
    "\n",
    "    main_df = main_df.iloc[:,:col_index]\n",
    "    \n",
    "    return main_df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf1e70af-c521-41e0-b844-d0c97ae33720",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Push_Cleaning(df = None, clean_start = None, clean_end = None, washing_time = None, usage_end = None, \n",
    "                  usage_start = None, index_val = 0, last_pos = None):\n",
    "    df = df.sort_values(['Usage_End', 'Usage_Start', 'Clean_Start_Time']).reset_index(drop = True).copy()\n",
    "    df = parallel_suffle(df =  df.copy(), clean_start_time = clean_start)\n",
    "    df1 = Push_Cleaning_Transfermation1(df = df.copy(), clean_end = clean_end, clean_start = clean_start)\n",
    "    last_index = df1[(df1['clean_start']==clean_start) & (df1['clean_end']==clean_end) & (df1['Resource']==res)].index[-1]\n",
    "#     last_index = df1.index[-1]\n",
    "    df_clean_push = Push_Cleaning_condition(df = df1.copy(), usage_end = usage_end)\n",
    "    df_backup = df1.copy(); cl_start = clean_start; cl_end = clean_end\n",
    "    max_clean_pos = max_clean; min_clean_pos = min_clean; flag = 1\n",
    "    \n",
    "    for i in range(df_clean_push.shape[0]):\n",
    "        df1 = df_backup.copy()\n",
    "        try:\n",
    "            df_clean_pushi = df_clean_push.iloc[i]  \n",
    "            push_clean_start = df_clean_pushi['Clean_Start_Time'];   clean_gap = df_clean_pushi['Clean_gap']\n",
    "            push_mc =  df_clean_pushi['MC Group'];      #push_const = df_clean_pushi['Constraint']\n",
    "            push_mc_list = df_cip_gconst[df_cip_gconst['Resource'] ==res]['MC Group'].tolist()\n",
    "            if push_mc not in push_mc_list:\n",
    "                continue\n",
    "            push_const_list = df_cip_gconst[(df_cip_gconst['Resource'] ==res) & \n",
    "                                            (df_cip_gconst['MC Group'] ==push_mc)]['Constraint'].tolist()\n",
    "            df_const_push_temp = df1.groupby('Constraint', as_index = False)['Clean_End_Time'].max()\n",
    "            df_const_push_temp = df_const_push_temp[df_const_push_temp['Constraint'].isin(push_const_list)]\n",
    "            if df_const_push_temp.shape[0] == 0:\n",
    "                push_const = push_const_list[0]\n",
    "                min_end = min_clean_pos\n",
    "                df1['C_Available_Time'] = (max_clean_pos - min_end)/(np.timedelta64(1, 's')*60) - (washing_time*60)\n",
    "            else:\n",
    "                min_end = df_const_push_temp['Clean_End_Time'].min()\n",
    "                push_const = df_const_push_temp[df_const_push_temp['Clean_End_Time']==min_end]['Constraint'].tolist()[0]\n",
    "\n",
    "#             df1['C_Available_Time'] = (max_clean_pos - min_end)/(np.timedelta64(1, 's')*60) - (washing_time*60)\n",
    "            df1['MC_Available_Time'] = np.where(df1['C_Available_Time']<df1['MC_Available_Time'], \n",
    "                                               df1['C_Available_Time'], df1['MC_Available_Time'])\n",
    "\n",
    "\n",
    "            last_used_mc = df1[(df1['MC Group']==push_mc) & (df1['Clean_End_Time'] <= push_clean_start)]['Clean_End_Time'].max()\n",
    "            last_used_const = df1[(df1['Constraint']==push_const) & (df1['Clean_End_Time'] <= push_clean_start)]['Clean_End_Time'].max()\n",
    "\n",
    "            df_push_mc = Push_Cleaning_CIP(df = df1.copy(), for_col = 'MC Group', \n",
    "                                           element  = push_mc, start_time = push_clean_start)\n",
    "            df_push_const = Push_Cleaning_CIP(df = df1.copy(), for_col = 'Constraint', \n",
    "                                               element  = push_const, start_time = push_clean_start)\n",
    "            if df_push_const.empty:\n",
    "                df_push_const = df_push_mc.copy()\n",
    "            \n",
    "            added_time = Push_Cleaning_Setuptime(last_mc = last_used_mc, last_const = last_used_const, start_time = push_clean_start,\n",
    "                                        clean_start = cl_start, clean_end = cl_end)\n",
    "            adjust_needed = clean_gap + added_time[0] + (buffer/5)\n",
    "            clean_start = added_time[1]\n",
    "            clean_end = added_time[2]\n",
    "            if clean_start > last_pos:\n",
    "                continue\n",
    "#             df_push_mc = Push_Cleaning_Updated(df = df_push_mc.copy(), add_min = adjust_needed, clean_min = washing_time)\n",
    "#             df_push_const = Push_Cleaning_Updated(df = df_push_const.copy(), add_min = adjust_needed, clean_min = washing_time)\n",
    "            df_push_mc = consecutive_push(df_push = df_push_mc, df_main = df1.copy(), focus = 'MC Group', gap = adjust_needed,\n",
    "                                          push_mc = push_mc, push_const = push_const)\n",
    "            df_push_const = consecutive_push(df_push = df_push_const,  df_main = df1.copy(), focus = 'Constraint',\n",
    "                                             gap = adjust_needed, push_mc = push_mc, push_const = push_const)\n",
    "\n",
    "            df = Push_Cleaning_final_ds(main_df = df1, cip_df = df_push_mc)\n",
    "            df = Push_Cleaning_final_ds(main_df = df, cip_df = df_push_const)\n",
    "            \n",
    "            df = df.drop('MC_Available_Time', axis = 1)\n",
    "\n",
    "            df.loc[last_index, 'Clean_Start_Time'] = added_time[1]\n",
    "            df['Clean_Start_Time'] = pd.to_datetime(df['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "            df.loc[last_index, 'Clean_End_Time'] = added_time[2]\n",
    "            df['Clean_End_Time'] = pd.to_datetime(df['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "            df.loc[last_index, 'MC Group'] = push_mc\n",
    "            df.loc[last_index, 'Constraint'] = push_const\n",
    "            df_cipc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "                          sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "            df_constc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "                          sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "            const_overlap = df_constc[df_constc['Available_Time']<0].shape[0]\n",
    "            dht_violation = df[df['Last Possible Clean']< df['Clean_Start_Time']].shape[0]\n",
    "            mc_overlap = df_cipc[df_cipc['Available_Time']<0].shape[0]\n",
    "        \n",
    "            if (const_overlap + dht_violation + mc_overlap) > 0:\n",
    "                continue\n",
    "            flag = 0\n",
    "            break\n",
    "        except:\n",
    "            flag = 1\n",
    "            continue\n",
    "    if flag ==1:\n",
    "        raise ValueError('A very specific bad thing happened.')\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7707c21-3bff-4a94-b98e-952199023a20",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Utilization_Shifter(df = None):   \n",
    "    cond1 = (df['CHT Violation Flag Pre Clean']== 1)\n",
    "    cond2 = (df['Last CHT'] > df['Max CHT'])\n",
    "\n",
    "    cond3 = (df['Resource'].isin(['615', '616', '617', 'I/N 1619']))   \n",
    "\n",
    "    cond4 = (df['Type'] == '2nd Susp. Buffer')\n",
    "    cond5 = (df['Resource'].isin(['I/N 2667', 'I/N 2668']))\n",
    "    diff = df['Last CHT']- (24 -(df['Usage_End'] - df['Usage_Start'])/(np.timedelta64(1, 's')*3600))\n",
    "    cond6 = (diff < df['Max CHT'])\n",
    "    \n",
    "    df['Utilization Shifted'] = 0\n",
    "    df_filt  = df[(cond1 & cond2 & cond6) & (cond3 | (cond4 & cond5))]\n",
    "    df = df[~((cond1 & cond2 & cond6) & (cond3 | (cond4 & cond5)))]\n",
    "    \n",
    "    df_filt['Utilization Shifted'] = 1\n",
    "\n",
    "    df_filt['Usage_Start'] = df_filt['Usage_Start'] - pd.to_timedelta((df_filt['Last CHT'] - df_filt['Max CHT'])*3600, unit='s')\n",
    "    df_filt['Usage_Start'] = df_filt['Usage_Start'] - pd.Timedelta(minutes = 10)\n",
    "    df = pd.concat([df, df_filt]).sort_values(['Usage_End', 'Clean_Start_Time']).reset_index(drop = True)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1847c5c-4086-4bbf-8822-cd2c2f74d588",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parallel_suffle(df =  None, clean_start_time = None):\n",
    "    df_par_iden = df[(~df['Parallel Clean Flag'].isnull())].sort_values('Parallel Clean Flag', ascending = False)\n",
    "    for par in df_par_iden['Parallel Clean Flag'].unique():\n",
    "        df_par_ds = df[df['Parallel Clean Flag']==par].sort_values('Last Possible Clean')\n",
    "        index_val = df_par_ds.index\n",
    "        df_sel_pc = df_par_ds.loc[index_val[0]]\n",
    "#         df.loc[index_val[1], 'Parallel Clean Flag'] = np.nan\n",
    "\n",
    "        pc_option = df_par_clean[df_par_clean['Process resource']==df_sel_pc['Resource']]['PC Option'].tolist()\n",
    "\n",
    "        df_pos_par = df[(df[\"Resource\"].str.startswith('Flexible')) & (df['Parallel Clean Flag'].isnull()) &\n",
    "                         (df['Clean_Start_Time']> df_sel_pc['Usage_End']) & (df['Resource'].isin(pc_option)) & \n",
    "                         (df['Clean_Start_Time']<= df_sel_pc['Last Possible Clean'])]\n",
    "        \n",
    "        if df_pos_par.shape[0]>=1:\n",
    "            df.loc[index_val[1], 'Parallel Clean Flag'] = np.nan\n",
    "\n",
    "            new_index_val = df_pos_par.sort_values('Clean_Start_Time').index    \n",
    "            df.loc[new_index_val[0], 'Parallel Clean Flag'] = par\n",
    "            df_pos_par.loc[new_index_val[0], 'Parallel Clean Flag'] = par\n",
    "            df_pos_par = df_pos_par.iloc[0]\n",
    "\n",
    "            col_list = ['Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Constraint', 'Parallel Clean Flag']\n",
    "            for col in col_list:\n",
    "                df.loc[index_val[0], col] = df_pos_par[col]\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13ff3c6a-b9fa-476d-b22e-072b3b8f0e03",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def consecutive_push(df_push = None, df_main = None, focus = 'MC Group', gap = 0, push_mc = None, push_const = None):\n",
    "    c_avail = df_push['C_Available_Time'][0]\n",
    "    mc_avail = df_push['MC_Available_Time'][0]\n",
    "    df_push['Clean_Start_Time_New'] = np.nan; df_push['Clean_End_Time_New'] = np.nan; df_temp = pd.DataFrame()\n",
    "    if (c_avail>=gap) & (mc_avail>=gap):\n",
    "        print('Life Sorted')\n",
    "        df_push.loc[0,'Clean_Start_Time_New'] = df_push['Clean_Start_Time'][0]  + pd.Timedelta(minutes = gap)\n",
    "        df_push.loc[0,'Clean_End_Time_New'] = df_push['Clean_End_Time'][0]  + pd.Timedelta(minutes = gap)\n",
    "\n",
    "    else:\n",
    "        for push_index in range(df_push.shape[0]):\n",
    "\n",
    "            c_avail = df_push['C_Available_Time'][push_index]\n",
    "            mc_avail = df_push['MC_Available_Time'][push_index]\n",
    "\n",
    "            if focus == 'MC Group':\n",
    "                element_focus = mc_avail; element_alter = c_avail; push_el = df_push['Constraint'][push_index]\n",
    "                el_foc = push_const\n",
    "            else:\n",
    "                element_focus = c_avail; element_alter = mc_avail; push_el = df_push['MC Group'][push_index]\n",
    "                el_foc = push_mc\n",
    "            print(push_el)\n",
    "            df_push.loc[push_index,'Clean_Start_Time_New'] = df_push['Clean_Start_Time'][push_index]  + pd.Timedelta(minutes = gap)\n",
    "            df_push.loc[push_index,'Clean_End_Time_New'] = df_push['Clean_End_Time'][push_index]  + pd.Timedelta(minutes = gap)\n",
    "\n",
    "            if (element_focus >= gap) &  (element_alter >= gap):\n",
    "                df_push = df_push.iloc[:push_index+1, :]\n",
    "                break\n",
    "            elif (element_alter < gap) & (push_el != el_foc):\n",
    "                df_temp = df1[(df1['Constraint'] == df_push['Constraint'][push_index]) & \n",
    "                              (df1['Clean_Start_Time'] > df_push['Clean_Start_Time'][push_index])].sort_values('Clean_Start_Time')\n",
    "                df_temp['Clean_Start_Time_New'] = df_temp['Clean_Start_Time'] + pd.Timedelta(minutes = gap)\n",
    "                df_temp['Clean_End_Time_New'] = df_temp['Clean_End_Time'] + pd.Timedelta(minutes = gap)\n",
    "            df_push = pd.concat([df_push, df_temp])\n",
    "    df_push['Clean_Start_Time_New'] = pd.to_datetime(df_push['Clean_Start_Time_New'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    df_push['Clean_End_Time_New'] = pd.to_datetime(df_push['Clean_End_Time_New'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    return df_push"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61b2f82d-98bf-4c79-9e47-989cb3e563f9",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recleaning\n",
    "1. Sort the datframe on basis of resource, usage_start, cleaing type\n",
    "- sorting in decending manner with Cleaning type will help to get  the freezed records first and unallocated one at the end.\n",
    "2. Create a blank dataframe df_new\n",
    "3. Loop all unique resources\n",
    "4. df_temp = subset of dataframe where resource name = res\n",
    "5. Prev_last_pos = last_possible_cleaning of previous utilization and fill 1st record with minimum clean time.\n",
    "6. Last_pos_clean = Clean start time of previous record(except freezed data this value will be null)\n",
    "7. LastPos_clean = if Last_pos_clean is null then Prev_Last_Pos else LastPos_clean.\n",
    "- in case we have a clean start time of previous utilization take that else pick the last possible cleaning \n",
    "- in case we have a freezed data we'll get a clean start time\n",
    "8. Time Gap = Usage_Start - Last_Pos_Clean\n",
    "- time gap will provide us the time gap between the utilization start and previous utilization's cleaning start time\n",
    "- we'll use this info. later to compare with CHT time, which will give us an idea if we'll violate max CHT\n",
    "9. Deduct washing time from time gap as\n",
    "10. Concatinate with df_new\n",
    "11. come out of loop\n",
    "12. Pre clean_start = usage_start - max_CHT - washing time\n",
    "13.  Pre clean_start if Pre_clean_start > last possible clean the pre clean start else last pos clean\n",
    "- this is to make sure after a reclean max CHT time is not violated\n",
    "14. create rec_iden_df--> recleaning identifying df if Time Gap > max CHT and Cleaning Type not equal to pre cleaning\n",
    "15. Take records where clean start time is null. This means we are removing all freez data\n",
    "16. Assign clean type as recleaning\n",
    "17. Last possible clean - usage start - washing time\n",
    "18. clean start = pre clean start\n",
    "19. clean end = clean start = washing\n",
    "20. concatinate this data frame with df_new\n",
    "- This means addtional rowas are added for recleanings\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def include_recleans(df= None):\n",
    "    df = df.sort_values(['Resource','Usage_Start', 'Cleaning Type'],  ascending=[True,True, False]).reset_index(drop = True)\n",
    "    df_new = pd.DataFrame()\n",
    "    for res in df['Resource'].unique():\n",
    "        df_temp = df[df['Resource'] == res]\n",
    "        df_temp['Prev_Last_Pos'] = df_temp['Last Possible Clean'].shift(1).fillna(min_clean)\n",
    "        df_temp['Last_Pos_Clean'] = df_temp['Clean_Start_Time'].shift(1)\n",
    "        df_temp['Last_Pos_Clean'] = np.where(df_temp['Last_Pos_Clean'].isnull(), df_temp['Prev_Last_Pos'],\n",
    "                                            df_temp['Last_Pos_Clean'])\n",
    "        df_temp['Time Gap'] = df_temp['Usage_Start'] - df_temp['Last_Pos_Clean']\n",
    "        df_temp['Time Gap'] = df_temp['Time Gap']/(np.timedelta64(1, 's')*3600)\n",
    "#         df_temp = df_temp[df_temp['Cleaning Type'] != 'q-Pre Cleaning']\n",
    "        df_temp['Time Gap'] = df_temp['Time Gap'] - df_temp['Washing Time']\n",
    "        df_new = pd.concat([df_new, df_temp]).reset_index(drop = True)\n",
    "        \n",
    "    \n",
    "    df_new['Pre Clean Start'] = df_new['Max CHT']+ df_new['Washing Time']\n",
    "    df_new['Pre Clean Start'] = df_new['Usage_Start'] - pd.to_timedelta(df_new['Pre Clean Start'], unit='h')\n",
    "    df_new['Pre Clean Start'] = pd.to_datetime(df_new['Pre Clean Start'], format = '%d/%m/%Y %H:%M:%S') \n",
    "    df_new['Pre Clean Start'] = np.where(df_new['Pre Clean Start'] > df_new['Last_Pos_Clean'], \n",
    "                                         df_new['Pre Clean Start'], df_new['Last_Pos_Clean'])\n",
    "    \n",
    "    rc_iden_df = df_new[(df_new['Time Gap'] > df_new['Max CHT']) & (df_new['Cleaning Type'] != 'q-Pre Cleaning')]\n",
    "    rc_iden_df = rc_iden_df[rc_iden_df['Clean_Start_Time'].isnull()]\n",
    "    rc_iden_df['Cleaning Type'] = 'Reclean'\n",
    "    rc_iden_df['Last Possible Clean'] = rc_iden_df['Usage_Start'] - pd.to_timedelta(rc_iden_df['Washing Time']*3600, unit='s')\n",
    "    rc_iden_df['clean_start'] = rc_iden_df['Pre Clean Start']\n",
    "    rc_iden_df['clean_end'] = rc_iden_df['clean_start']+pd.to_timedelta(rc_iden_df['Washing Time']*3600, unit='s')\n",
    "    df_new = pd.concat([df_new, rc_iden_df]).reset_index(drop = True)\n",
    "    return df_new.copy()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c2a7316-d6aa-45dc-afc1-738dc6e7f5f0",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_overlaps(df = None):\n",
    "\n",
    "    df_cipc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "                  sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "    df_constc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "                  sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "    const_overlap = df_constc[df_constc['Available_Time']<0].shape[0]\n",
    "    dht_violation = df[df['Last Possible Clean']< df['Clean_Start_Time']].shape[0]\n",
    "    mc_overlap = df_cipc[df_cipc['Available_Time']<0].shape[0]\n",
    "\n",
    "    return const_overlap, dht_violation, mc_overlap"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04fc3166-4497-4375-9dd5-e8ec1eec15dc",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cip_positions(df = None, var = None):\n",
    "    df_mis = df[df[var].isnull()]\n",
    "    df = df[~df[var].isnull()]\n",
    "    print(df_mis.shape)\n",
    "    temp_df = pd.DataFrame()\n",
    "    for mc in df[var].unique():\n",
    "        df_mc = df[df[var]==mc].sort_values('Clean_End_Time')\n",
    "        df_mc['Last_'+var+'_End'] = df_mc['Clean_End_Time'].shift(1).fillna(min_clean)\n",
    "        df_mc['Next_'+var+'_Start'] = df_mc['Clean_Start_Time'].shift(-1).fillna(max_clean)\n",
    "        temp_df = pd.concat([temp_df, df_mc], axis = 0)\n",
    "    df = pd.concat([temp_df, df_mis])\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "449fab63-a453-48e5-8e15-ca5438a5a434",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_missing_parallel(df = None, count = None):\n",
    "    df_flex = df[(df['Resource'].str.contains('Flex')) & (df['Parallel Clean Flag'].isnull())]\n",
    "    df_flex_miss = df_flex[df_flex['MC Group'].isnull()].sort_values('Last Possible Clean')\n",
    "    for flex_ind, flex_row in df_flex_miss.iterrows():\n",
    "        clean_start = flex_row['clean_start']\n",
    "        clean_end = flex_row['clean_end']\n",
    "        last_poss = flex_row['Last Possible Clean']\n",
    "        flex_res = flex_row['Resource']\n",
    "        \n",
    "        pc_option = df_par_clean[df_par_clean['Process resource']==flex_res]['PC Option'].tolist()\n",
    "        \n",
    "        df_pos = df_flex[(~df_flex['MC Group'].isnull()) & \n",
    "                         (df_flex['Clean_Start_Time']< last_poss) &\n",
    "                         (df_flex['Clean_Start_Time']> clean_start) &\n",
    "                         (df_flex['Pre_Exist']!=1) &\n",
    "                         (df_flex['Resource'].isin(pc_option)) &\n",
    "                         (df_flex['Parallel Clean Flag'].isnull())].sort_values('Last Possible Clean')\n",
    "#         print(flex_res, df_pos.shape)\n",
    "        if df_pos.shape[0]>0:\n",
    "            count = count + 1\n",
    "            print('Resource:', flex_res, 'Count:', df_pos.shape[0], 'Par Count', count)\n",
    "            index_list = df_pos.index\n",
    "#             print(df_pos)\n",
    "            df.loc[index_list[0], 'Parallel Clean Flag'] = count\n",
    "            df.loc[flex_ind, 'Parallel Clean Flag'] = count\n",
    "            df.loc[flex_ind, 'Clean_Start_Time'] = df_pos['Clean_Start_Time'][index_list[0]]\n",
    "            df.loc[flex_ind, 'Clean_End_Time'] = df_pos['Clean_End_Time'][index_list[0]]\n",
    "            df.loc[flex_ind, 'MC Group'] = df_pos['MC Group'][index_list[0]]\n",
    "            df.loc[flex_ind, 'Constraint'] = df_pos['Constraint'][index_list[0]]\n",
    "            df_flex = df[(df['Resource'].str.contains('Flex')) & (df['Parallel Clean Flag'].isnull())]\n",
    "    return df, count"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e0f1427-1ee9-4271-9724-3694f2d7056c",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def move_schedule_options(df = None, excl_res = [], mising_rec = None):\n",
    "    clean_start = mising_rec['clean_start']\n",
    "    last_possible = mising_rec['Last Possible Clean']\n",
    "\n",
    "    df = cip_positions(df = df.copy(), var = 'MC Group')\n",
    "    df = cip_positions(df = df.copy(), var = 'Constraint')\n",
    "\n",
    "    df_move = df[(df['Clean_Start_Time']>=clean_start) & (df['Clean_Start_Time']<=last_possible) \n",
    "                 & (~df['Resource'].isin(excl_res)) \n",
    "                 & (df['Pre_Exist']!= 1) \n",
    "                 & (df['Parallel Clean Flag'].isnull())\n",
    "                ]\n",
    "\n",
    "    df_move['Cleaning Duration'] = df_move['Washing Time']*60\n",
    "    df_move['Available Time'] = (df_move['Last Possible Clean'] - df_move['Clean_Start_Time'])/(np.timedelta64(1, 's')*60)\n",
    "\n",
    "    df_move['MC Order'] = df_move['MC Group'].replace({'MC4 - Group': 1, 'MC1 - Group':1, 'MC3 - Group':3, 'MC2 - Group':3})\n",
    "    df_move = df_move.sort_values(['MC Order', 'Available Time'], ascending = [True, False]) \n",
    "    return df_move"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8a8533e-d29f-4deb-b1c2-4821f894e3a1",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def constrain_basedon_mc(mc_group = None):\n",
    "    if (use_mc == 'MC4 - Group') | (use_mc == 'MC1 - Group'):\n",
    "        useable_const = ['V', 'E']\n",
    "    elif (use_mc == 'MC2 - Group') | (use_mc == 'MC3 - Group'):\n",
    "        useable_const = ['F']\n",
    "    return useable_const"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d52f1048-df35-4d90-9b1a-82ba9feb93cc",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def final_constraint_use(constraint_list = [], df = None, move_clean_st = None):\n",
    "    early_avail = []\n",
    "    for const in constraint_list:\n",
    "        df_last_used = df[(df['Constraint']==const) & (df['Clean_End_Time']<move_clean_start)]\n",
    "        max_last_used = df_last_used['Clean_End_Time'].max()\n",
    "        early_avail.append(max_last_used)\n",
    "    last_const_used = min(early_avail)\n",
    "    logical_min = early_avail.index(last_const_used)\n",
    "    final_const = constraint_list[logical_min]\n",
    "    return final_const, last_const_used"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3109ff0-d0b9-492c-9d2c-a5f473b3b8b5",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mc_schedule_moving(df = None, clean_start_move= None, move_res = None, mc = None):\n",
    "    df_cipc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group',\n",
    "                        sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "    df_cipc = df_cipc[df_cipc['Available_Time']<0]\n",
    "    \n",
    "    move_add_time = abs(df_cipc.iloc[0]['Available_Time'])\n",
    "    move_row_index = df[(df['MC Group']==mc) & \n",
    "                        (df['Clean_Start_Time'] == clean_start_move) &\n",
    "                        (df['Resource']==move_res)].index[0]\n",
    "    clean_end_cip = df_cipc.iloc[0]['Clean_End_Time']\n",
    "    \n",
    "    washing_time = df.loc[move_row_index]['Washing Time']\n",
    "    clean_start_move = clean_end_cip + pd.Timedelta(minutes = 1)\n",
    "    clean_end_move = clean_start_move + pd.Timedelta(minutes = washing_time*60)\n",
    "    \n",
    "    next_usage = df.loc[move_row_index]['Next_MC Group_Start']\n",
    "    next_move_index = df[(df['MC Group']==mc) & \n",
    "                         (df['Clean_Start_Time'] == next_usage)].index[0]\n",
    "    if df.loc[move_row_index]['Parallel Clean Flag'] >0:\n",
    "        pc_num = df.loc[move_row_index]['Parallel Clean Flag']\n",
    "        move_row_index = df[df['Parallel Clean Flag']==pc_num].index.tolist()\n",
    "    df.loc[move_row_index, ['Clean_Start_Time', 'Clean_End_Time']] = [clean_start_move, clean_end_move]\n",
    "    df = df.loc[:, :'New Last Possible']\n",
    "    df = cip_positions(df = df.copy(), var = 'MC Group')\n",
    "    df = cip_positions(df = df.copy(), var = 'Constraint')\n",
    "    return df, next_move_index"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "614f38bd-8ee3-4024-8753-2c2f98d1d6fd",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def const_schedule_moving(df = None):\n",
    "    df_constc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "                      sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "    df_constc = df_constc[df_constc['Available_Time']<0]\n",
    "    move_constc = df_constc.iloc[0]['Constraint']\n",
    "    rec_end = df_constc.iloc[0]['Clean_End_Time']\n",
    "    move_mc = df_constc.iloc[0]['MC Group']\n",
    "    rec_start = df_constc.iloc[0]['Clean_Start_Time']\n",
    "#     rec_res = df[(df['Clean_End_Time']==rec_end) & (df['Clean_Start_Time']==rec_start) & \n",
    "#                  (df['MC Group']==move_mc) & (df['Constraint']==move_constc)].iloc[0]['Resource']\n",
    "    \n",
    "    move_add_time = abs(df_constc.iloc[0]['Available_Time'])\n",
    "#     move_row_index = df[(df['Constraint']==move_constc) & \n",
    "#                         (df['Last_Constraint_End'] == rec_end) & \n",
    "#                         (df['Resource'] != rec_res)].index[0]\n",
    "    df_rec = df[(df['Clean_End_Time']==rec_end) & (df['Clean_Start_Time']==rec_start) & \n",
    "                 (df['MC Group']==move_mc) & (df['Constraint']==move_constc)]\n",
    "    rec_res = df_rec.iloc[0]['Resource']\n",
    "    rec_index = df_rec.index[0]\n",
    "    move_row_df = df[(df['Constraint']==move_constc) & (df['Clean_Start_Time']== rec_end - pd.Timedelta(minutes = move_add_time))]\n",
    "    move_row_index = [i for i in move_row_df.index if i != rec_index][0]\n",
    "    clean_end_cip = rec_end\n",
    "    washing_time = df.loc[move_row_index]['Washing Time']\n",
    "    clean_start_move = clean_end_cip + pd.Timedelta(minutes = 1)\n",
    "    clean_end_move = clean_start_move + pd.Timedelta(minutes = washing_time*60)\n",
    "\n",
    "#     next_usage = df.loc[move_row_index]['Next_Constraint_Start']\n",
    "#     next_move_index = df[(df['Constraint']==move_constc) & \n",
    "#                          (df['Clean_Start_Time'] == next_usage)].index[0]\n",
    "    if df.loc[move_row_index]['Parallel Clean Flag'] > 0:\n",
    "        pc_num = df.loc[move_row_index]['Parallel Clean Flag']\n",
    "        move_row_index = df[df['Parallel Clean Flag']==pc_num].index.tolist()\n",
    "\n",
    "    df.loc[move_row_index, ['Clean_Start_Time', 'Clean_End_Time']] = [clean_start_move, clean_end_move]\n",
    "    df = df.loc[:, :'New Last Possible']\n",
    "    df = cip_positions(df = df.copy(), var = 'MC Group')\n",
    "    df = cip_positions(df = df.copy(), var = 'Constraint')\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3985dd30-7147-4fa9-9468-61d498d87721",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating df_const_avail and df_cip_avail\n",
    "1. Sort value of data_bkp on basis of Priority, Resource Rank, Usage_End and store it in data.\n",
    "- This will help us to sort our resources, higher priority will appear first\n",
    "2. From df_cip_gconst_bkp drop slno as not in use.\n",
    "3. Rename column Value to Constraint in df_cip_gconst\n",
    "4. Create a blank dataframe final_df which columns from data dataframe\n",
    "5. min_clean = data.usage_end.min()\n",
    "- min start time of cleaning\n",
    "6. max_clean = max of clean end time\n",
    "7. max_dht_clean = take the max DHT of the record where clean_end = max_clean\n",
    "- As the cleaning can be scheduled withing the maxt DHT available for last utilization, dht is taken into consideration\n",
    "8. 15 hours is added to max_clean to have a safe bandwidth of max_clean\n",
    "9. Static time is just for reference\n",
    "10. df_cip_avail -- dataframe containing unique resources\n",
    "11. df_const_avail --- contains unique constraints\n",
    "12. df_cip_avail, df_const_avail dataframes will contain Clean_Start_Time, Clean_End_Time, idle_till_time and Available_Time\n",
    "- Clean_start_time initially will contain max_clean values\n",
    "- Clean_End_Time = min_clean\n",
    "- Available_Time = max_clean - min_clean\n",
    "- this dataframes will get updated after each allocation of cleaning to utilizations.\n",
    "\n",
    "\n",
    "# visit later\n",
    "new record inserted:\n",
    "new record ideal time = previous record clean start time\n",
    "new record clean start time = utilization's clean start time\n",
    "new record clean end time = utilization's clean end time\n",
    "\n",
    "\n",
    "DOUBT\n",
    "what about the clean end time we kept earlier dont we require that ideal time??"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creation of blanl lists to be used while checking the constraints\n",
    "# data = data_bkp.sort_values(['Priority', 'Usage_End']).copy()\n",
    "data = data_bkp.sort_values(['Priority', 'Resource Rank', 'Usage_End']).reset_index(drop = True).copy()\n",
    "df_cip_gconst = df_cip_gconst_bkp.drop('slno', axis = 1).drop_duplicates().copy()\n",
    "df_cip_gconst = df_cip_gconst.rename(columns = {'value': 'Constraint'})\n",
    "final_df = pd.DataFrame(columns = data.columns)\n",
    "\n",
    "# Some blank lists might be used in the process\n",
    "used_res = []; used_const1 = []; used_mc1 = []; mc_avail = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "################# change 06/12/21"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_clean = min(data.Clean_Start_Time.min(),data.Usage_Start.min())\n",
    "if len(df_qcip)==0:\n",
    "    min_clean = data.Usage_Start.min()\n",
    "else:\n",
    "    min_clean = min(data.Clean_Start_Time.min(),data.Usage_Start.min())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#min_clean = data.Usage_Start.min()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "################# change 06/12/21 end"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#min_clean=  pd.to_datetime('05/05/2022 15:48:00', format = '%d/%m/%Y %H:%M:%S')\n",
    "max_clean = data.clean_end.max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cleaning to be given\n",
    "# min_clean = data.Usage_Start.min()\n",
    "#min_clean = data.Usage_Start.min()\n",
    "max_clean = data.clean_end.max()\n",
    "\n",
    "max_dht_clean = data[data['clean_end'] == max_clean].reset_index(drop = True)['Max DHT'][0]\n",
    "\n",
    "max_clean = max_clean + pd.Timedelta(minutes=15*60)\n",
    "\n",
    "\n",
    "# min_cht_clean = data[data['Usage_Start']==min_clean].iloc[0]['Max CHT']\n",
    "# min_clean = min_clean - pd.Timedelta(minutes=min_cht_clean*60)\n",
    "\n",
    "static_date = np.datetime64('2000-01-01'); count = 0\n",
    "\n",
    "df_cip_avail = df_cip_gconst[['MC Group']].drop_duplicates()\n",
    "df_const_avail = df_cip_gconst[['Constraint']].drop_duplicates()\n",
    "df_cip_avail['Clean_Start_Time']= df_const_avail['Clean_Start_Time'] = max_clean\n",
    "df_cip_avail['Clean_End_Time'] = df_const_avail['Clean_End_Time'] = min_clean\n",
    "df_cip_avail['idle_till_time'] = df_const_avail['idle_till_time']=  max_clean\n",
    "df_cip_avail['Available_Time'] = (max_clean - min_clean)/(np.timedelta64(1, 's')*3600)\n",
    "df_const_avail['Available_Time'] = (max_clean - min_clean)/(np.timedelta64(1, 's')*3600)\n",
    "# df_cip_avail['Last_pos_Clean'] = df_cip_avail['idle_till_time'] - pd.Timedelta(minutes=clean_dur*60+10)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3bec4c53-4795-4330-9a21-65c407e273e3",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#######new change start 3/12/2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df_downtime_test = df_downtime.copy()\n",
    "#df_downtime_test['Clean_Start_Time'] = pd.to_datetime(df_downtime_test['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "#df_downtime_test['Clean_End_Time'] = pd.to_datetime(df_downtime_test['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "correct_df =df_downtime[df_downtime['Clean_End_Time']< max_clean]\n",
    "incorrect_df = df_downtime[df_downtime['Clean_End_Time']> max_clean]\n",
    "incorrect_df['Clean_End_Time']=max_clean\n",
    "df_downtime = pd.concat([correct_df,incorrect_df])\n",
    "# df_downtime_test['Clean_End_Time'] =  np.where(df_downtime_test['Clean_End_Time']> max_clean , max_clean , pd.to_datetime(df_downtime_test['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S'))\n",
    "\n",
    "#for index , row in df_downtime_test.iterrows():\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##########new change end 3/12/2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PC flag\n",
    "1. Create a column pc_flag in data dataframe containing 1 if resourse is present in par_cl_res else 0\n",
    "- this flag is to indentify ulizations/resources which can have a parallel cleaning.\n",
    "2. data_par dataframe contains prallel resources where MC group is null.\n",
    "- This will be used in parallel cleaning optimization\n",
    "3. create data_flex data frame containing all flexible resources\n",
    "4. Remove these from data dataframe\n",
    "5. append the flexible resources @ end"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data['pc_flag'] = np.where(data['Resource'].isin(par_cl_res), 1, 0)\n",
    "\n",
    "col_order = ['Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Constraint', 'Cleaning Type']\n",
    "dataA = data[col_order]\n",
    "dataB = data.drop(col_order, axis = 1)\n",
    "data = pd.concat([dataB, dataA], axis = 1)\n",
    "\n",
    "data_par = data[(data['pc_flag']==1)  & (data['MC Group'].isnull())].sort_values('Usage_End').reset_index(drop = True)\n",
    "data_flex = data[data[\"Resource\"].str.startswith('Flexible')].sort_values('Usage_End').reset_index(drop = True)\n",
    "data = data[~data[\"Resource\"].str.startswith('Flexible')].reset_index(drop = True)\n",
    "data = pd.concat([data, data_flex]).reset_index(drop = True)\n"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5ab3975-c22a-4263-87ba-daa82ecea6bb",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recleanings and final df\n",
    "1. Take all freezed records in data_freeze dataframe\n",
    "2. call include_recleans function to add rows of expected recleanings\n",
    "3. Sort the data with clean start, last possible clean and rank\n",
    "4. place the flexible data at the end\n",
    "5. pre_exit = in case allocation is done will be indicated as 1 else 0\n",
    "- all freezed records will hold 1\n",
    "6. Initial assignment and push assignment is nan for all records\n",
    "7. final df contains all assigned cleanings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_freeze = data[~data['Cleaning Type'].isnull()]\n",
    "data = include_recleans(df= data.copy())\n",
    "data = data.sort_values(['clean_start', 'Last Possible Clean', 'Resource Rank']).reset_index(drop = True).copy()\n",
    "data_flex = data[data['Resource'].str.contains('Flexible')]\n",
    "data_other = data[~data['Resource'].str.contains('Flexible')]\n",
    "\n",
    "data = pd.concat([data_other, data_flex]).reset_index(drop = True)\n",
    "\n",
    "data['Pre_Exist'] = np.where(data['MC Group'].isnull(), 0, 1)\n",
    "data['Initial Assignment'] = np.nan\n",
    "data['Push Assignment'] = np.nan\n",
    "# data['Parallel Clean Flag'] = np.nan\n",
    "\n",
    "final_df = data[data['Pre_Exist'] ==1].reset_index(drop = True)\n",
    "data = pd.concat([final_df, data[data['Pre_Exist'] != 1]]).reset_index(drop = True)\n",
    "data = pd.concat([data['Resource'], data.drop('Resource', axis = 1)], axis = 1)\n",
    "# final_df['MC Group'] = final_df['MC Group']+ ' - Group'"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6929d64b-8add-41da-9504-f9da3a50a3d9",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.to_clipboard(index = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count = 0; par_count = last_par_count\n",
    "data_freeze['Cleaning Type'].value_counts()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0ce05e6a-4168-465e-a3c6-e33e40e67ab0",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "mc_possible_df = check_cond(df = df_cip_avail_new, col = 'MC Group', cleaning_start = clean_start, \n",
    "                         cleaning_end = clean_end)\n",
    "mc_possible = mc_possible_df['MC Group'].unique().tolist()\n",
    "const_possible_df = check_cond(df = df_const_avail_new, col = 'Constraint', cleaning_start = clean_start, \n",
    "                            cleaning_end = clean_end)\n",
    "const_possible = const_possible_df['Constraint'].unique().tolist()\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.to_clipboard()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for index, row in data.iterrows():\n",
    "    res = row['Resource']; clean_start = row['clean_start']; clean_end = row['clean_end']\n",
    "    max_clean_start = row['Last Possible Clean']; priority =  row['Priority']; washing_time = row['Washing Time']\n",
    "    \n",
    "    if final_df[(row['LOT'] == final_df['LOT']) & (res == final_df['Resource']) & \n",
    "                (row['Type'] == final_df['Type']) & \n",
    "                (row['pc_flag'] ==1) & (final_df['Cleaning Type'].isnull())].shape[0] > 0:\n",
    "        continue\n",
    "    if row['Pre_Exist']==1:\n",
    "        continue\n",
    "\n",
    "    mc_list = df_cip_gconst[df_cip_gconst['Group'].isin([1,2])]['MC Group'].unique().tolist()\n",
    "    const_list = df_cip_gconst[df_cip_gconst['Group'].isin([1,2])]['Constraint'].unique().tolist()\n",
    "        \n",
    "    df_cip_avail_new = cleaning_availablity(df = final_df.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                            sub_col =  'Clean_End_Time', for_col = 'MC Group', max_clean = max_clean, \n",
    "                                            clean_dur = washing_time, df_avail = df_cip_avail.copy(), buffer_mins = buffer)\n",
    "    print('index:', index, res, row['LOT'], 'df Shape:', final_df.shape)\n",
    "    print('Missing Rec',final_df[final_df['MC Group'].isnull()].shape)\n",
    "    print('MC Overlap:', df_cip_avail_new[df_cip_avail_new['Available_Time']<0].shape[0])\n",
    "\n",
    "    df_const_avail_new = cleaning_availablity(df = final_df.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                              sub_col =  'Clean_End_Time', for_col = 'Constraint', max_clean = max_clean, \n",
    "                                              clean_dur = washing_time, df_avail = df_const_avail.copy(),buffer_mins = 0)\n",
    "    \n",
    "    print('Const Overlap:', df_const_avail_new[df_const_avail_new['Available_Time']<0].shape[0])\n",
    "    print('DHT Violation:', final_df[final_df['Last Possible Clean']< final_df['Clean_Start_Time']].shape[0])\n",
    "    df_m = pd.DataFrame()\n",
    "    pc_option = df_par_clean[df_par_clean['Process resource']==res]['PC Option'].tolist()\n",
    "    pc_av_logic =  data_par['Resource'].isin(pc_option).sum()\n",
    "#     if index == 62:\n",
    "#         break\n",
    "    for mins in range(0,1000):\n",
    "        clean_start = row['clean_start'] + pd.Timedelta(minutes=mins)\n",
    "        clean_end = clean_start + pd.Timedelta(minutes= washing_time*60)\n",
    "        if (max_clean_start < clean_start) & (df_m.shape[0] ==0):\n",
    "            final_df = pd.concat([final_df, data.loc[[index],:]])\n",
    "            print('No Slot Available for:', res, clean_start)\n",
    "            count = count +1\n",
    "            rows=1\n",
    "            dict_values = {'Clean_Start_Time': np.nan, 'Clean_End_Time': np.nan, \n",
    "                           'MC Group': np.nan, 'Constraint': np.nan, 'Usage_Start': None, \n",
    "                           'Usage_End': None, 'Maint_Flag': 0}\n",
    "            break\n",
    "        elif (max_clean_start < clean_start):\n",
    "            break\n",
    "        mc_possible_df = check_cond(df = df_cip_avail_new, col = 'MC Group', cleaning_start = clean_start, \n",
    "                                 cleaning_end = clean_end)\n",
    "        mc_possible = mc_possible_df['MC Group'].unique().tolist()\n",
    "        const_possible_df = check_cond(df = df_const_avail_new, col = 'Constraint', cleaning_start = clean_start, \n",
    "                                    cleaning_end = clean_end)\n",
    "        const_possible = const_possible_df['Constraint'].unique().tolist()\n",
    "\n",
    "        \n",
    "        if (len(mc_possible)==0) | (len(const_possible)==0):\n",
    "            continue\n",
    "        mc_possible = [elem for elem in mc_possible if elem in mc_list]\n",
    "        const_possible = [elem for elem in const_possible if elem in const_list]\n",
    "        \n",
    "        \n",
    "        th_time = max_clean_start\n",
    "        reclean = 0\n",
    "        rows = 0\n",
    "        dict_values = {}\n",
    "        for mc in mc_possible:\n",
    "            for const in const_possible:\n",
    "                rows = df_cip_gconst[(df_cip_gconst['Resource'] ==res) & (df_cip_gconst['Constraint'] ==const) & \n",
    "                                     (df_cip_gconst['MC Group'] ==mc)].shape[0]\n",
    "                \n",
    "                if rows>0:\n",
    "                    cl_start, cl_end = enough_gap_constraint(df = final_df.copy(), mc_group = mc, resource = res, \n",
    "                                                             mins = buffer,clean_start_tm = clean_start,\n",
    "                                                             clean_end_tm = clean_end, th_time = max_clean_start)\n",
    "                    cl_start, cl_end = test_overlap(df = data.copy(), fin_data= final_df.iloc[:-1,:], cl_st = cl_start, \n",
    "                                                    cl_end = cl_end, cip_gr = mc,const_gr= const, mins = buffer)\n",
    "                    if (cl_start > th_time):\n",
    "                        rows = 0\n",
    "                        continue\n",
    "                    clean_start, clean_end = cl_start, cl_end\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if rows>0:\n",
    "                dict_values = {'Clean_Start_Time': [clean_start], 'Clean_End_Time': [clean_end], 'MC Group': [mc], \n",
    "                               'Constraint': [const], 'Initial Assignment': [1], 'Usage_Start': [None], \n",
    "                               'Usage_End': [None], 'Maint_Flag': [0]}\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if (rows>0):\n",
    "            df_t = pd.DataFrame(dict_values)\n",
    "            df_m = pd.concat([df_m, df_t]).reset_index(drop = True)\n",
    "            if (pc_av_logic > 0) & (row['pc_flag'] ==1) :\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        # Extra added step\n",
    "        else:\n",
    "            continue   \n",
    "\n",
    "    if df_m.shape[0]>0:\n",
    "        dict_values = df_m.to_dict(orient='records')[0]\n",
    "        mc = dict_values['MC Group']; const = dict_values['Constraint']\n",
    "        if (row['pc_flag'] != 1) | (pc_av_logic == 0):\n",
    "            df_cip_avail['Clean_End_Time'].loc[(df_cip_avail['MC Group']==mc)] = max_clean\n",
    "            df_const_avail['Clean_End_Time'].loc[(df_const_avail['Constraint']==const)] = max_clean\n",
    "        data = CIP_assignment(df = data, index = index, main_flag = 0, val_dict = dict_values)\n",
    "        temp_data1 = data.loc[[index],:].copy()\n",
    "        final_df = pd.concat([final_df, temp_data1])    \n",
    "    if (max_clean_start < clean_start) & (df_m.shape[0] ==0):\n",
    "#         break\n",
    "        try:\n",
    "            final_df_push = Push_Cleaning(df = final_df.copy(), clean_start = row['clean_start'], clean_end = row['clean_end'], \n",
    "                                         washing_time = washing_time, usage_end = row['Usage_End'], usage_start = row['Usage_Start'],\n",
    "                                         index_val = index, last_pos = row['Last Possible Clean'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Module****Pushing Skipped****')\n",
    "            continue\n",
    "        final_df = final_df_push.reset_index(drop = True).copy()\n",
    "        final_df.loc[index, 'Push Assignment'] = 1\n",
    "        temp_data1 = final_df_push.tail(1)\n",
    "        mc =  temp_data1['MC Group'].tolist()[0]; const =  temp_data1['Constraint'].tolist()[0]\n",
    "        clean_start =  temp_data1['Clean_Start_Time'].tolist()[0]\n",
    "        clean_end =  temp_data1['Clean_End_Time'].tolist()[0]\n",
    "        dict_values = {'Clean_Start_Time': [clean_start], 'Clean_End_Time': [clean_end], 'MC Group': [mc], \n",
    "                        'Constraint': [const], 'Push Assignment': [1], 'Usage_Start': [None], 'Usage_End': [None], \n",
    "                        'Maint_Flag': [0]}\n",
    "        df_m = pd.DataFrame(dict_values) \n",
    "\n",
    "    if (pc_av_logic > 0) & (row['pc_flag'] ==1):\n",
    "#         break\n",
    "        last_record = final_df.iloc[[-1], :]\n",
    "        for i in range(0, df_m.shape[0]):\n",
    "            final_df = final_df.iloc[:-1,:]\n",
    "            dict_values = df_m.to_dict(orient='records')[i]\n",
    "            data = CIP_assignment(df = data, index = index, main_flag = 0, val_dict = dict_values)\n",
    "            temp_data1 = data.loc[[index],:].copy()\n",
    "            final_df = pd.concat([final_df, temp_data1])\n",
    "            df_cip_av = cleaning_availablity(df = final_df.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                            sub_col =  'Clean_End_Time', for_col = 'MC Group', max_clean = max_clean, \n",
    "                                            clean_dur = washing_time, df_avail = df_cip_avail.copy(), buffer_mins = buffer)\n",
    "\n",
    "            df_const_av = cleaning_availablity(df = final_df.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                                    sub_col =  'Clean_End_Time', for_col = 'Constraint', max_clean = max_clean, \n",
    "                                                    clean_dur = washing_time, df_avail = df_const_avail.copy(), buffer_mins = 0)\n",
    "            mc = dict_values['MC Group']; const = dict_values['Constraint']\n",
    "            clean_start = dict_values['Clean_Start_Time']; clean_end = dict_values['Clean_End_Time']\n",
    "\n",
    "            mc_data = df_cip_av[(df_cip_av['Clean_Start_Time'] == clean_start) & (df_cip_av['MC Group'] == mc) & \n",
    "                                (df_cip_av['Clean_End_Time'] == clean_end)]\n",
    "            mc_data = mc_data.reset_index(drop = True)\n",
    "\n",
    "            const_data = df_const_av[(df_const_av['Clean_Start_Time'] == clean_start) & (df_const_av['Constraint'] == const) & \n",
    "                                     (df_const_av['Clean_End_Time'] == clean_end)]\n",
    "            const_data = const_data.reset_index(drop = True)\n",
    "            ###### ##### ####### ####### ########\n",
    "            df_senstive = data[(clean_start>= data['clean_start']) & (data['MC Group'].isnull()) & \n",
    "                                   (data['idle_time'].round(1)<=1)]\n",
    "            ######  #######  #######  ##########\n",
    "            temp_data1['New Last Possible'] = min(mc_data['Last_pos_Clean'][0],const_data['Last_pos_Clean'][0], row['Last Possible Clean'])\n",
    "            clean_av = min(mc_data['Last_pos_Clean'][0],const_data['Last_pos_Clean'][0], row['Last Possible Clean'])\n",
    "\n",
    "            pc_option = df_par_clean[df_par_clean['Process resource']==res]['PC Option'].tolist()\n",
    "            temp_data2 = data_par[(data_par['clean_start'] < clean_av) & (data_par['Usage_End'] < clean_start) &\n",
    "                          (data_par['Last Possible Clean'] >= clean_start) & (data_par['Resource'].isin(pc_option))]\n",
    "            \n",
    "            if (temp_data2.shape[0]>0) & (df_senstive.shape[0]==0):\n",
    "                temp_data2=temp_data2.sort_values('Unutilized_gap').head(1)\n",
    "                drop_index = temp_data2.index[0]\n",
    "                \n",
    "                par_count = par_count + 1\n",
    "                data_par = data_par.drop(drop_index, axis = 0)\n",
    "                print('Hurrey...! Its Parellal No:', par_count)\n",
    "                break\n",
    "            elif df_senstive.shape[0]>0:\n",
    "                temp_data2 = pd.DataFrame(columns = temp_data1.columns)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        if temp_data2.shape[0]==0:\n",
    "            print('Saste me kam chalalo')\n",
    "            dict_values = df_m.to_dict(orient='records')[0]\n",
    "            data = CIP_assignment(df = data, index = index, main_flag = 0, val_dict = dict_values)\n",
    "            temp_data1 = last_record.copy()\n",
    "        temp_data = pd.concat([temp_data1, temp_data2.sort_values('Unutilized_gap').head(1)])\n",
    "        temp_data[['MC Group', 'Constraint']] = temp_data[['MC Group', 'Constraint']].ffill()\n",
    "        if temp_data.shape[0]> 1:\n",
    "            par_clean_start = temp_data2.reset_index(drop = True)['clean_start'][0]\n",
    "            par_clean_start = max(par_clean_start, clean_start)\n",
    "            temp_data['Clean_Start_Time'] = par_clean_start\n",
    "            temp_data['Clean_End_Time'] = temp_data['Clean_Start_Time'] + pd.to_timedelta(temp_data['Washing Time']*3600, \n",
    "                                                                                          unit='s')\n",
    "            temp_data['Parallel Clean Flag'] = par_count\n",
    "        temp_data1 = temp_data1.iloc[0]\n",
    "        data_par  = data_par[~((data_par['LOT']== temp_data1['LOT']) & (data_par['Type'] == temp_data1['Type']) & \n",
    "                            (data_par['Resource'] == temp_data1['Resource']))]\n",
    "        print('Data Parallel Shape', data_par.shape)\n",
    "        mc = temp_data['MC Group'].iloc[0]\n",
    "        const  = temp_data['Constraint'].iloc[0]\n",
    "        df_cip_avail['Clean_End_Time'].loc[(df_cip_avail['MC Group']==mc)] = max_clean\n",
    "        df_const_avail['Clean_End_Time'].loc[(df_const_avail['Constraint']==const)] = max_clean\n",
    "        final_df = pd.concat([final_df.iloc[:-1,:], temp_data]).reset_index(drop = True)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb071c74-eaf3-4b50-874b-5c87ffbe4e35",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df.to_clipboard()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#final_df.to_clipboard()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df_bkp = final_df.reset_index(drop = True).copy()\n",
    "par_count_bkp = par_count"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a73aaa0c-bf84-40e7-abab-614abb0273df",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "par_count = par_count_bkp\n",
    "final_df = final_df_bkp.copy()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09e31880-6170-475a-994d-34da673921f9",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = final_df.reset_index(drop = True).copy()\n",
    "\n",
    "df_cipc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "              sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "df_constc = cip_avail(df = df.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "              sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "const_overlap = df_constc[df_constc['Available_Time']<0].shape[0]\n",
    "dht_violation = df[df['Last Possible Clean']< df['Clean_Start_Time']].shape[0]\n",
    "mc_overlap = df_cipc[df_cipc['Available_Time']<0].shape[0]\n",
    "\n",
    "const_overlap + dht_violation + mc_overlap"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "199905d2-e890-4388-9e1a-bae8b2d813d2",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df, par_count = process_missing_parallel(df= final_df.copy(), count=par_count)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "355faa1d-15f0-4058-a7c5-6b1f5c96e220",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### One step Automated Miss Clean"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66b45411-9583-4242-bd93-da5ef5925172",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df.loc[(final_df['MC Group'].isnull()) & (~final_df['Resource'].str.contains('Flex')), ['MC Group', 'Constraint']] = 'NA'"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "22ae1e99-637a-4b05-a5dd-b0aa2c500557",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "exclude_schedules = ['P1', 'P2','P3', 'P4', 'AS26 TD', 'SA', 'SB', 'SC', 'SD']\n",
    "final_df = final_df.reset_index(drop = True).copy()\n",
    "num_mis_rec = final_df['MC Group'].isnull().sum()\n",
    "for im in range(num_mis_rec):\n",
    "    df_mis_cleaning = final_df[final_df['MC Group'].isnull()]\n",
    "    print('Missing Shape', df_mis_cleaning.shape)\n",
    "    if df_mis_cleaning.shape[0]==0:\n",
    "        break\n",
    "    mis_rec = df_mis_cleaning.iloc[0]\n",
    "    mis_index = df_mis_cleaning.index[0]\n",
    "    df_backup = final_df.copy()\n",
    "    clean_start = mis_rec['clean_start']; clean_end = mis_rec['clean_start']\n",
    "    df_move_options = move_schedule_options(df = final_df, excl_res = exclude_schedules, mising_rec = mis_rec)\n",
    "    if df_move_options.shape[0] == 0:\n",
    "        const_overlap = 0; dht_violation = 0; mc_overlap = 0\n",
    "        final_df.loc[mis_index, 'MC Group'] = 'NA'\n",
    "        final_df.loc[mis_index, 'Constraint'] = 'NA'\n",
    "    print('Move Options' , df_move_options.shape[0])\n",
    "    for i in df_move_options.index:\n",
    "        df = df_backup.copy(); move_row_index = i;\n",
    "        move_rec = df_move_options.loc[i]; move_const = move_rec['Constraint']; use_mc = move_rec['MC Group']; \n",
    "        move_clean_start = move_rec['Clean_Start_Time']; last_mc_used = move_rec['Last_MC Group_End']\n",
    "        last_possible_clean = move_rec['Last Possible Clean']; moved_res = move_rec['Resource']\n",
    "\n",
    "        use_const = constrain_basedon_mc(mc_group = None)\n",
    "        const_detail = final_constraint_use(constraint_list = use_const, \n",
    "                                            df = df.copy(), move_clean_st = move_clean_start)\n",
    "        use_const = const_detail[0]; last_const_used = const_detail[1]\n",
    "\n",
    "        assign_clean_start = max(last_mc_used, last_const_used, clean_start)+pd.Timedelta(minutes = 1)\n",
    "        assign_clean_end = assign_clean_start+pd.Timedelta(minutes = mis_rec['Washing Time']*60)\n",
    "        if last_possible_clean < assign_clean_start:\n",
    "            print('Initial start time crossing the threshold time')\n",
    "            continue\n",
    "        df.loc[mis_index, ['Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Constraint']] = [assign_clean_start, \n",
    "                                                                                           assign_clean_end, use_mc, use_const]\n",
    "        clean_start_move = move_rec['Clean_Start_Time']\n",
    "\n",
    "        df = df.loc[:, :'New Last Possible']\n",
    "        df = cip_positions(df = df.copy(), var = 'MC Group')\n",
    "        df = cip_positions(df = df.copy(), var = 'Constraint')\n",
    "\n",
    "        const_overlap, dht_violation, mc_overlap = check_overlaps(df = df.copy())\n",
    "        print('Initial Slot assignment', const_overlap, dht_violation, mc_overlap)\n",
    "        if (dht_violation==0) & (mc_overlap==0) & (const_overlap==0):\n",
    "            print('1-No need to work on Pushing anything, Assigning to Final_df')\n",
    "            df = df.loc[:, :'New Last Possible']\n",
    "            final_df = df.copy()\n",
    "            final_df, par_count = process_missing_parallel(df= final_df.copy(), count=par_count)\n",
    "            break\n",
    "        elif (mc_overlap>0):\n",
    "            updated_df = df.copy()\n",
    "            mc_loop_count = 0\n",
    "            while (dht_violation ==0) & (mc_overlap>0):\n",
    "                mc_loop_count = mc_loop_count+1\n",
    "                updated_df, next_index = mc_schedule_moving(df = updated_df.copy(), clean_start_move= move_clean_start, \n",
    "                                                            move_res = moved_res, mc = use_mc)\n",
    "                moved_res = updated_df.loc[next_index]['Resource']\n",
    "                move_clean_start = updated_df.loc[next_index]['Clean_Start_Time']\n",
    "                move_constc = updated_df.loc[next_index]['Constraint']\n",
    "                const_overlap, dht_violation, mc_overlap = check_overlaps(df = updated_df.copy())\n",
    "                print('MC Level Push', const_overlap, dht_violation, mc_overlap)\n",
    "                if (dht_violation ==0) & (mc_overlap==0) & (const_overlap==0):\n",
    "                    print('2-No need to work on Constrainint, Assigning to Final_df')\n",
    "                    updated_df = updated_df.loc[:, :'New Last Possible']; break\n",
    "                elif (mc_overlap==0) & (const_overlap>0):\n",
    "                    print('Check with constraint pushing'); break\n",
    "                elif mc_overlap>2:\n",
    "                    print('Go to next Move option'); break\n",
    "                if mc_loop_count==50:\n",
    "                    break\n",
    "            if (dht_violation>0) | (mc_overlap>=2):\n",
    "                print('Due to more than 2 overlaps Go to next move option'); continue\n",
    "            elif (dht_violation==0) & (mc_overlap==0) & (const_overlap>0):\n",
    "                const_loop_count = 0\n",
    "                while (dht_violation==0) & (mc_overlap<=1) & (const_overlap>0):\n",
    "                    updated_df = const_schedule_moving(df = updated_df.copy())\n",
    "                    const_overlap, dht_violation, mc_overlap = check_overlaps(df = updated_df.copy())\n",
    "                    print('Constraint Level Push', const_overlap, dht_violation, mc_overlap)\n",
    "                    if (dht_violation==0) & (mc_overlap==0) & (const_overlap==0):\n",
    "                        print('3-Constrainint push successful, Assigning to Final_df')\n",
    "                        updated_df = updated_df.loc[:, :'New Last Possible']; break\n",
    "                    elif (dht_violation>0) | (mc_overlap>1):\n",
    "                        break\n",
    "                    if const_loop_count == 50:\n",
    "                        break\n",
    "        if (dht_violation==0) & (mc_overlap==0) & (const_overlap==0):\n",
    "            final_df = updated_df.copy()\n",
    "            final_df, par_count = process_missing_parallel(df= final_df.copy(), count=par_count)\n",
    "            print('Successfull Processed')\n",
    "            df_mis_cleaning = df_mis_cleaning.iloc[1:,:]\n",
    "            break\n",
    "        else:\n",
    "            print('Go to next record')\n",
    "            continue\n",
    "    if (dht_violation!=0) | (mc_overlap!=0) | (const_overlap !=0):\n",
    "        if (i == df_move_options.index[-1]):\n",
    "            final_df.loc[mis_index, 'MC Group'] = 'NA'\n",
    "            final_df.loc[mis_index, 'Constraint'] = 'NA'\n",
    "            print('<<<<<<<<bhai>>>>>>>>')\n",
    "    #         df_mis_cleaning = df_mis_cleaning.iloc[1:,:]\n",
    "            continue\n",
    "final_df.loc[final_df['MC Group']=='NA', ['MC Group', 'Constraint']] = np.nan"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47036c71-0566-4f1a-ab7a-8c4099fd7398",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Maintenance"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "032ba1b7-ddac-484d-99b2-7d193df869b8",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df_bkp = final_df.reset_index(drop = True).copy()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36e0942e-4f77-4642-9a94-18c6a6a78071",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_P_assigned = final_df_bkp.reset_index(drop = True).copy()"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "151bfb3d-b1a3-47ed-9964-1edb6ce927f6",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df = final_df_bkp.copy()\n",
    "final_df['Clean_Start_Time'] = pd.to_datetime(final_df['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "final_df['Clean_End_Time'] = pd.to_datetime(final_df['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "final_df.loc[final_df['Cleaning Type']=='Reclean', 'Next Usage'] = final_df['Usage_Start']\n",
    "# final_df['Last_M_endtime'] = final_df['Next Usage'] - pd.to_timedelta(final_df['Washing Time']*3600, unit='s') #subtract 30 minutes\n",
    "# final_df['Last_M_endtime'] = final_df['Last_M_endtime'].fillna(final_df['Usage_End'].max())\n",
    "final_df['Last_M_endtime'] = final_df['Last Possible Clean']\n",
    "final_df['Time_av_for_M'] = final_df['Last_M_endtime'] - final_df['Clean_End_Time']\n",
    "final_df['Time_av_for_M'] = final_df['Time_av_for_M']/(np.timedelta64(1, 's')*3600)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "430e64fb-4845-4f29-a7ad-7c92b7497982",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = final_df.copy()\n",
    "x_df_remaining = df_cip_gconst.copy()\n",
    "count = 0\n",
    "for index, row in df_maintenance.iterrows():\n",
    "    count = count + 1\n",
    "    res = row['Resource']; start_time = row['Start_time']; end_time = row['End_time']; M_time = row['M_time']\n",
    "    M_DHT = row['Max DHT']; M_code = row['Code']\n",
    "    nearest_df = df[df['Resource']==res]\n",
    "    nearest_df['Maintain_pre_CHT'] = start_time - pd.to_timedelta(nearest_df['Max CHT']*3600, unit='s')\n",
    "    nearest_df = nearest_df[(nearest_df['Maintain_pre_CHT'] < nearest_df['Clean_End_Time']) & \n",
    "                            (nearest_df['Time_av_for_M'] > M_time) & (nearest_df['Cleaning Type'] != 'q-Pre Cleaning') ]\n",
    "    washingtime = mapping_df[mapping_df['Resource'] == res].reset_index(drop = True)['Washing Time'][0]\n",
    "    print(res, nearest_df.shape)\n",
    "    ######################## change on 01/12/2021\n",
    "    map_columns = ['MC Group', 'Constraint', 'Washing Time','Clean_Start_Time', 'Clean_End_Time','Parallel Clean Flag']\n",
    "    ####################### change on 01/12/2021\n",
    "    df_mc_group = df[map_columns]\n",
    "    \n",
    "    df_MC_available = cleaning_availablity(df = df_mc_group.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "                                           sub_col = 'Clean_End_Time', max_clean = max_clean, clean_dur = washing_time, \n",
    "                                           buffer_mins = buffer/5)                                                              #why buffer/5?\n",
    "    df_cons_available = cleaning_availablity(df = df_mc_group.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "                                             sub_col = 'Clean_End_Time', max_clean = max_clean, clean_dur = washing_time, \n",
    "                                             buffer_mins = 0)\n",
    "    rows = 0\n",
    "    if nearest_df.shape[0]>0:\n",
    "        for nearindex , nearrow in nearest_df.iterrows():\n",
    "            print(nearindex)\n",
    "            nearres = nearrow['Resource']; clean_end = nearrow['Clean_End_Time']; clean_start = nearrow['Clean_Start_Time']\n",
    "            last_possible = nearrow['Last_M_endtime']; washingtime = nearrow['Washing Time']\n",
    "            \n",
    "            start_time = clean_end + pd.Timedelta(minutes = buffer/2)\n",
    "            end_time = start_time + pd.Timedelta(minutes = M_time*60)\n",
    "            print(start_time, end_time)\n",
    "            if start_time < clean_end:\n",
    "                continue \n",
    "            for mins in range(0,1000):\n",
    "                clean_assign_start = end_time + pd.Timedelta(minutes = mins)\n",
    "                clean_assign_end = clean_assign_start + pd.Timedelta(minutes = washingtime*60)\n",
    "                end_time = clean_assign_start - pd.Timedelta(minutes = 1)\n",
    "                start_time = end_time - pd.Timedelta(minutes = M_time*60)\n",
    "                if clean_assign_start > last_possible:\n",
    "                    print(nearres, ' No Slot for cleaning')\n",
    "                    break\n",
    "                rows, nearest_df = clean_allocate(df_mc = df_MC_available, df_cons = df_cons_available, res = nearres,\n",
    "                                                  df_update = df.copy(), index = nearindex, loop_df = nearest_df,main_flag = 1,\n",
    "                                                  mc_const_df = df_cip_gconst.copy(), clean_start = clean_assign_start, \n",
    "                                                  clean_end = clean_assign_end, m_start = start_time, m_end = end_time, \n",
    "                                                  th_time = last_possible, buffer_time= buffer/5)\n",
    "                if rows>0:\n",
    "                    nearest_df.loc[nearindex,'run_flag'] = 0\n",
    "                    nearest_df.loc[nearindex,'LOT'] = M_code\n",
    "                    df = pd.concat([df, nearest_df.loc[[nearindex],:]])\n",
    "                    print('Final', start_time, end_time)\n",
    "                    break\n",
    "            if rows>0:\n",
    "                break\n",
    "    if (nearest_df.shape[0]==0) | (rows==0):\n",
    "        print('Me Andar aya')\n",
    "        blank_df = pd.DataFrame([[np.nan] * len(df.columns)], columns=df.columns)\n",
    "        ########################## new section added on 03/12/2021\n",
    "        #if df[df['Resource']==res].shape[0] == 0:\n",
    "        #    print(\"I didn't had any utilization\")\n",
    "        #    start_time = start_time\n",
    "        #    end_time = start_time + pd.Timedelta(minutes = M_time*60)\n",
    "        #    blank_df = blank_df\n",
    "        #    print(start_time, end_time)\n",
    "        #    if len(df_cip_gconst[df_cip_gconst[\"Resource\"]==res])==0:\n",
    "        #        add_mapping= mapping_df[mapping_df[\"Resource\"]==res]\n",
    "        #        select_var = ['Resource']+allmcgroup\n",
    "        #        add_mapping=add_mapping.loc[:, select_var]\n",
    "        #        add_mapping = pd.melt(add_mapping, id_vars = add_mapping.columns[0], value_vars=add_mapping.columns[1:], var_name='MC Group')\n",
    "        #        add_mapping = add_mapping[~(add_mapping['value'].isnull())]\n",
    "        #        add_mapping = add_mapping[~(add_mapping['MC Group'].isnull())]\n",
    "        #        add_mapping.columns = [\"Resource\",\"MC Group\",\"Constraint\"]\n",
    "        #        df_cip_gconst = pd.concat([df_cip_gconst,add_mapping]).reset_index(drop = True)  \n",
    "        ########################## new section ended on 03/12/2021\n",
    "        if df[df['Resource']==res].shape[0] > 0:\n",
    "            start_time = df[df['Resource']==res]['Clean_End_Time'].max() + pd.Timedelta(minutes = buffer/5)\n",
    "            end_time = start_time + pd.Timedelta(minutes = M_time*60)\n",
    "            blank_df = df[df['Resource']==res].head(1).reset_index(drop = True)\n",
    "            print(start_time, end_time)\n",
    "        for mins in range(0,1200):\n",
    "#             print(mins)\n",
    "            clean_assign_start = end_time + pd.Timedelta(minutes = mins*1)\n",
    "            clean_assign_end = clean_assign_start + pd.Timedelta(minutes = washingtime*60)\n",
    "            start_time = start_time + pd.Timedelta(minutes = mins*1)\n",
    "            end_time = end_time + pd.Timedelta(minutes = mins*1)\n",
    "            \n",
    "            rows, blank_df = clean_allocate(df_mc = df_MC_available, df_cons = df_cons_available, res = res,\n",
    "                                            df_update = df.copy(), index = 0, loop_df = blank_df, main_flag = 1, \n",
    "                                            mc_const_df = df_cip_gconst.copy(), clean_start = clean_assign_start, \n",
    "                                            clean_end = clean_assign_end, m_start = start_time, m_end = end_time, \n",
    "                                            th_time = max_clean, buffer_time= buffer/5)\n",
    "                   \n",
    "            if rows>0:\n",
    "                print('Yes Mil gaya')\n",
    "                clean_assign_start = blank_df['Clean_Start_Time'][0]\n",
    "                end_time = clean_assign_start - pd.Timedelta(minutes=10)\n",
    "                start_time = end_time -  pd.Timedelta(minutes=M_time*60)\n",
    "                \n",
    "                blank_df.loc[0,'Usage_Start'] = start_time\n",
    "                blank_df.loc[0,'Usage_End'] = end_time\n",
    "                blank_df.loc[0,'clean_flag'] = 1\n",
    "                blank_df.loc[0,'run_flag'] = 0\n",
    "                blank_df.loc[0,'Resource'] = res\n",
    "                blank_df.loc[0,'LOT'] = M_code\n",
    "                #if df[df['Resource']==res].shape[0] == 0:\n",
    "                #    print(\"aya\"+ str(df[df['Resource']==res].shape[0]))\n",
    "                #    blank_df.loc[0,'Last Possible Clean'] = max_clean \n",
    "                df = pd.concat([df, blank_df.loc[[0],:]])\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "afcdb14b-d6c2-42ab-b95e-d132e851fd02",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[[x for x in df.columns if x not in ['']]]\n",
    "#col_null = ['Utilized', 'addT', 'Next Usage', 'Unutilized_gap', 'clean_start', 'clean_end', 'idle_time',\n",
    "#       'run_flag', 'clean_flag', 'Max DHT Time', 'Last Possible Clean',\n",
    "#       'Last Cleaning_DHT', 'Start Cleaning_CHT', 'Last_pos_preclean',\n",
    "#       'Day Flag', 'pc_flag','Initial Assignment', 'Cleaning Type',\n",
    "#       'Push Assignment', 'Parallel Clean Flag', 'New Last Possible', 'Pre_Exist', 'Cleaning Type']\n",
    "###############change on 1/12/2021\n",
    "col_null = ['Utilized', 'addT', 'Next Usage', 'Unutilized_gap', 'clean_start', 'clean_end', 'idle_time',\n",
    "       'run_flag', 'clean_flag', 'Max DHT Time', 'Last Possible Clean',\n",
    "       'Last Cleaning_DHT', 'Start Cleaning_CHT', 'Last_pos_preclean',\n",
    "       'Day Flag', 'pc_flag','Initial Assignment', 'Cleaning Type',\n",
    "       'Push Assignment', 'Parallel Clean Flag', 'Pre_Exist', 'Cleaning Type']\n",
    "############# chnage on 1/12/2021 end\n",
    "\n",
    "if df_maintenance.shape[0] >0:\n",
    "    df.loc[df['Maint_Flag']==1, col_null]= np.nan\n",
    "else:\n",
    "    df['Maint_Flag'] = np.nan\n",
    "df_rec_performed = df[~df['Cleaning Type'].isnull()].reset_index(drop = True)\n",
    "df = df[df['Cleaning Type'].isnull()].reset_index(drop = True)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "901ab75f-caa6-4c38-bf01-ebbe7f52c42e",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_M_assigned = df.reset_index(drop = True)\n",
    "# Creation of cleaning start just after the first buffer is finished\n",
    "df['clean_start'] = df['Usage_End'] + pd.Timedelta(minutes = 1)\n",
    "\n",
    "# Assuming the cleaning time is exactly as the defined time\n",
    "df['clean_end'] = np.nan\n",
    "\n",
    "temp = mapping_df.rename(columns = {'Max DHT': 'DHT Time', 'Max CHT': 'CHT Time'})\n",
    "df_assigned = pd.merge(df.copy(), temp[['Resource', 'DHT Time', 'CHT Time']].drop_duplicates(), \n",
    "                       on = 'Resource', how = 'inner')\n",
    "\n",
    "df_assigned['Max DHT'] = np.where(df_assigned['Max DHT'].isnull(), \n",
    "                                          df_assigned['DHT Time'], df_assigned['Max DHT'])\n",
    "df_assigned['Max CHT'] = np.where(df_assigned['Max CHT'].isnull(), \n",
    "                                          df_assigned['CHT Time'], df_assigned['Max CHT'])\n",
    "df_assigned = df_assigned.drop(['DHT Time', 'CHT Time'], axis = 1)\n",
    "\n",
    "# Data Updation after maintenance\n",
    "df_assigned.Usage_Start= pd.to_datetime(df_assigned.Usage_Start, format = '%d/%m/%Y %H:%M:%S')\n",
    "df_assigned.Usage_End= pd.to_datetime(df_assigned.Usage_End, format = '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "df_assigned = df_assigned.sort_values(['Resource', 'Usage_End']).reset_index(drop= True)\n",
    "df_assigned['Utilized'] = df_assigned['Usage_End'] - df_assigned['Usage_Start']\n",
    "df_assigned['Utilized'] = df_assigned['Utilized']/(np.timedelta64(1, 's')*3600)\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "for res in df_assigned.Resource.unique():\n",
    "    data_temp = df_assigned[df_assigned['Resource'] == res]\n",
    "    data_temp = data_temp.sort_values('Usage_Start')\n",
    "    # Shift the data 1 level up to get perform the substraction operation\n",
    "    data_temp['Next Usage'] = data_temp['Usage_Start'].shift(-1)\n",
    "    data_temp['Unutilized_gap'] = data_temp['Next Usage'] - data_temp['Usage_End']\n",
    "    temp_df = pd.concat([temp_df, data_temp])\n",
    "df_assigned = temp_df.reset_index(drop = True)\n",
    "del(temp_df)\n",
    "print(check_overlaps(df_assigned))\n",
    "df_assigned['Unutilized_gap'] = df_assigned['Unutilized_gap']/(np.timedelta64(1, 's')*3600)\n",
    "df_assigned['idle_time'] = df_assigned['Unutilized_gap']-data1['Washing Time']\n",
    "\n",
    "df_assigned['Max DHT Time'] = df_assigned['Usage_End'] + pd.to_timedelta(df_assigned['Max DHT']*3600, unit='s')\n",
    "df_assigned['Last Possible Clean'] = df_assigned['Next Usage'] - pd.to_timedelta(df_assigned['Washing Time']*3600, unit='s')\n",
    "df_assigned['Last Cleaning_DHT'] = df_assigned['Max DHT Time'] - pd.to_timedelta(df_assigned['Washing Time']*3600, unit='s')\n",
    "cond = (df_assigned['Last Possible Clean']> df_assigned['Last Cleaning_DHT']) | (df_assigned['Last Possible Clean'].isnull())\n",
    "df_assigned['Last Possible Clean'] = np.where(cond, df_assigned['Last Cleaning_DHT'], df_assigned['Last Possible Clean'])\n",
    "\n",
    "df_assigned['Start Cleaning_CHT'] = df_assigned['Next Usage'] - pd.to_timedelta(df_assigned['Max CHT']*3600, unit='s')\n",
    "df_assigned['Start Cleaning_CHT'] = df_assigned['Start Cleaning_CHT'] - pd.to_timedelta(df_assigned['Washing Time']*3600, unit='s')\n",
    "\n",
    "res_adjust = ['P1', 'P2', 'P3', 'P4', 'SA', 'SB', 'SC', 'SD', 'AS26 TD',\n",
    "              'I/N 97', 'I/N 2224', 'I/N 2667','I/N 2315', 'I/N 1619','I/N 2668','I/N 2316','I/N 1554', \n",
    "              'Dome 121', 'Dome 89', 'Dome 2215']\n",
    "\n",
    "cond1 = (df_assigned['Start Cleaning_CHT']> df_assigned['clean_start']) & df_assigned['Resource'].isin(res_adjust) & (~df_assigned['Start Cleaning_CHT'].isnull())\n",
    "\n",
    "df_assigned['clean_start'] = np.where(cond1, df_assigned['Start Cleaning_CHT'], df_assigned['clean_start'])\n",
    "# Assuming the cleaning time is exactly as the defined time\n",
    "df_assigned['clean_end'] = df_assigned['clean_start'] + pd.to_timedelta(df_assigned['Washing Time']*3600, unit='s')\n",
    "df_assigned['Last_pos_preclean'] = df_assigned['Usage_Start'] - pd.to_timedelta(df_assigned['Washing Time']*3600, unit='s') - pd.Timedelta(minutes = buffer/5)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "583bdabd-f9b1-400c-b88b-75ef98310873",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Transfermation to identify the CHT &  DHT constraints"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2264ffb-dd35-48d2-a403-3517e29c61d5",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_assigned['Clean_Start_Time'] = pd.to_datetime(df_assigned['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "df_assigned['Clean_End_Time'] = pd.to_datetime(df_assigned['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "df_assigned['CHT Adjusted'] = np.nan\n",
    "df_assigned = calculate_constraint(df = df_assigned.copy())\n",
    "\n",
    "df_assigned = calculate_constraint_preclean(df = df_assigned.copy())\n",
    "df_assigned = pd.concat([df_assigned, df_rec_performed]).reset_index(drop = True)\n",
    "data_wo_cht_adj = df_assigned.copy()\n",
    "data_cht_adj = df_assigned[df_assigned['CHT_Adjust']==1].sort_values('Usage_End')\n",
    "df_assigned.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_cip_gconst = df_cip_gconst.rename(columns = {'value': 'Constraint'})\n",
    "\n",
    "data_cht_adj['Clean_Start_Time'] = np.nan; data_cht_adj['Clean_End_Time'] = np.nan\n",
    "data_cht_adj['MC Group'] = np.nan; data_cht_adj['Constraint'] = np.nan\n",
    "\n",
    "\n",
    "count = 0\n",
    "for adj_index, adj_row in data_cht_adj.iterrows():\n",
    "    \n",
    "    adj_res = adj_row['Resource']; adj_wash_time = adj_row['Washing Time']\n",
    "    print(adj_res)\n",
    "    idle_adj_start = adj_row['Start Cleaning_CHT']; dle_adj_last = adj_row['Last Cleaning_DHT']\n",
    "    idle_adj_end = idle_adj_start + pd.Timedelta(minutes = adj_wash_time*60)\n",
    "    \n",
    "    mc_avail_df = cleaning_availablity(df = data_wo_cht_adj.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "                                       max_clean = max_clean, clean_dur = adj_wash_time,\n",
    "                                       buffer_mins = buffer/5) \n",
    "    # Transfermed Constraint cleaning dataset\n",
    "    const_avail_df = cleaning_availablity(df = data_wo_cht_adj.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint',\n",
    "                                          max_clean = max_clean, clean_dur = adj_wash_time,\n",
    "                                          buffer_mins = 0)\n",
    "    rows = 0\n",
    "    for mins in range(0,1000): \n",
    "        # Move the slot(5 mins step)\n",
    "        idle_adj_start = adj_row['Start Cleaning_CHT'] + pd.Timedelta(minutes=(mins*1))\n",
    "        idle_adj_last = adj_row['Last Cleaning_DHT']\n",
    "        idle_adj_end = idle_adj_start + pd.Timedelta(minutes = adj_wash_time*60)\n",
    "        # No CIP can be assigned if the time has reached to last cleaning time\n",
    "        if idle_adj_start > idle_adj_last:\n",
    "            idle_adj_start_new = adj_row['Start Cleaning_CHT']\n",
    "            idle_adj_end_new = idle_adj_start_new + pd.Timedelta(minutes = adj_wash_time*60)\n",
    "            print('<<<<<<<<<<<<<<<<NO SLOT>>>>>>>>>>>>>>>')\n",
    "#             rows = 1\n",
    "            break\n",
    "        else:\n",
    "            rows, data_cht_adj = clean_allocate(df_mc = mc_avail_df, df_cons = const_avail_df, res = adj_res,\n",
    "                                                df_update = data_wo_cht_adj.copy(), index = adj_index, loop_df = data_cht_adj, \n",
    "                                                main_flag = 0, mc_const_df = df_cip_gconst.copy(), clean_start = idle_adj_start, \n",
    "                                                clean_end = idle_adj_end, m_start = None, m_end = None, th_time = idle_adj_last, \n",
    "                                                flag_var = \"CHT Adjusted\", buffer_time= buffer/5)\n",
    "            if rows>0:\n",
    "                data_wo_cht_adj = data_wo_cht_adj.drop(adj_index, axis = 0)\n",
    "                data_wo_cht_adj = pd.concat([data_wo_cht_adj, data_cht_adj.loc[[adj_index],:]])\n",
    "                \n",
    "                print('success')\n",
    "                break\n"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7dbd8259-c648-43ed-9593-1d8e67e0e33e",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_rec_performed = data_wo_cht_adj[data_wo_cht_adj['Cleaning Type']=='Reclean'].reset_index(drop = True)\n",
    "data_wo_cht_adj = data_wo_cht_adj[data_wo_cht_adj['Cleaning Type']!='Reclean'].reset_index(drop = True)\n",
    "\n",
    "# data_wo_cht_adj['Max DHT'] = data_wo_cht_adj['Max DHT']- (10/60)\n",
    "data_wo_cht_adj['Clean_Start_Time'] = pd.to_datetime(data_wo_cht_adj['Clean_Start_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "data_wo_cht_adj['Clean_End_Time'] = pd.to_datetime(data_wo_cht_adj['Clean_End_Time'], format = '%d/%m/%Y %H:%M:%S')\n",
    "data_wo_cht_adj.shape"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a3867fc-5ea8-4f45-9e46-1a2051d51c9d",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre Cleaning Handle"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7aa039ea-82cc-42d6-8401-c5745eb907f5",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_assigned1 = calculate_constraint(df = data_wo_cht_adj.copy())\n",
    "data_assigned2 = calculate_constraint_preclean(df = data_assigned1)\n",
    "\n",
    "data_assigned_shift = Utilization_Shifter(df = data_assigned2.copy())\n",
    "data_assigned1 = calculate_constraint(df = data_assigned_shift.copy())\n",
    "data_assigned2 = calculate_constraint_preclean(df = data_assigned1).reset_index(drop = True)\n",
    "data_assigned2.shape"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2df8020-1f21-421c-b5af-a3d4718cd4e7",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reclean_df = data_assigned2[(data_assigned2['CHT Violation Flag Pre Clean']==1)]\n",
    "reclean_df = reclean_df.sort_values('clean_start')"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbe7a9f5-55d1-4a0c-8278-62efb0619c9f",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "############################ change 06/12/2021\n",
    "if len(df_qcip)==0:\n",
    "    print(\"I am in If\")\n",
    "    min_clean = data.Usage_Start.min()\n",
    "    min_cht_clean = data[data['Usage_Start']==min_clean].iloc[0]['Max CHT']\n",
    "    min_clean = min_clean - pd.Timedelta(minutes=min_cht_clean*60)\n",
    "else:\n",
    "    min_clean = min(data.Clean_Start_Time.min(),data.Usage_Start.min())\n",
    "    try:\n",
    "        min_cht_clean = data[data['Usage_Start']==min_clean].iloc[0]['Max CHT']\n",
    "        min_clean = min_clean - pd.Timedelta(minutes=min_cht_clean*60)\n",
    "    except:\n",
    "        print(\"I am in except\")\n",
    "        min_cht_clean = data[data['Usage_Start']==data.Usage_Start.min()].iloc[0]['Max CHT']\n",
    "        min_usage_clean = data.Usage_Start.min() - pd.Timedelta(minutes=min_cht_clean*60)\n",
    "        min_clean = min(data.Clean_Start_Time.min(),min_usage_clean)\n",
    "############################ change end 06/12/2021       "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reclean_df['DHT Violation Flag'] = 0\n",
    "reclean_df['CHT Violation Flag'] = 0\n",
    "reclean_df['run_flag'] = 0\n",
    "reclean_df['Initial Assignment'] = 0\n",
    "reclean_df['Push Assignment'] = 0\n",
    "reclean_df['CHT Adjusted'] = 0\n",
    "data_assigned2['CHT Violation Flag'] = 0\n",
    "reclean_df['Pre Clean Start'] = np.where(reclean_df['Pre Clean Start']<=reclean_df['Last Clean End'], \n",
    "                                        reclean_df['Last Clean End'], reclean_df['Pre Clean Start'])\n",
    "reclean_df['Pre Clean Start'] = pd.to_datetime(reclean_df['Pre Clean Start'], \n",
    "                                                format = '%d/%m/%Y %H:%M:%S').dt.round('S').astype('datetime64[s]')\n",
    "\n",
    "reclean_df['preclean'] = 1\n",
    "reclean_df['Clean_Start_Time'] = np.nan; reclean_df['Clean_End_Time'] = np.nan\n",
    "reclean_df['MC Group'] = np.nan; reclean_df['Constraint'] = np.nan"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "439df654-ef91-49c3-986c-eedf55f7f13c",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count = 0\n",
    "for i, r in reclean_df.iterrows():\n",
    "    reclean_record = df_rec_performed[(df_rec_performed['Resource'] == r['Resource']) & (df_rec_performed['LOT'] == r['LOT']) & \n",
    "                                      (df_rec_performed['Type']== r['Type'])].reset_index(drop = True)\n",
    "    if reclean_record.shape[0]>0:\n",
    "        count = count+1\n",
    "        reclean_record = reclean_record.iloc[0]\n",
    "        reclean_df.loc[i, 'Clean_Start_Time'] = reclean_record['Clean_Start_Time']\n",
    "        reclean_df.loc[i, 'Clean_End_Time'] = reclean_record['Clean_End_Time']\n",
    "        reclean_df.loc[i, 'MC Group'] = reclean_record['MC Group']\n",
    "        reclean_df.loc[i, 'Constraint'] = reclean_record['Constraint']\n",
    "        reclean_df.loc[i, 'run_flag'] = 0\n",
    "        reclean_df.loc[i, 'CHT Violation Flag Pre Clean'] = 0\n",
    "        reclean_df.loc[i, 'Parallel Clean Flag'] = reclean_record['Parallel Clean Flag']\n",
    "        data_assigned2.loc[i,'CHT Violation Flag'] = 0\n",
    "\n",
    "reclean_filled = reclean_df[~reclean_df['MC Group'].isnull()]\n",
    "reclean_filled['Cleaning Type'] = 'Recleaning'\n",
    "data_assigned2['Cleaning Type'] = np.where(data_assigned2['Cleaning Type'].isnull(), \n",
    "                                           'Cleaning', 'Freezed')\n",
    "data_assigned2 = pd.concat([data_assigned2, reclean_filled])\n",
    "data_assigned2.shape"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97cc7ce7-3086-4cd4-a701-da0a77ea80fc",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_assigned2 = calculate_constraint(df = data_assigned2.copy())\n",
    "data_assigned2 = calculate_constraint_preclean(df = data_assigned2).reset_index(drop = True)\n",
    "reclean_df = data_assigned2[(data_assigned2['CHT Violation Flag Pre Clean']==1) & (data_assigned2['preclean'].isnull())]\n",
    "reclean_df = reclean_df.sort_values('clean_start')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, r in reclean_df.iterrows():\n",
    "    if data_freeze[(r['LOT']==data_freeze['LOT']) &  (r['Resource']==data_freeze['Resource']) & \n",
    "                  (r['Type'] == data_freeze['Type'])].shape[0]>0:\n",
    "        reclean_df = reclean_df.drop(i)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a07083e-3822-4b72-bbff-88c9dd3174a1",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_df_remaining = df_cip_gconst.rename(columns = {'value': 'Constraint'})\n",
    "reclean_df['preclean'] = 1\n",
    "reclean_df['Clean_Start_Time'] = np.nan; reclean_df['Clean_End_Time'] = np.nan\n",
    "reclean_df['MC Group'] = np.nan; reclean_df['Constraint'] = np.nan\n",
    "count = 0\n",
    "for reclean_index, reclean_row in reclean_df.iterrows():\n",
    "    count = count+1\n",
    "    reclean_wash_time = reclean_row['Washing Time']\n",
    "    reclean_res = reclean_row['Resource']\n",
    "    print(reclean_res)\n",
    "    # Transfermed CIP cleaning dataset\n",
    "    mc_avail_df = cleaning_availablity(df = data_assigned2.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                        sub_col =  'Clean_End_Time', for_col = 'MC Group', max_clean = max_clean, \n",
    "                                        clean_dur = reclean_wash_time, df_avail = df_cip_avail.copy(), \n",
    "                                       buffer_mins = buffer)\n",
    "    print(count, reclean_index, mc_avail_df[mc_avail_df['Available_Time']<0].shape[0])\n",
    "\n",
    "    # Transfermed Constraint cleaning dataset\n",
    "    const_avail_df = cleaning_availablity(df = data_assigned2.copy(), sort_by_col = 'Clean_Start_Time', \n",
    "                                        sub_col =  'Clean_End_Time', for_col = 'Constraint', max_clean = max_clean, \n",
    "                                        clean_dur = reclean_wash_time, df_avail = df_const_avail.copy(),\n",
    "                                          buffer_mins = 0)\n",
    "    rows = 0\n",
    "    if 'Flex' in reclean_res:\n",
    "        pc_option = df_par_clean[df_par_clean['Process resource']==reclean_res]['PC Option'].tolist()\n",
    "        df_par_pos = data_assigned2[(data_assigned2['Clean_Start_Time']>= reclean_row['Pre Clean Start']) &\n",
    "                                    (data_assigned2['Resource'].isin(pc_option)) & \n",
    "                                    (data_assigned2['Parallel Clean Flag'].isnull()) &\n",
    "                                    (data_assigned2['Last Possible Clean']<= reclean_row['Last_pos_preclean'])]\n",
    "        if df_par_pos.shape[0]>0:\n",
    "            par_count = par_count + 1\n",
    "            index_list = df_par_pos.index\n",
    "            data_assigned2.loc[index_list[0], 'Parallel Clean Flag'] = par_count\n",
    "            reclean_df.loc[reclean_index, 'Clean_Start_Time'] = data_assigned2['Clean_Start_Time'][index_list[0]]\n",
    "            reclean_df.loc[reclean_index, 'Clean_End_Time'] = data_assigned2['Clean_End_Time'][index_list[0]]\n",
    "            reclean_df.loc[reclean_index, 'MC Group'] = data_assigned2['MC Group'][index_list[0]]\n",
    "            reclean_df.loc[reclean_index, 'Constraint'] = data_assigned2['Constraint'][index_list[0]]\n",
    "            reclean_df.loc[reclean_index, 'Parallel Clean Flag'] = data_assigned2['Parallel Clean Flag'][index_list[0]]\n",
    "            reclean_df.loc[reclean_index, 'Reclean Status'] = 0\n",
    "            rows = 1;  cht_violation_flag = 0\n",
    "            print('Tata, Bye Bye, Gaya...')\n",
    "        \n",
    "    for mins in range(0,2500):\n",
    "        if rows == 1:\n",
    "            break\n",
    "        idle_reclean_start = reclean_row['Pre Clean Start'] + pd.Timedelta(minutes=(mins*1))\n",
    "        idle_reclean_end = idle_reclean_start + pd.Timedelta(minutes = reclean_wash_time*60)\n",
    "        idle_reclean_last = reclean_row['Last_pos_preclean']\n",
    "        # No CIP can be assigned if the time has reached to last cleaning time\n",
    "        if idle_reclean_start > idle_reclean_last:\n",
    "            idle_reclean_start_new = reclean_row['Last_pos_preclean']\n",
    "            idle_reclean_end_new = idle_reclean_start_new + pd.Timedelta(minutes = reclean_wash_time*60)\n",
    "            cht_violation_flag = 1\n",
    "            print(reclean_res, '<<<<<<<<<<<<<<<< NO SLOT >>>>>>>>>>>>>>>')\n",
    "            rows = 1\n",
    "            break\n",
    "        else:\n",
    "            rows, reclean_df = clean_allocate(df_mc = mc_avail_df, df_cons = const_avail_df, res = reclean_res, \n",
    "                                              df_update = data_assigned2.copy(), index = reclean_index, \n",
    "                                              loop_df = reclean_df, main_flag = 0, mc_const_df = x_df_remaining.copy(),\n",
    "                                              clean_start = idle_reclean_start, clean_end = idle_reclean_end, \n",
    "                                              m_start = None, m_end = None, th_time = idle_reclean_last, \n",
    "                                              reclean = 0, flag_var = 'Reclean Status', buffer_time= buffer/5)\n",
    "        \n",
    "        if rows>0:\n",
    "            cht_violation_flag = 0\n",
    "            print(mins)\n",
    "            break\n",
    "    if rows>0:\n",
    "        reclean_df.loc[reclean_index,'run_flag'] = 0\n",
    "        data_assigned2.loc[reclean_index,'CHT Violation Flag'] = cht_violation_flag\n",
    "        reclean_df.loc[reclean_index, 'CHT Violation Flag Pre Clean'] = 0\n",
    "        data_assigned2 = pd.concat([data_assigned2, reclean_df.loc[[reclean_index],:]])\n",
    "    else:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7bfc3ee-a914-45a4-b662-b18c3fbd12fb",
     "showTitle": false,
     "title": ""
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_cipc = cip_avail(df = data_assigned2.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "              sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "df_constc = cip_avail(df = data_assigned2.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "              sub_col = 'Clean_End_Time', max_clean = max_clean).drop('idle_till_time', axis = 1)\n",
    "\n",
    "const_overlap = df_constc[df_constc['Available_Time']<0].shape[0]\n",
    "dht_violation = data_assigned2[data_assigned2['Last Possible Clean']< data_assigned2['Clean_Start_Time']].shape[0]\n",
    "mc_overlap = df_cipc[df_cipc['Available_Time']<0].shape[0]\n",
    "(const_overlap + dht_violation + mc_overlap)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09ea509e-300d-4802-9a2f-abf6cbcfaf3f",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Plotting post handing the mandatory pre cleaning scenario"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a705bebe-754d-41a4-ba22-b600078eacf2",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_assigned2[data_assigned2['Last Possible Clean']<data_assigned2['Clean_Start_Time']]"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "999fb9d0-947f-4477-835c-991c06c9eadb",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_assigned2['MC Group'] = data_assigned2['MC Group'].apply(lambda x: str(x).replace(' - Group', ''))\n",
    "data_assigned2['MC Group'] = data_assigned2['MC Group'].replace('nan', np.nan)\n",
    "replace_di = {'AS26 TD': 'AS26 TD Line - L2445', 'AS26 VII': 'AS26 VII Line - L2464'}\n",
    "data_assigned2.Resource = data_assigned2.Resource.replace(replace_di) \n",
    "\n",
    "\n",
    "data_assigned2 = data_assigned2.rename(columns = {'Resource Exported': 'Resource Alt', 'Resource': 'Resource', \n",
    "                                                  'Type': 'Product Type'})\n",
    "\n",
    "data_assigned2.loc[data_assigned2[\"Resource\"].str.startswith('Flexible'), 'Resource Alt'] = 'Flexible'\n",
    "\n",
    "\n",
    "# data_assigned2['Cleaning Type'] = 'Post Utilization Cleaning'\n",
    "# # data_assigned2['Cleaning Type'] = np.where(~(data_assigned2['Parallel Clean Flag'].isnull()),\n",
    "# #                                            'Parallel Cleaning', data_assigned2['Cleaning Type'])\n",
    "# data_assigned2['Cleaning Type'] = np.where(data_assigned2['preclean']==1,\n",
    "#                                            'Re-Cleaning', data_assigned2['Cleaning Type'])  \n",
    "data_assigned_temp = data_assigned2.groupby('Resource', as_index= False)['Usage_Start'].min()\n",
    "data_assigned_temp = data_assigned_temp.rename(columns = {'Usage_Start': 'Usage_Start_min'})\n",
    "data_assigned2 = pd.merge(data_assigned2, data_assigned_temp, on = 'Resource', how = 'left')\n",
    "\n",
    " "
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "878767b0-405c-4772-9186-aff2d8c4a46c",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "############# new section"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = data_assigned2.copy()\n",
    "x_df_remaining = df_cip_gconst.copy()\n",
    "count = 0\n",
    "for index, row in df_maintenance.iterrows():\n",
    "    count = count + 1\n",
    "    res = row['Resource']; start_time = row['Start_time']; end_time = row['End_time']; M_time = row['M_time']\n",
    "    M_DHT = row['Max DHT']; M_code = row['Code']\n",
    "    nearest_df = df[df['Resource']==res]\n",
    "    nearest_df['Maintain_pre_CHT'] = start_time - pd.to_timedelta(nearest_df['Max CHT']*3600, unit='s')\n",
    "    nearest_df = nearest_df[(nearest_df['Maintain_pre_CHT'] < nearest_df['Clean_End_Time']) & \n",
    "                            (nearest_df['Time_av_for_M'] > M_time) & (nearest_df['Cleaning Type'] != 'q-Pre Cleaning') ]\n",
    "    washingtime = mapping_df[mapping_df['Resource'] == res].reset_index(drop = True)['Washing Time'][0]\n",
    "    print(res, nearest_df.shape)\n",
    "    ######################## change on 01/12/2021\n",
    "    map_columns = ['MC Group', 'Constraint', 'Washing Time','Clean_Start_Time', 'Clean_End_Time','Parallel Clean Flag']\n",
    "    ####################### change on 01/12/2021\n",
    "    df_mc_group = df[map_columns]\n",
    "    \n",
    "    df_MC_available = cleaning_availablity(df = df_mc_group.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'MC Group', \n",
    "                                           sub_col = 'Clean_End_Time', max_clean = max_clean, clean_dur = washing_time, \n",
    "                                           buffer_mins = buffer/5)                                                              #why buffer/5?\n",
    "    df_cons_available = cleaning_availablity(df = df_mc_group.copy(), sort_by_col = 'Clean_Start_Time', for_col = 'Constraint', \n",
    "                                             sub_col = 'Clean_End_Time', max_clean = max_clean, clean_dur = washing_time, \n",
    "                                             buffer_mins = 0)\n",
    "    rows = 0\n",
    "    if (nearest_df.shape[0]==0):\n",
    "        print('Me Andar aya')\n",
    "        blank_df = pd.DataFrame([[np.nan] * len(df.columns)], columns=df.columns)\n",
    "        ########################## new section added on 03/12/2021\n",
    "        if df[df['Resource']==res].shape[0] == 0:\n",
    "            print(\"I didn't had any utilization\")\n",
    "            start_time = start_time\n",
    "            end_time = start_time + pd.Timedelta(minutes = M_time*60)\n",
    "            blank_df = blank_df\n",
    "            print(start_time, end_time)\n",
    "            if len(df_cip_gconst[df_cip_gconst[\"Resource\"]==res])==0:\n",
    "                add_mapping= mapping_df[mapping_df[\"Resource\"]==res]\n",
    "                select_var = ['Resource']+allmcgroup\n",
    "                add_mapping=add_mapping.loc[:, select_var]\n",
    "                add_mapping = pd.melt(add_mapping, id_vars = add_mapping.columns[0], value_vars=add_mapping.columns[1:], var_name='MC Group')\n",
    "                add_mapping = add_mapping[~(add_mapping['value'].isnull())]\n",
    "                add_mapping = add_mapping[~(add_mapping['MC Group'].isnull())]\n",
    "                add_mapping.columns = [\"Resource\",\"MC Group\",\"Constraint\"]\n",
    "                df_cip_gconst = pd.concat([df_cip_gconst,add_mapping]).reset_index(drop = True)  \n",
    "        ######################### new section ended on 03/12/2021\n",
    "        for mins in range(0,1200):\n",
    "#             print(mins)\n",
    "            clean_assign_start = end_time + pd.Timedelta(minutes = mins*1)\n",
    "            clean_assign_end = clean_assign_start + pd.Timedelta(minutes = washingtime*60)\n",
    "            start_time = start_time + pd.Timedelta(minutes = mins*1)\n",
    "            end_time = end_time + pd.Timedelta(minutes = mins*1)\n",
    "            \n",
    "            rows, blank_df = clean_allocate(df_mc = df_MC_available, df_cons = df_cons_available, res = res,\n",
    "                                            df_update = df.copy(), index = 0, loop_df = blank_df, main_flag = 1, \n",
    "                                            mc_const_df = df_cip_gconst.copy(), clean_start = clean_assign_start, \n",
    "                                            clean_end = clean_assign_end, m_start = start_time, m_end = end_time, \n",
    "                                            th_time = max_clean, buffer_time= buffer/5)\n",
    "               \n",
    "            if rows>0:\n",
    "                blank_df['MC Group'] = blank_df['MC Group'].str.replace(' - Group', '')\n",
    "                print('Yes Mil gaya')\n",
    "                clean_assign_start = blank_df['Clean_Start_Time'][0]\n",
    "                end_time = clean_assign_start - pd.Timedelta(minutes=10)\n",
    "                start_time = end_time -  pd.Timedelta(minutes=M_time*60)\n",
    "                \n",
    "                blank_df.loc[0,'Usage_Start'] = start_time\n",
    "                blank_df.loc[0,'Usage_End'] = end_time\n",
    "                blank_df.loc[0,'clean_flag'] = 1\n",
    "                blank_df.loc[0,'run_flag'] = 0\n",
    "                blank_df.loc[0,'Resource'] = res\n",
    "                blank_df.loc[0,'LOT'] = M_code\n",
    "                if df[df['Resource']==res].shape[0] == 0:\n",
    "                    print(\"aya\"+ str(df[df['Resource']==res].shape[0]))\n",
    "                    blank_df.loc[0,'Last Possible Clean'] = max_clean\n",
    "                    blank_df.loc[0,'DHT Violation Flag'] = 0\n",
    "                    blank_df.loc[0,'CHT Violation Flag'] = 0\n",
    "                df = pd.concat([df, blank_df.loc[[0],:]])\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_assigned2['Cleaning Type'] = np.where(data_assigned2['preclean']==1,\n",
    "                                           'Recleaning', data_assigned2['Cleaning Type']) \n",
    "data_assigned2['Cleaning Type'] = np.where((data_assigned2['preclean']==1) & \n",
    "                                           (data_assigned2['Usage_Start']==data_assigned2['Usage_Start_min']),\n",
    "                                           'Pre-Cleaning', data_assigned2['Cleaning Type'])  \n",
    "data_assigned2['Cleaning Type'] = np.where((data_assigned2['Maint_Flag']==1),\n",
    "                                           'Maintenance', data_assigned2['Cleaning Type']) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########### new section"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del_col = ['clean_start', 'clean_end', 'Last Cleaning_DHT', 'Start Cleaning_CHT',\n",
    "           'Last_pos_preclean', 'Day Flag','Last_M_endtime', 'Time_av_for_M', 'CHT_Adjust', \n",
    "           'Last Clean End', 'Pre Clean Start']\n",
    "data_assigned_pipe3 = data_assigned2.drop(del_col, axis = 1)\n",
    "# data_assigned_pipe3['Washing Time'] = data_assigned_pipe3['Washing Time']*60\n",
    "data_assigned_pipe3['Washing Time'] = data_assigned_pipe3['Clean_End_Time'] - data_assigned_pipe3['Clean_Start_Time']\n",
    "data_assigned_pipe3['Washing Time'] = data_assigned_pipe3['Washing Time']/(np.timedelta64(1, 's')*60)\n",
    "\n",
    "data_assigned_pipe3 = data_assigned_pipe3.rename(columns = {'idle_time' : 'Idle Time Ex Cleaning',\n",
    "                                                            'Max DHT Time': 'End DHT Time',\n",
    "                                                            'Washing Time': 'Cleaning Duration'})\n",
    "\n",
    "data_assigned_pipe3['End CHT Time'] = data_assigned_pipe3['Clean_End_Time'] + pd.to_timedelta(data_assigned_pipe3['Max CHT']*3600, unit='s')\n",
    "\n",
    "round_col =  ['Utilized', 'Unutilized_gap', 'Idle Time Ex Cleaning', 'Actual DHT', 'Actual CHT', 'Last CHT']\n",
    "for col in round_col:\n",
    "    data_assigned_pipe3[col] = data_assigned_pipe3[col].round(2) \n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cde95748-a211-4920-8e93-42d047474d76",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "order_col = ['LOT', 'Resource', 'Resource Alt', 'Product Type', 'Usage_Start', 'Usage_End', 'Utilized', 'Next Usage', \n",
    "             'Unutilized_gap', 'Cleaning Duration', 'Idle Time Ex Cleaning', 'Max DHT', 'Max CHT', 'End DHT Time', \n",
    "             'End CHT Time',  'Clean_Start_Time', 'Clean_End_Time', 'MC Group', 'Constraint', 'run_flag', 'clean_flag', \n",
    "             'Maint_Flag',  'Initial Assignment','Push Assignment','Actual DHT', 'Actual CHT', 'DHT Violation Flag', \n",
    "             'Last CHT', 'CHT Adjusted', 'Utilization Shifted', 'CHT Violation(Intermediate Gap)', \n",
    "             'CHT Violation Flag Pre Clean', 'CHT Violation Flag', 'preclean', 'Parallel Clean Flag',\n",
    "             'Cleaning Type']\n",
    "\n",
    "sort_col = ['Resource', 'Usage_Start', 'Clean_Start_Time']\n",
    "data_assigned_pipe4 = data_assigned_pipe3[order_col].sort_values(sort_col).reset_index(drop = True)\n",
    "flag_col = ['preclean', 'Maint_Flag', 'Initial Assignment','Push Assignment', 'CHT Adjusted']\n",
    "data_assigned_pipe4[flag_col] = data_assigned_pipe4[flag_col].fillna(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pre_final_df = data_assigned_pipe4[['Resource', 'LOT', 'Clean_Start_Time', 'Cleaning Duration', 'MC Group',\n",
    "                                    'Usage_Start', 'Usage_End', 'Clean_End_Time', 'End DHT Time', 'End CHT Time', \n",
    "                                    'Constraint', 'Cleaning Type']]\n",
    "df_maintenance = df_maintenance.rename(columns = {'Code': 'LOT'})\n",
    "pre_final_df = pd.merge(pre_final_df, df_maintenance[['LOT', 'Resource', 'Start_time', 'End_time']], \n",
    "                        on = ['LOT', 'Resource'], how = 'left')\n",
    "\n",
    "pre_final_df = pre_final_df.rename(columns = {'Start_time':'Old_Maintenance_Start_Time', \n",
    "                                              'End_time':'Old_Maintenance_End_Time'})"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b255c183-91c4-424c-b49d-37c8ee45a1c1",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_exp_maint = ['Resource', 'LOT', 'Usage_Start', 'Usage_End', 'Old_Maintenance_Start_Time', 'Old_Maintenance_End_Time']\n",
    "df_maint = pre_final_df[pre_final_df['Cleaning Type']=='Maintenance'][columns_exp_maint]\n",
    "pre_final_df = pre_final_df.drop(columns_exp_maint[-2:], axis = 1)"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44761323-6ca1-40a4-b350-47093bfd1b2d",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime \n",
    "time_now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M\")"
   ],
   "outputs": [],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a998e2b9-7229-428e-a870-d08dab270139",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_path  = r'C:\\Users\\AtulPoddar\\OneDrive - TheMathCompany Private Limited\\Documents\\Takeda\\testing\\lot_code_test3'\n",
    "\n",
    "export_file_name = output_path +'\\\\'+today+'_Takeda_Priority Production planning_Outcomev13.xlsx'\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(export_file_name, engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "data_assigned_pipe4.to_excel(writer, sheet_name='Optimized Schedule Output (R)',index = False)\n",
    "pre_final_df.to_excel(writer, sheet_name='Input ->Infor(OFD)', index = False)\n",
    "df_maint.to_excel(writer, sheet_name='Maintenance ->Infor(OFD)', index = False)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "# dbutils.fs.mkdirs(last_modified_file_path() + '/output/')\n",
    "output_path  = create_folder() + '/output/'\n",
    "#dbutils.fs.mkdirs(output_path)\n",
    "# export_file_name = output_path +'\\\\'+today+'_Takeda_Priority Production planning_Outcomev3.xlsx'\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "dbutils.fs.mkdirs(output_path + today + '_' + utcnow() +'_folder_file')\n",
    "data_assigned_pipe4.to_csv(last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_Optimized_Schedule_Output_(R).csv',index = False)\n",
    "pre_final_df.to_csv(last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_Input-Infor(OFD).csv', index = False)\n",
    "df_maint.to_csv(last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_Maintenance-Infor(OFD).csv', index = False)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d2d5cd7-c382-48ce-84cd-cef6735b6228",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "path = last_modified_file_path() + '/output'\n",
    "arcname = path.split('/')[-1]\n",
    "\n",
    "zf = zipfile.ZipFile('/tmp/output_zip.zip', mode='w')\n",
    "zf.write(last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_Optimized_Schedule_Output_(R).csv',arcname = today + '_' + utcnow() + '_Optimized_Schedule_Output_(R).csv')\n",
    "zf.write(last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_Input-Infor(OFD).csv',arcname = today + '_' + utcnow() + '_Input-Infor(OFD).csv')\n",
    "zf.write(last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_Maintenance-Infor(OFD).csv',arcname = today + '_' + utcnow() + '_Maintenance-Infor(OFD).csv')\n",
    "zf.close()\n",
    "shutil.copy('/tmp/output_zip.zip',last_modified_file_path() + '/output/' + today + '_' + utcnow() + '_output_zip.zip')\n",
    "shutil.copy('/tmp/output_zip.zip', '/dbfs/mnt/TakedaMount2/merged_output_repository/' + 'output_zip.zip')"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b687d267-92ac-4ead-9493-bd1433caac51",
     "showTitle": false,
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "schedule = pd.read_csv(r\"C:\\Users\\AtulPoddar\\OneDrive - TheMathCompany Private Limited\\Documents\\Takeda\\testing\\lot_code_test3\\Schedule.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#################### generate gant chart#######################################################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from dateutil import parser\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "schedule=schedule.reset_index(drop=True)\n",
    "def filter_data(selected_filter, df):\n",
    "    cols_in_df = list(df.columns)\n",
    "    # Filtering data\n",
    "    if len(selected_filters) > 0:\n",
    "        for key in selected_filters.keys():\n",
    "            if type(selected_filters[key])==dict and selected_filters[key].get('start_date',False) and selected_filters[key].get('end_date',False):\n",
    "                selected_filters[key]['start_date']=pd.to_datetime(selected_filters[key]['start_date'],infer_datetime_format=True,utc=True)\n",
    "                selected_filters[key]['end_date']=pd.to_datetime(selected_filters[key]['end_date'],infer_datetime_format=True,utc=True)\n",
    "                print(selected_filters)\n",
    "                df=df[(df['Start']>selected_filters[key]['start_date']) & (df['Finish']<selected_filters[key]['end_date'])]\n",
    "            elif key in cols_in_df:\n",
    "                if type(selected_filter[key]) == str or type(selected_filter[key]) == int:\n",
    "                    if selected_filter[key] == 'All':\n",
    "                        continue\n",
    "                    else:\n",
    "                        df = df[df[key] == selected_filter[key]]\n",
    "                else:\n",
    "                    if isinstance(selected_filter[key],list) and 'All' in selected_filter[key]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        df = df[df[key].isin(selected_filter[key])]\n",
    "            else:\n",
    "                continue\n",
    "    return df\n",
    "util_features=['Resource','Usage_Start','Usage_End','Cleaning Type','MC Group','Cleaning Duration']\n",
    "clean_features=['Resource','Clean_Start_Time','Clean_End_Time','Cleaning Type','MC Group','Cleaning Duration']\n",
    "util_schedule=schedule[util_features]\n",
    "util_schedule=util_schedule.rename(columns={'Resource':'Task','Usage_Start':'Start','Usage_End':'Finish'})\n",
    "options=['Pre-Cleaning','Recleaning']\n",
    "util_schedule['Resource']=np.where((util_schedule['MC Group'].isnull()) & (util_schedule['Cleaning Type'].isin(options)),'Utilization Post CHT violation','Utilization')\n",
    "util_schedule['Resource']=np.where((util_schedule['Cleaning Type']=='Cleaning') & (util_schedule['Cleaning Duration'].isnull()) & (util_schedule['Resource']=='Utilization'),'Cleaning Post DHT violation',util_schedule['Resource'])\n",
    "clean_schedule=schedule[clean_features]\n",
    "clean_schedule=clean_schedule.rename(columns={'Resource':'Task','Clean_Start_Time':'Start','Clean_End_Time':'Finish','Cleaning Type':'Resource'})\n",
    "final_schedule=pd.concat([util_schedule,clean_schedule])\n",
    "final_schedule['Start']= pd.to_datetime(final_schedule['Start'],infer_datetime_format=True,utc=True)\n",
    "final_schedule['Finish']= pd.to_datetime(final_schedule['Finish'],infer_datetime_format=True,utc=True)\n",
    "final_schedule=final_schedule.rename(columns={'MC Group':'Description'})\n",
    "final_schedule=filter_data(selected_filters,final_schedule)\n",
    "final_schedule['Task'] = np.where((final_schedule['Task']=='P6 - Cutting&Loading'),'P6',final_schedule['Task'])\n",
    "final_schedule['Task'] = np.where((final_schedule['Task']=='P5 - Cutting&Loading'),'P5',final_schedule['Task'])\n",
    "final_schedule['Resource']=np.where(final_schedule['Resource']=='Freezed','Cleaning',final_schedule['Resource'])\n",
    "sorter=['P5','P6','T78','T79','T80','T81','T82','P1','P2','P3','P4','SA','SB','SC','SD','Short Line - L717S','AS26 TD Line - L2445','Factor Line - L777','AS16 Line - L2103','AS26 VII Line - L2464','Long Transfer Line - L717L','4F Line - L2437','Dome 121','Dome 2215','Dome 89','615','616','617','I/N 1619','I/N 2668','I/N 97','I/N 2315','I/N 2667','I/N 2224','Flexibles - Cryo KIT A','Flexibles - Cryo KIT B',\n",
    "'Flexibles - Cryo KIT Colorati 1','Flexibles - Cryo KIT Colorati 2','Flexibles - Mass Capture 1/2','Flexibles - Mass Capture 3/4','Flexibles - Factor IX/FEIBA','Flexibles - AS16','Flexibles - AS26 VII','I/N 2316','I/N 1554','Gauthier Filter','T50','T51','T52','T53','T54','T55','T58','T59',\n",
    "'T60','T61','T62','T63','T64','T65','Flexibles - Kit 4F','Flexibles - Kit UF/DF', 'Flexibles - Kit Colonna','Flexibles - Kit Eluato','Flexibles - Kit Lavaggio','Flexibles - Kit Manifold','Flexibles - DDCPP']\n",
    "final_schedule['Task'] = pd.Categorical(final_schedule['Task'], categories = sorter)\n",
    "final_schedule=final_schedule.sort_values('Task').reset_index(drop= True)\n",
    "colors = {'Pre-Cleaning': 'rgb(153, 0, 255)',\n",
    "          'Utilization': 'rgb(0, 255, 100)',\n",
    "          'Cleaning':(1, 0.9, 0.16),\n",
    "          'Recleaning':'rgb(0, 0, 220)',\n",
    "          'Cleaning Post DHT violation':'rgb(220, 0, 0)',\n",
    "           'Utilization Post CHT violation':'rgb(255, 140, 0)',\n",
    "           'Maintenance':'rgb(100, 50, 100)'}\n",
    "fig = ff.create_gantt(final_schedule, colors=colors, index_col='Resource', show_colorbar=True,\n",
    "                      group_tasks=True)\n",
    "fig.update_xaxes(\n",
    "    tickformat=\"%d-%b-%Y %H:%M\")\n",
    "fig.layout.xaxis.rangeselector = None\n",
    "dynamic_outputs = plotly.io.to_json(fig)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Takeda CIP Assignment_Phase1",
   "notebookOrigID": 328527231519779,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}